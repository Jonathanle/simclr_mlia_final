{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20e0afa8-b669-4d09-935a-0ddfd90e5523",
   "metadata": {},
   "source": [
    "# Key Components of this Implementation\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "- **Normalization and Resizing**: Input images are normalized to [0, 1] and resized to 224x224 using a Lanczos filter.\n",
    "- **Augmentation**: SimCLR uses strong augmentations such as:\n",
    "  - Random resized cropping\n",
    "  - Horizontal flipping\n",
    "  - Color jittering\n",
    "  - Gaussian blur  \n",
    "  These augmentations create two distinct views of each image.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "- A `SimCLRDataset` class generates paired augmented views `(img_1, img_2)` for the unsupervised contrastive learning phase.\n",
    "\n",
    "## SimCLR Model\n",
    "\n",
    "- **Backbone (Encoder)**: ResNet-18 pretrained on ImageNet, modified to accept single-channel images.\n",
    "- **Projection Head**: A small fully connected network with two layers and ReLU activation, mapping features from the encoder to a lower-dimensional latent space.\n",
    "\n",
    "## NT-Xent Loss (Contrastive Loss)\n",
    "\n",
    "- Encourages similar views of the same image to have a higher similarity, while dissimilar pairs (views from different images) have a lower similarity.\n",
    "- Uses normalized temperature-scaled cross-entropy loss.\n",
    "\n",
    "## Unsupervised Pretraining\n",
    "\n",
    "- The model is trained using NT-Xent loss in an unsupervised manner, leveraging the augmented pairs of images without labels.\n",
    "\n",
    "## Supervised Fine-Tuning\n",
    "\n",
    "- A classification head is added to the encoder, and both are fine-tuned using labeled training data for the classification task.\n",
    "\n",
    "## Testing\n",
    "\n",
    "- The model's encoder and classification head are evaluated on the test dataset for classification accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "# Training Details of the SimCLR Model\n",
    "\n",
    "## Training Parameters\n",
    "\n",
    "- **Batch Size**: \n",
    "  - 512 for SimCLR pretraining \n",
    "  - 128 for supervised fine-tuning classification task\n",
    "- **Learning Rate**:\n",
    "  - 1e-4 for the SimCLR model during pretraining.\n",
    "  - 1e-5 for the encoder and 1e-3 for the classification head during fine-tuning.\n",
    "- **Optimizer**: Adam optimizer.\n",
    "- **Scheduler**: StepLR with a decay factor of 0.5 every 10 epochs (fine-tuning phase).\n",
    "\n",
    "## Augmentation Strategy\n",
    "\n",
    "- Contrastive learning heavily relies on strong augmentations to create diverse views of the same image for robust feature learning.\n",
    "\n",
    "## Unsupervised Pretraining\n",
    "\n",
    "- Pretrained for 100 epochs using the NT-Xent loss on the unlabeled training dataset.\n",
    "\n",
    "## Supervised Fine-Tuning\n",
    "\n",
    "- Trained for 100 epochs using cross-entropy loss on labeled training data to classify the brain images.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "- Accuracy is calculated on the test dataset after fine-tuning the classification head.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d90dba-f12a-4951-981a-1a45dda37571",
   "metadata": {},
   "source": [
    "# PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba2a17b8-1368-498e-ab5e-59725246896a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Section 1: Import Libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Section 2: Load and Preprocess Dataset\n",
    "train_images_path = \"brain_train_image_final.npy\"\n",
    "train_labels_path = \"brain_train_label.npy\"\n",
    "test_images_path = \"brain_test_image_final.npy\"\n",
    "test_labels_path = \"brain_test_label.npy\"\n",
    "\n",
    "# Load the data\n",
    "final_X_train_modified = np.load(train_images_path)[:, 1, :, :]\n",
    "final_X_test_modified = np.load(test_images_path)[:, 1, :, :]\n",
    "train_labels = np.load(train_labels_path)\n",
    "test_labels = np.load(test_labels_path)\n",
    "\n",
    "# Normalize and Resize Images using Pillow\n",
    "def normalize_and_resize(images, target_size=(224, 224)):\n",
    "    resized_images = []\n",
    "    for img in images:\n",
    "        img = Image.fromarray((img * 255).astype(np.uint8))\n",
    "        img_resized = img.resize(target_size, Image.Resampling.LANCZOS)\n",
    "        resized_images.append(np.array(img_resized) / 255.0)\n",
    "    return np.array(resized_images)\n",
    "\n",
    "final_X_train_resized = normalize_and_resize(final_X_train_modified)\n",
    "final_X_test_resized = normalize_and_resize(final_X_test_modified)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37c30fd-efa4-4196-a83d-569d45342817",
   "metadata": {},
   "source": [
    "# SimCLR Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14242746-7370-47ab-80a7-e97ccc695f62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Section 3: Define SimCLR Augmentation and Dataset\n",
    "transform_simclr = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0), ratio=(3/4, 4/3)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.8, contrast=0.8, saturation=0.8, hue=0.2),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=23, sigma=(0.1, 2.0))], p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "class SimCLRDataset(Dataset):\n",
    "    def __init__(self, images, transform):\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        img_1 = self.transform(img)\n",
    "        img_2 = self.transform(img)\n",
    "        return img_1, img_2\n",
    "\n",
    "train_dataset = SimCLRDataset(final_X_train_resized, transform_simclr)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db515b04-1434-442c-80ea-863837ee240a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image_0_aug1.png and image_0_aug2.png to augmented_images\n",
      "Saved image_1_aug1.png and image_1_aug2.png to augmented_images\n",
      "Saved image_2_aug1.png and image_2_aug2.png to augmented_images\n",
      "Saved image_3_aug1.png and image_3_aug2.png to augmented_images\n",
      "Saved image_4_aug1.png and image_4_aug2.png to augmented_images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Section 4: Save Augmented Images\n",
    "def save_augmented_images(dataset, save_dir=\"augmented_images\", num_images=5):\n",
    "    \"\"\"\n",
    "    Save a set of augmented images from the dataset to a directory.\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for i in range(num_images):\n",
    "        img_original = dataset.images[i]  # Original image\n",
    "        img_aug1, img_aug2 = dataset[i]  # Augmented pair\n",
    "\n",
    "        # Unnormalize images\n",
    "        img_aug1 = transforms.ToPILImage()(img_aug1 * 0.5 + 0.5)\n",
    "        img_aug2 = transforms.ToPILImage()(img_aug2 * 0.5 + 0.5)\n",
    "\n",
    "        # Save images\n",
    "        img_aug1.save(os.path.join(save_dir, f\"image_{i}_aug1.png\"))\n",
    "        img_aug2.save(os.path.join(save_dir, f\"image_{i}_aug2.png\"))\n",
    "\n",
    "        print(f\"Saved image_{i}_aug1.png and image_{i}_aug2.png to {save_dir}\")\n",
    "\n",
    "# Save augmented images from the training dataset\n",
    "save_augmented_images(train_dataset, save_dir=\"augmented_images\", num_images=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5d6a85-888f-4e31-9f7a-801705d5a2af",
   "metadata": {},
   "source": [
    "# Training on Brain train images with batch size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8306d584-1620-4f0a-b83b-c2c2d206155c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 8.2772\n",
      "Epoch [2/100], Loss: 6.8251\n",
      "Epoch [3/100], Loss: 6.6696\n",
      "Epoch [4/100], Loss: 6.6111\n",
      "Epoch [5/100], Loss: 6.5437\n",
      "Epoch [6/100], Loss: 6.5174\n",
      "Epoch [7/100], Loss: 6.4632\n",
      "Epoch [8/100], Loss: 6.4140\n",
      "Epoch [9/100], Loss: 6.3949\n",
      "Epoch [10/100], Loss: 6.3732\n",
      "Epoch [11/100], Loss: 6.2921\n",
      "Epoch [12/100], Loss: 6.2366\n",
      "Epoch [13/100], Loss: 6.2183\n",
      "Epoch [14/100], Loss: 6.1661\n",
      "Epoch [15/100], Loss: 6.0848\n",
      "Epoch [16/100], Loss: 5.9737\n",
      "Epoch [17/100], Loss: 5.9455\n",
      "Epoch [18/100], Loss: 5.8610\n",
      "Epoch [19/100], Loss: 5.8191\n",
      "Epoch [20/100], Loss: 5.7111\n",
      "Epoch [21/100], Loss: 5.6278\n",
      "Epoch [22/100], Loss: 5.5041\n",
      "Epoch [23/100], Loss: 5.3868\n",
      "Epoch [24/100], Loss: 5.3143\n",
      "Epoch [25/100], Loss: 5.2859\n",
      "Epoch [26/100], Loss: 5.1270\n",
      "Epoch [27/100], Loss: 5.0033\n",
      "Epoch [28/100], Loss: 4.9615\n",
      "Epoch [29/100], Loss: 4.8229\n",
      "Epoch [30/100], Loss: 4.8235\n",
      "Epoch [31/100], Loss: 4.6408\n",
      "Epoch [32/100], Loss: 4.5560\n",
      "Epoch [33/100], Loss: 4.4728\n",
      "Epoch [34/100], Loss: 4.4754\n",
      "Epoch [35/100], Loss: 4.3225\n",
      "Epoch [36/100], Loss: 4.1914\n",
      "Epoch [37/100], Loss: 4.2145\n",
      "Epoch [38/100], Loss: 4.1191\n",
      "Epoch [39/100], Loss: 3.9942\n",
      "Epoch [40/100], Loss: 3.8343\n",
      "Epoch [41/100], Loss: 3.9036\n",
      "Epoch [42/100], Loss: 3.7688\n",
      "Epoch [43/100], Loss: 3.6788\n",
      "Epoch [44/100], Loss: 3.7844\n",
      "Epoch [45/100], Loss: 3.6997\n",
      "Epoch [46/100], Loss: 3.5567\n",
      "Epoch [47/100], Loss: 3.5555\n",
      "Epoch [48/100], Loss: 3.4665\n",
      "Epoch [49/100], Loss: 3.3747\n",
      "Epoch [50/100], Loss: 3.4499\n",
      "Epoch [51/100], Loss: 3.4349\n",
      "Epoch [52/100], Loss: 3.3726\n",
      "Epoch [53/100], Loss: 3.2670\n",
      "Epoch [54/100], Loss: 3.2838\n",
      "Epoch [55/100], Loss: 3.2837\n",
      "Epoch [56/100], Loss: 3.1972\n",
      "Epoch [57/100], Loss: 3.2367\n",
      "Epoch [58/100], Loss: 3.1820\n",
      "Epoch [59/100], Loss: 3.0866\n",
      "Epoch [60/100], Loss: 3.1339\n",
      "Epoch [61/100], Loss: 3.1024\n",
      "Epoch [62/100], Loss: 3.1507\n",
      "Epoch [63/100], Loss: 2.9905\n",
      "Epoch [64/100], Loss: 3.0509\n",
      "Epoch [65/100], Loss: 3.0374\n",
      "Epoch [66/100], Loss: 2.9900\n",
      "Epoch [67/100], Loss: 2.9229\n",
      "Epoch [68/100], Loss: 2.9011\n",
      "Epoch [69/100], Loss: 2.9013\n",
      "Epoch [70/100], Loss: 3.0267\n",
      "Epoch [71/100], Loss: 2.8598\n",
      "Epoch [72/100], Loss: 2.9227\n",
      "Epoch [73/100], Loss: 2.9355\n",
      "Epoch [74/100], Loss: 2.8170\n",
      "Epoch [75/100], Loss: 2.8147\n",
      "Epoch [76/100], Loss: 2.8941\n",
      "Epoch [77/100], Loss: 2.8195\n",
      "Epoch [78/100], Loss: 2.7569\n",
      "Epoch [79/100], Loss: 2.7880\n",
      "Epoch [80/100], Loss: 2.8099\n",
      "Epoch [81/100], Loss: 2.6838\n",
      "Epoch [82/100], Loss: 2.7209\n",
      "Epoch [83/100], Loss: 2.7843\n",
      "Epoch [84/100], Loss: 2.6737\n",
      "Epoch [85/100], Loss: 2.6397\n",
      "Epoch [86/100], Loss: 2.6790\n",
      "Epoch [87/100], Loss: 2.6868\n",
      "Epoch [88/100], Loss: 2.6679\n",
      "Epoch [89/100], Loss: 2.6490\n",
      "Epoch [90/100], Loss: 2.6822\n",
      "Epoch [91/100], Loss: 2.6770\n",
      "Epoch [92/100], Loss: 2.6407\n",
      "Epoch [93/100], Loss: 2.6761\n",
      "Epoch [94/100], Loss: 2.5286\n",
      "Epoch [95/100], Loss: 2.5765\n",
      "Epoch [96/100], Loss: 2.5836\n",
      "Epoch [97/100], Loss: 2.4300\n",
      "Epoch [98/100], Loss: 2.7017\n",
      "Epoch [99/100], Loss: 2.5605\n",
      "Epoch [100/100], Loss: 2.5725\n"
     ]
    }
   ],
   "source": [
    "# Section 4: Define SimCLR Model and NT-Xent Loss\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, base_encoder, projection_dim):\n",
    "        super(SimCLR, self).__init__()\n",
    "        self.encoder = base_encoder\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, projection_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        z = self.projector(h)\n",
    "        return z\n",
    "\n",
    "class NTXentLoss(nn.Module):\n",
    "    def __init__(self, batch_size, temperature):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        N = z_i.size(0) + z_j.size(0)\n",
    "        z = torch.cat((z_i, z_j), dim=0)\n",
    "        sim = torch.matmul(z, z.T) / self.temperature\n",
    "        mask = ~torch.eye(N, dtype=torch.bool, device=z.device)\n",
    "\n",
    "        positives = torch.cat([\n",
    "            torch.diag(sim, z_i.size(0)),\n",
    "            torch.diag(sim, -z_i.size(0))\n",
    "        ])\n",
    "\n",
    "        negatives = sim[mask].view(N, -1)\n",
    "        logits = torch.cat((positives.unsqueeze(1), negatives), dim=1)\n",
    "        labels = torch.zeros(N, dtype=torch.long, device=z.device)\n",
    "        loss = self.criterion(logits, labels) / N\n",
    "        return loss\n",
    "\n",
    "# Section 5: Initialize and Train SimCLR Model\n",
    "base_encoder = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "base_encoder.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "base_encoder.fc = nn.Identity()\n",
    "\n",
    "model = SimCLR(base_encoder, projection_dim=128).to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = NTXentLoss(batch_size=512, temperature=0.5)\n",
    "\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for img_1, img_2 in train_loader:\n",
    "        img_1, img_2 = img_1.to(\"cuda\"), img_2.to(\"cuda\")\n",
    "        z_i = model(img_1)\n",
    "        z_j = model(img_2)\n",
    "\n",
    "        loss = criterion(z_i, z_j)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/100], Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1da5cd75-ef91-47f5-91c6-4bb29a2f7030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained SimCLR model saved to simclr_pretrained_18_512.pth\n"
     ]
    }
   ],
   "source": [
    "# Section 6: Save the Pretrained SimCLR Model\n",
    "import os\n",
    "\n",
    "# Define the path to save the model\n",
    "save_path = \"simclr_pretrained_18_512.pth\"\n",
    "\n",
    "# Save the model's state dictionary\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epoch': epoch,\n",
    "    'loss': total_loss / len(train_loader)\n",
    "}, save_path)\n",
    "\n",
    "print(f\"Pretrained SimCLR model saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0438f2-58ab-4317-ad03-193e06a365ee",
   "metadata": {},
   "source": [
    "# Testing on Brain test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237f7977-7778-4606-b5b0-f03b4c59eb92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 3.4937, Accuracy: 0.4237\n",
      "Epoch [2/100], Loss: 3.0544, Accuracy: 0.5492\n",
      "Epoch [3/100], Loss: 2.8101, Accuracy: 0.6053\n",
      "Epoch [4/100], Loss: 2.5918, Accuracy: 0.6578\n",
      "Epoch [5/100], Loss: 2.3303, Accuracy: 0.7158\n",
      "Epoch [6/100], Loss: 2.0782, Accuracy: 0.7538\n",
      "Epoch [7/100], Loss: 1.8342, Accuracy: 0.8002\n",
      "Epoch [8/100], Loss: 1.5554, Accuracy: 0.8491\n",
      "Epoch [9/100], Loss: 1.2786, Accuracy: 0.8847\n",
      "Epoch [10/100], Loss: 1.0565, Accuracy: 0.9179\n",
      "Epoch [11/100], Loss: 0.8176, Accuracy: 0.9469\n",
      "Epoch [12/100], Loss: 0.6860, Accuracy: 0.9662\n",
      "Epoch [13/100], Loss: 0.6101, Accuracy: 0.9734\n",
      "Epoch [14/100], Loss: 0.5217, Accuracy: 0.9831\n",
      "Epoch [15/100], Loss: 0.4395, Accuracy: 0.9867\n",
      "Epoch [16/100], Loss: 0.3945, Accuracy: 0.9916\n",
      "Epoch [17/100], Loss: 0.3326, Accuracy: 0.9940\n",
      "Epoch [18/100], Loss: 0.2885, Accuracy: 0.9958\n",
      "Epoch [19/100], Loss: 0.2317, Accuracy: 0.9982\n",
      "Epoch [20/100], Loss: 0.1984, Accuracy: 0.9988\n",
      "Epoch [21/100], Loss: 0.1882, Accuracy: 0.9976\n",
      "Epoch [22/100], Loss: 0.1615, Accuracy: 0.9994\n",
      "Epoch [23/100], Loss: 0.1557, Accuracy: 0.9988\n",
      "Epoch [24/100], Loss: 0.1444, Accuracy: 0.9988\n",
      "Epoch [25/100], Loss: 0.1343, Accuracy: 0.9994\n",
      "Epoch [26/100], Loss: 0.1218, Accuracy: 0.9994\n",
      "Epoch [27/100], Loss: 0.1165, Accuracy: 0.9994\n",
      "Epoch [28/100], Loss: 0.1148, Accuracy: 1.0000\n",
      "Epoch [29/100], Loss: 0.1052, Accuracy: 0.9994\n",
      "Epoch [30/100], Loss: 0.0940, Accuracy: 0.9994\n",
      "Epoch [31/100], Loss: 0.0855, Accuracy: 1.0000\n",
      "Epoch [32/100], Loss: 0.0900, Accuracy: 1.0000\n",
      "Epoch [33/100], Loss: 0.0827, Accuracy: 1.0000\n",
      "Epoch [34/100], Loss: 0.0818, Accuracy: 1.0000\n",
      "Epoch [35/100], Loss: 0.0823, Accuracy: 1.0000\n",
      "Epoch [36/100], Loss: 0.0792, Accuracy: 1.0000\n",
      "Epoch [37/100], Loss: 0.0746, Accuracy: 1.0000\n",
      "Epoch [38/100], Loss: 0.0705, Accuracy: 1.0000\n",
      "Epoch [39/100], Loss: 0.0661, Accuracy: 1.0000\n",
      "Epoch [40/100], Loss: 0.0705, Accuracy: 1.0000\n",
      "Epoch [41/100], Loss: 0.0614, Accuracy: 1.0000\n",
      "Epoch [42/100], Loss: 0.0650, Accuracy: 1.0000\n",
      "Epoch [43/100], Loss: 0.0636, Accuracy: 1.0000\n",
      "Epoch [44/100], Loss: 0.0593, Accuracy: 1.0000\n",
      "Epoch [45/100], Loss: 0.0621, Accuracy: 1.0000\n",
      "Epoch [46/100], Loss: 0.0637, Accuracy: 1.0000\n",
      "Epoch [47/100], Loss: 0.0613, Accuracy: 1.0000\n",
      "Epoch [48/100], Loss: 0.0597, Accuracy: 1.0000\n",
      "Epoch [49/100], Loss: 0.0576, Accuracy: 1.0000\n",
      "Epoch [50/100], Loss: 0.0590, Accuracy: 1.0000\n",
      "Epoch [51/100], Loss: 0.0615, Accuracy: 1.0000\n",
      "Epoch [52/100], Loss: 0.0542, Accuracy: 1.0000\n",
      "Epoch [53/100], Loss: 0.0559, Accuracy: 1.0000\n",
      "Epoch [54/100], Loss: 0.0511, Accuracy: 1.0000\n",
      "Epoch [55/100], Loss: 0.0535, Accuracy: 1.0000\n",
      "Epoch [56/100], Loss: 0.0554, Accuracy: 1.0000\n",
      "Epoch [57/100], Loss: 0.0557, Accuracy: 1.0000\n",
      "Epoch [58/100], Loss: 0.0546, Accuracy: 1.0000\n",
      "Epoch [59/100], Loss: 0.0530, Accuracy: 1.0000\n",
      "Epoch [60/100], Loss: 0.0509, Accuracy: 1.0000\n",
      "Epoch [61/100], Loss: 0.0539, Accuracy: 1.0000\n",
      "Epoch [62/100], Loss: 0.0528, Accuracy: 1.0000\n",
      "Epoch [63/100], Loss: 0.0537, Accuracy: 1.0000\n",
      "Epoch [64/100], Loss: 0.0527, Accuracy: 1.0000\n",
      "Epoch [65/100], Loss: 0.0545, Accuracy: 1.0000\n",
      "Epoch [66/100], Loss: 0.0527, Accuracy: 1.0000\n",
      "Epoch [67/100], Loss: 0.0524, Accuracy: 1.0000\n",
      "Epoch [68/100], Loss: 0.0503, Accuracy: 1.0000\n",
      "Epoch [69/100], Loss: 0.0486, Accuracy: 1.0000\n",
      "Epoch [70/100], Loss: 0.0519, Accuracy: 1.0000\n",
      "Epoch [71/100], Loss: 0.0535, Accuracy: 1.0000\n",
      "Epoch [72/100], Loss: 0.0557, Accuracy: 1.0000\n",
      "Epoch [73/100], Loss: 0.0503, Accuracy: 1.0000\n",
      "Epoch [74/100], Loss: 0.0485, Accuracy: 1.0000\n",
      "Epoch [75/100], Loss: 0.0477, Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Section 7: Define Classification Dataset and Head\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "train_dataset = TestDataset(final_X_train_resized, train_labels, transform=train_transform)\n",
    "test_dataset = TestDataset(final_X_test_resized, test_labels, transform=test_transform)\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "classification_head = ClassificationHead(input_dim=512, num_classes=len(np.unique(train_labels))).to(\"cuda\")\n",
    "\n",
    "# Section 8: Fine-tune and Evaluate Classification Model\n",
    "optimizer_cls = optim.Adam([\n",
    "    {\"params\": model.encoder.parameters(), \"lr\": 1e-5},\n",
    "    {\"params\": classification_head.parameters(), \"lr\": 1e-3},\n",
    "])\n",
    "scheduler_cls = StepLR(optimizer_cls, step_size=10, gamma=0.5)\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "\n",
    "# Fine-tune\n",
    "for epoch in range(100):\n",
    "    model.encoder.train()\n",
    "    classification_head.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for img, label in DataLoader(train_dataset, batch_size=128, shuffle=True):\n",
    "        img, label = img.to(\"cuda\"), label.to(\"cuda\")\n",
    "        features = model.encoder(img)\n",
    "        logits = classification_head(features)\n",
    "        loss = criterion_cls(logits, label)\n",
    "\n",
    "        optimizer_cls.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_cls.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct += (logits.argmax(dim=1) == label).sum().item()\n",
    "\n",
    "    accuracy = correct / len(train_labels)\n",
    "    scheduler_cls.step()\n",
    "    print(f\"Epoch [{epoch+1}/100], Loss: {total_loss/len(train_loader):.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate\n",
    "classification_head.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for img, label in DataLoader(test_dataset, batch_size=128, shuffle=False):\n",
    "        img, label = img.to(\"cuda\"), label.to(\"cuda\")\n",
    "        features = model.encoder(img)\n",
    "        logits = classification_head(features)\n",
    "        correct += (logits.argmax(dim=1) == label).sum().item()\n",
    "\n",
    "test_accuracy = correct / len(test_labels)\n",
    "print(f\"Test Accuracy%: {test_accuracy*100:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d656c6b4-0080-4691-b0e1-5a95f80a2646",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.4.0",
   "language": "python",
   "name": "pytorch-2.4.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
