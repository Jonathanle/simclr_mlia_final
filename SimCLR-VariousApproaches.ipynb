{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a527a29b-83da-408e-85da-87dc57040807",
   "metadata": {},
   "source": [
    "# Explanation of the SimCLR Implementation with ResNet50\n",
    "\n",
    "**Note**: Due to GPU memory limitations, I could not run the ResNet50 implementation. Below is an explanation of its structure and key features:\n",
    "\n",
    "## 1. Data Preprocessing and Augmentation\n",
    "\n",
    "### Normalization and Resizing:\n",
    "- Input images are resized to `224x224` and normalized with mean `[0.5]` and standard deviation `[0.5]` to match the expected range for ResNet-50.\n",
    "\n",
    "### SimCLR Augmentation:\n",
    "- Transformations include:\n",
    "  - `RandomResizedCrop`\n",
    "  - `RandomHorizontalFlip`\n",
    "  - `ColorJitter`\n",
    "  - `GaussianBlur`\n",
    "  - `RandomApply`\n",
    "- These augmentations generate two views of the same image, critical for contrastive learning, ensuring the model learns representations invariant to these transformations.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Dataset and DataLoader\n",
    "\n",
    "### Custom Dataset:\n",
    "- Provides two augmented views of each image for SimCLR training.\n",
    "\n",
    "### Batch Size:\n",
    "- Large batch sizes (`1024` for contrastive learning, `64` for fine-tuning) align with SimCLR's reliance on a diverse batch for contrastive loss effectiveness.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. SimCLR Model Architecture\n",
    "\n",
    "### Base Encoder:\n",
    "- A modified ResNet-50 backbone, pre-trained on ImageNet:\n",
    "  - The final `fc` layer is replaced with an `Identity` layer to retain feature embeddings.\n",
    "  - Input channels are modified to handle single-channel (grayscale) images.\n",
    "\n",
    "### Projection Head:\n",
    "- A two-layer neural network:\n",
    "  - `Linear(2048, 1024)` → `ReLU` → `Linear(1024, 128)`.\n",
    "- Projects features into a lower-dimensional space.\n",
    "- Only encoder features are used for downstream tasks, while the projection head aids in learning representations during pre-training.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Loss Function\n",
    "\n",
    "### NT-Xent Loss (Normalized Temperature-Scaled Cross-Entropy Loss):\n",
    "- Measures similarity between positive pairs (two views of the same image) and contrasts them against negative pairs (different images in the batch).\n",
    "- Features:\n",
    "  - Temperature scaling improves gradient flow and learning.\n",
    "  - Implements a softmax over the similarity matrix.\n",
    "  - CrossEntropyLoss calculates the loss.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Training Procedure\n",
    "\n",
    "### Contrastive Learning Stage:\n",
    "- The SimCLR model is trained using:\n",
    "  - Augmented image pairs.\n",
    "  - NT-Xent loss.\n",
    "\n",
    "### Optimization:\n",
    "- **Optimizer**: Adam with a learning rate of `3e-4`.\n",
    "- **Scheduler**: CosineAnnealingLR for gradual learning rate decay.\n",
    "\n",
    "### Key Factors:\n",
    "- Diverse data augmentation.\n",
    "- Sufficiently large batch size.\n",
    "- Effective contrastive loss with temperature scaling.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Downstream Task: Classification\n",
    "\n",
    "### Linear Evaluation Protocol:\n",
    "- Pre-trained encoder representations are fine-tuned using a linear classification head (`Linear(2048, num_classes)`).\n",
    "\n",
    "### Optimization:\n",
    "- **Loss**: CrossEntropyLoss.\n",
    "- Encoder weights are fine-tuned at a smaller learning rate (`1e-5`), while the classification head adapts with a higher learning rate (`3e-4`).\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Key Features Contributing to Accuracy\n",
    "\n",
    "1. **Augmentation Diversity**:\n",
    "   - Rich transformations help the model learn invariant features that generalize well to unseen data.\n",
    "   \n",
    "2. **Projection Head**:\n",
    "   - Helps learn meaningful low-dimensional embeddings, improving representation quality.\n",
    "\n",
    "3. **ResNet-50 Encoder**:\n",
    "   - A powerful pre-trained backbone provides a strong initialization.\n",
    "\n",
    "4. **NT-Xent Loss**:\n",
    "   - Promotes robust and discriminative embeddings through contrastive learning.\n",
    "\n",
    "5. **Large Batch Sizes**:\n",
    "   - Increases the number of negative samples, enhancing contrastive loss effectiveness.\n",
    "\n",
    "6. **Cosine Learning Rate Scheduler**:\n",
    "   - Smoothly decays the learning rate, ensuring stable convergence.\n",
    "\n",
    "7. **Separate Fine-Tuning**:\n",
    "   - Retains learned representations in the encoder while allowing the classification head to adapt to the specific task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f490a123-d9e2-49e5-8e27-fadae71a7895",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import ResNet50_Weights\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "train_images_path = \"brain_train_image_final.npy\"\n",
    "train_labels_path = \"brain_train_label.npy\"\n",
    "test_images_path = \"brain_test_image_final.npy\"\n",
    "test_labels_path = \"brain_test_label.npy\"\n",
    "\n",
    "# Load the data\n",
    "final_X_train_modified = np.load(train_images_path)[:, 1, :, :]\n",
    "final_X_test_modified = np.load(test_images_path)[:, 1, :, :]\n",
    "train_labels = np.load(train_labels_path)\n",
    "test_labels = np.load(test_labels_path)\n",
    "\n",
    "# Normalize and Resize Images using Pillow\n",
    "def normalize_and_resize(images, target_size=(224, 224)):\n",
    "    resized_images = []\n",
    "    for img in images:\n",
    "        img = Image.fromarray((img * 255).astype(np.uint8))\n",
    "        img_resized = img.resize(target_size, Image.Resampling.LANCZOS)\n",
    "        resized_images.append(np.array(img_resized) / 255.0)\n",
    "    return np.array(resized_images)\n",
    "\n",
    "final_X_train_resized = normalize_and_resize(final_X_train_modified)\n",
    "final_X_test_resized = normalize_and_resize(final_X_test_modified)\n",
    "\n",
    "# Define SimCLR Augmentation Transform\n",
    "transform_simclr = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0), ratio=(3/4, 4/3)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.8, contrast=0.8, saturation=0.8, hue=0.2),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=23, sigma=(0.1, 2.0))], p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Custom Dataset for SimCLR\n",
    "class SimCLRDataset(Dataset):\n",
    "    def __init__(self, images, transform):\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        img_1 = self.transform(img)\n",
    "        img_2 = self.transform(img)\n",
    "        return img_1, img_2\n",
    "\n",
    "train_dataset = SimCLRDataset(final_X_train_resized, transform_simclr)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "# Define SimCLR Model\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, base_encoder, projection_dim):\n",
    "        super(SimCLR, self).__init__()\n",
    "        self.encoder = base_encoder\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),  # Reduce dimensions here\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, projection_dim)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        z = self.projector(h)\n",
    "        return z\n",
    "\n",
    "# Define NT-Xent Loss\n",
    "class NTXentLoss(nn.Module):\n",
    "    def __init__(self, temperature):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        N = z_i.size(0) + z_j.size(0)\n",
    "        z = torch.cat((z_i, z_j), dim=0)\n",
    "        sim = torch.mm(z, z.T) / self.temperature\n",
    "        sim = torch.nn.functional.softmax(sim, dim=1)\n",
    "\n",
    "        labels = torch.cat([\n",
    "            torch.arange(z_i.size(0), device=z.device),\n",
    "            torch.arange(z_j.size(0), device=z.device)\n",
    "        ])\n",
    "        loss = self.criterion(sim, labels)\n",
    "        return loss\n",
    "\n",
    "# Initialize ResNet-50 Encoder with updated weights argument\n",
    "base_encoder = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "base_encoder.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "base_encoder.fc = nn.Identity()\n",
    "\n",
    "# Initialize SimCLR Model\n",
    "model = SimCLR(base_encoder, projection_dim=128).to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion = NTXentLoss(temperature=0.5)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "# Train SimCLR Model\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for img_1, img_2 in train_loader:\n",
    "        img_1, img_2 = img_1.to(\"cuda\"), img_2.to(\"cuda\")\n",
    "        z_i = model(img_1)\n",
    "        z_j = model(img_2)\n",
    "\n",
    "        loss = criterion(z_i, z_j)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch [{epoch+1}/100], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Define Dataset for Classification\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# Initialize Training and Test Dataset and DataLoader\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "train_dataset = TestDataset(final_X_train_resized, train_labels, transform=train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = TestDataset(final_X_test_resized, test_labels, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Add Classification Head\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "classification_head = ClassificationHead(input_dim=2048, num_classes=len(np.unique(train_labels))).to(\"cuda\")\n",
    "optimizer_cls = optim.Adam([\n",
    "    {\"params\": model.encoder.parameters(), \"lr\": 1e-5},\n",
    "    {\"params\": classification_head.parameters(), \"lr\": 3e-4},\n",
    "])\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "\n",
    "# Fine-tune Classification Head\n",
    "for epoch in range(100):\n",
    "    model.encoder.train()\n",
    "    classification_head.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for img, label in train_loader:\n",
    "        img, label = img.to(\"cuda\"), label.to(\"cuda\")\n",
    "        features = model.encoder(img)\n",
    "        logits = classification_head(features)\n",
    "        loss = criterion_cls(logits, label)\n",
    "\n",
    "        optimizer_cls.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_cls.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct += (logits.argmax(dim=1) == label).sum().item()\n",
    "\n",
    "    accuracy = correct / len(train_labels)\n",
    "    print(f\"Epoch [{epoch+1}/100], Loss: {total_loss/len(train_loader):.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate on Test Dataset\n",
    "classification_head.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for img, label in test_loader:\n",
    "        img, label = img.to(\"cuda\"), label.to(\"cuda\")\n",
    "        features = model.encoder(img)\n",
    "        logits = classification_head(features)\n",
    "        correct += (logits.argmax(dim=1) == label).sum().item()\n",
    "\n",
    "test_accuracy = correct / len(test_labels)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d35553b-5bd9-4b45-abdd-b657fdc72197",
   "metadata": {},
   "source": [
    "# The Below one got 90% accuracy, with ResNet18\n",
    "\n",
    "# SimCLR Implementation Explanation\n",
    "\n",
    "## **1. Data Preprocessing and Augmentation**\n",
    "- **Normalization and Resizing**:\n",
    "  - Images are resized to `224x224` using Lanczos resampling and normalized with a mean of `0.5` and standard deviation of `0.5`.\n",
    "- **Data Augmentation**:\n",
    "  - Augmentation pipeline includes:\n",
    "    - `RandomResizedCrop`: Crops images to random sizes and aspect ratios.\n",
    "    - `RandomHorizontalFlip`: Randomly flips the image horizontally.\n",
    "    - `ColorJitter`: Randomly changes brightness, contrast, saturation, and hue.\n",
    "    - `GaussianBlur`: Blurs the image with a random kernel size and sigma.\n",
    "  - These augmentations help generate two different views of the same image, which is crucial for contrastive learning.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Custom Dataset**\n",
    "- **SimCLRDataset**:\n",
    "  - Generates two augmented views of each input image to be used as positive pairs for contrastive learning.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. SimCLR Model Architecture**\n",
    "- **Base Encoder**:\n",
    "  - A pre-trained `ResNet-18` is used as the backbone.\n",
    "  - The first convolutional layer (`conv1`) is modified to handle single-channel (grayscale) images.\n",
    "  - The fully connected layer (`fc`) is replaced with an `Identity` layer to output feature embeddings.\n",
    "- **Projection Head**:\n",
    "  - A two-layer MLP with a hidden size of `256` and output size of `128`.\n",
    "  - Nonlinear activation (`ReLU`) is applied between the layers.\n",
    "  - The projection head helps learn representations better suited for contrastive loss.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Contrastive Loss (NT-Xent Loss)**\n",
    "- **Normalized Temperature-Scaled Cross-Entropy Loss**:\n",
    "  - Encourages the model to maximize the similarity of positive pairs (different views of the same image) while minimizing similarity to negative pairs (different images in the batch).\n",
    "  - Implements temperature scaling and softmax normalization for better gradient flow.\n",
    "  - Cross-entropy loss is used to optimize the logits.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Training Procedure**\n",
    "- **Contrastive Learning Stage**:\n",
    "  - The SimCLR model is trained using the contrastive loss on augmented image pairs.\n",
    "  - Key hyperparameters:\n",
    "    - Batch size: `512`\n",
    "    - Learning rate: `1e-4`\n",
    "    - Temperature: `0.5`\n",
    "  - The Adam optimizer is used with a `StepLR` scheduler for learning rate decay.\n",
    "- **Training Outputs**:\n",
    "  - Loss values are logged for each epoch to monitor the learning process.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Downstream Task: Classification**\n",
    "- **Linear Evaluation Protocol**:\n",
    "  - The pre-trained encoder is fine-tuned with a classification head for supervised learning on the labeled dataset.\n",
    "  - Classification head:\n",
    "    - A single linear layer mapping encoder outputs (`512`) to the number of classes.\n",
    "  - Fine-tuning:\n",
    "    - Encoder weights are updated at a slower learning rate (`1e-5`) than the classification head (`1e-3`).\n",
    "    - Cross-entropy loss is used for optimization.\n",
    "- **Performance Evaluation**:\n",
    "  - Test accuracy is computed on the held-out test set to measure the model's performance.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Key Features Contributing to Accuracy**\n",
    "1. **Data Augmentation**:\n",
    "   - Rich transformations ensure the model learns robust and invariant features.\n",
    "2. **Pre-trained ResNet-18 Encoder**:\n",
    "   - Provides a strong starting point for feature extraction.\n",
    "3. **Projection Head**:\n",
    "   - Helps improve representation learning by mapping features to a contrastive space.\n",
    "4. **Contrastive Loss**:\n",
    "   - NT-Xent loss ensures the learned features are discriminative and generalizable.\n",
    "5. **Fine-tuning Strategy**:\n",
    "   - Gradual learning rate decay and careful tuning of encoder and head ensure effective adaptation to the downstream task.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b13db38a-11d6-4b1c-97ed-a3f34551607c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 8.1232\n",
      "Epoch [2/100], Loss: 6.6977\n",
      "Epoch [3/100], Loss: 6.5628\n",
      "Epoch [4/100], Loss: 6.4204\n",
      "Epoch [5/100], Loss: 6.2939\n",
      "Epoch [6/100], Loss: 6.2331\n",
      "Epoch [7/100], Loss: 6.1769\n",
      "Epoch [8/100], Loss: 6.0556\n",
      "Epoch [9/100], Loss: 5.9819\n",
      "Epoch [10/100], Loss: 5.8945\n",
      "Epoch [11/100], Loss: 5.7674\n",
      "Epoch [12/100], Loss: 5.6089\n",
      "Epoch [13/100], Loss: 5.5685\n",
      "Epoch [14/100], Loss: 5.4512\n",
      "Epoch [15/100], Loss: 5.3110\n",
      "Epoch [16/100], Loss: 5.1675\n",
      "Epoch [17/100], Loss: 5.1007\n",
      "Epoch [18/100], Loss: 4.9024\n",
      "Epoch [19/100], Loss: 4.8179\n",
      "Epoch [20/100], Loss: 4.7292\n",
      "Epoch [21/100], Loss: 4.6493\n",
      "Epoch [22/100], Loss: 4.5762\n",
      "Epoch [23/100], Loss: 4.4276\n",
      "Epoch [24/100], Loss: 4.3101\n",
      "Epoch [25/100], Loss: 4.2424\n",
      "Epoch [26/100], Loss: 4.1504\n",
      "Epoch [27/100], Loss: 4.0430\n",
      "Epoch [28/100], Loss: 3.9242\n",
      "Epoch [29/100], Loss: 3.8458\n",
      "Epoch [30/100], Loss: 3.7267\n",
      "Epoch [31/100], Loss: 3.8131\n",
      "Epoch [32/100], Loss: 3.7184\n",
      "Epoch [33/100], Loss: 3.6482\n",
      "Epoch [34/100], Loss: 3.5053\n",
      "Epoch [35/100], Loss: 3.5145\n",
      "Epoch [36/100], Loss: 3.5030\n",
      "Epoch [37/100], Loss: 3.3975\n",
      "Epoch [38/100], Loss: 3.4146\n",
      "Epoch [39/100], Loss: 3.3098\n",
      "Epoch [40/100], Loss: 3.2722\n",
      "Epoch [41/100], Loss: 3.3186\n",
      "Epoch [42/100], Loss: 3.2364\n",
      "Epoch [43/100], Loss: 3.2709\n",
      "Epoch [44/100], Loss: 3.1460\n",
      "Epoch [45/100], Loss: 3.1851\n",
      "Epoch [46/100], Loss: 3.0631\n",
      "Epoch [47/100], Loss: 3.0215\n",
      "Epoch [48/100], Loss: 2.9695\n",
      "Epoch [49/100], Loss: 3.0149\n",
      "Epoch [50/100], Loss: 2.9137\n",
      "Epoch [51/100], Loss: 2.9919\n",
      "Epoch [52/100], Loss: 2.9731\n",
      "Epoch [53/100], Loss: 2.9027\n",
      "Epoch [54/100], Loss: 2.9106\n",
      "Epoch [55/100], Loss: 2.7613\n",
      "Epoch [56/100], Loss: 2.8544\n",
      "Epoch [57/100], Loss: 2.7912\n",
      "Epoch [58/100], Loss: 2.8075\n",
      "Epoch [59/100], Loss: 2.8088\n",
      "Epoch [60/100], Loss: 2.7960\n",
      "Epoch [61/100], Loss: 2.7302\n",
      "Epoch [62/100], Loss: 2.6979\n",
      "Epoch [63/100], Loss: 2.7768\n",
      "Epoch [64/100], Loss: 2.5908\n",
      "Epoch [65/100], Loss: 2.6183\n",
      "Epoch [66/100], Loss: 2.6636\n",
      "Epoch [67/100], Loss: 2.7368\n",
      "Epoch [68/100], Loss: 2.5110\n",
      "Epoch [69/100], Loss: 2.5872\n",
      "Epoch [70/100], Loss: 2.5331\n",
      "Epoch [71/100], Loss: 2.6693\n",
      "Epoch [72/100], Loss: 2.6570\n",
      "Epoch [73/100], Loss: 2.6254\n",
      "Epoch [74/100], Loss: 2.6197\n",
      "Epoch [75/100], Loss: 2.6155\n",
      "Epoch [76/100], Loss: 2.5663\n",
      "Epoch [77/100], Loss: 2.5080\n",
      "Epoch [78/100], Loss: 2.4358\n",
      "Epoch [79/100], Loss: 2.4760\n",
      "Epoch [80/100], Loss: 2.4601\n",
      "Epoch [81/100], Loss: 2.5365\n",
      "Epoch [82/100], Loss: 2.5075\n",
      "Epoch [83/100], Loss: 2.3272\n",
      "Epoch [84/100], Loss: 2.3784\n",
      "Epoch [85/100], Loss: 2.4384\n",
      "Epoch [86/100], Loss: 2.3695\n",
      "Epoch [87/100], Loss: 2.4377\n",
      "Epoch [88/100], Loss: 2.4123\n",
      "Epoch [89/100], Loss: 2.4479\n",
      "Epoch [90/100], Loss: 2.4060\n",
      "Epoch [91/100], Loss: 2.3577\n",
      "Epoch [92/100], Loss: 2.3522\n",
      "Epoch [93/100], Loss: 2.4021\n",
      "Epoch [94/100], Loss: 2.4130\n",
      "Epoch [95/100], Loss: 2.3261\n",
      "Epoch [96/100], Loss: 2.4397\n",
      "Epoch [97/100], Loss: 2.3650\n",
      "Epoch [98/100], Loss: 2.2320\n",
      "Epoch [99/100], Loss: 2.3905\n",
      "Epoch [100/100], Loss: 2.2970\n",
      "Epoch [1/100], Loss: 1.0364, Accuracy: 0.4568\n",
      "Epoch [2/100], Loss: 0.9136, Accuracy: 0.5673\n",
      "Epoch [3/100], Loss: 0.8605, Accuracy: 0.6150\n",
      "Epoch [4/100], Loss: 0.7857, Accuracy: 0.6693\n",
      "Epoch [5/100], Loss: 0.7083, Accuracy: 0.7206\n",
      "Epoch [6/100], Loss: 0.6258, Accuracy: 0.7719\n",
      "Epoch [7/100], Loss: 0.5381, Accuracy: 0.8159\n",
      "Epoch [8/100], Loss: 0.4524, Accuracy: 0.8582\n",
      "Epoch [9/100], Loss: 0.3609, Accuracy: 0.9028\n",
      "Epoch [10/100], Loss: 0.2725, Accuracy: 0.9390\n",
      "Epoch [11/100], Loss: 0.2193, Accuracy: 0.9559\n",
      "Epoch [12/100], Loss: 0.1906, Accuracy: 0.9662\n",
      "Epoch [13/100], Loss: 0.1556, Accuracy: 0.9819\n",
      "Epoch [14/100], Loss: 0.1338, Accuracy: 0.9867\n",
      "Epoch [15/100], Loss: 0.1124, Accuracy: 0.9922\n",
      "Epoch [16/100], Loss: 0.0980, Accuracy: 0.9934\n",
      "Epoch [17/100], Loss: 0.0812, Accuracy: 0.9952\n",
      "Epoch [18/100], Loss: 0.0727, Accuracy: 0.9976\n",
      "Epoch [19/100], Loss: 0.0609, Accuracy: 0.9976\n",
      "Epoch [20/100], Loss: 0.0494, Accuracy: 0.9988\n",
      "Epoch [21/100], Loss: 0.0446, Accuracy: 0.9988\n",
      "Epoch [22/100], Loss: 0.0400, Accuracy: 0.9994\n",
      "Epoch [23/100], Loss: 0.0377, Accuracy: 0.9988\n",
      "Epoch [24/100], Loss: 0.0380, Accuracy: 0.9994\n",
      "Epoch [25/100], Loss: 0.0358, Accuracy: 0.9994\n",
      "Epoch [26/100], Loss: 0.0309, Accuracy: 0.9994\n",
      "Epoch [27/100], Loss: 0.0302, Accuracy: 0.9994\n",
      "Epoch [28/100], Loss: 0.0278, Accuracy: 0.9994\n",
      "Epoch [29/100], Loss: 0.0262, Accuracy: 0.9994\n",
      "Epoch [30/100], Loss: 0.0250, Accuracy: 0.9994\n",
      "Epoch [31/100], Loss: 0.0229, Accuracy: 0.9994\n",
      "Epoch [32/100], Loss: 0.0224, Accuracy: 0.9994\n",
      "Epoch [33/100], Loss: 0.0227, Accuracy: 0.9994\n",
      "Epoch [34/100], Loss: 0.0203, Accuracy: 0.9994\n",
      "Epoch [35/100], Loss: 0.0194, Accuracy: 1.0000\n",
      "Epoch [36/100], Loss: 0.0192, Accuracy: 0.9994\n",
      "Epoch [37/100], Loss: 0.0183, Accuracy: 1.0000\n",
      "Epoch [38/100], Loss: 0.0178, Accuracy: 1.0000\n",
      "Epoch [39/100], Loss: 0.0183, Accuracy: 1.0000\n",
      "Epoch [40/100], Loss: 0.0184, Accuracy: 1.0000\n",
      "Epoch [41/100], Loss: 0.0171, Accuracy: 1.0000\n",
      "Epoch [42/100], Loss: 0.0171, Accuracy: 1.0000\n",
      "Epoch [43/100], Loss: 0.0171, Accuracy: 1.0000\n",
      "Epoch [44/100], Loss: 0.0158, Accuracy: 1.0000\n",
      "Epoch [45/100], Loss: 0.0151, Accuracy: 1.0000\n",
      "Epoch [46/100], Loss: 0.0144, Accuracy: 1.0000\n",
      "Epoch [47/100], Loss: 0.0164, Accuracy: 1.0000\n",
      "Epoch [48/100], Loss: 0.0155, Accuracy: 1.0000\n",
      "Epoch [49/100], Loss: 0.0138, Accuracy: 1.0000\n",
      "Epoch [50/100], Loss: 0.0151, Accuracy: 1.0000\n",
      "Epoch [51/100], Loss: 0.0139, Accuracy: 1.0000\n",
      "Epoch [52/100], Loss: 0.0135, Accuracy: 1.0000\n",
      "Epoch [53/100], Loss: 0.0142, Accuracy: 1.0000\n",
      "Epoch [54/100], Loss: 0.0147, Accuracy: 1.0000\n",
      "Epoch [55/100], Loss: 0.0150, Accuracy: 1.0000\n",
      "Epoch [56/100], Loss: 0.0145, Accuracy: 1.0000\n",
      "Epoch [57/100], Loss: 0.0143, Accuracy: 1.0000\n",
      "Epoch [58/100], Loss: 0.0151, Accuracy: 1.0000\n",
      "Epoch [59/100], Loss: 0.0124, Accuracy: 1.0000\n",
      "Epoch [60/100], Loss: 0.0128, Accuracy: 1.0000\n",
      "Epoch [61/100], Loss: 0.0125, Accuracy: 1.0000\n",
      "Epoch [62/100], Loss: 0.0125, Accuracy: 1.0000\n",
      "Epoch [63/100], Loss: 0.0125, Accuracy: 1.0000\n",
      "Epoch [64/100], Loss: 0.0124, Accuracy: 1.0000\n",
      "Epoch [65/100], Loss: 0.0127, Accuracy: 1.0000\n",
      "Epoch [66/100], Loss: 0.0121, Accuracy: 1.0000\n",
      "Epoch [67/100], Loss: 0.0136, Accuracy: 1.0000\n",
      "Epoch [68/100], Loss: 0.0117, Accuracy: 1.0000\n",
      "Epoch [69/100], Loss: 0.0125, Accuracy: 1.0000\n",
      "Epoch [70/100], Loss: 0.0123, Accuracy: 1.0000\n",
      "Epoch [71/100], Loss: 0.0129, Accuracy: 1.0000\n",
      "Epoch [72/100], Loss: 0.0122, Accuracy: 1.0000\n",
      "Epoch [73/100], Loss: 0.0125, Accuracy: 1.0000\n",
      "Epoch [74/100], Loss: 0.0130, Accuracy: 1.0000\n",
      "Epoch [75/100], Loss: 0.0125, Accuracy: 1.0000\n",
      "Epoch [76/100], Loss: 0.0132, Accuracy: 1.0000\n",
      "Epoch [77/100], Loss: 0.0128, Accuracy: 1.0000\n",
      "Epoch [78/100], Loss: 0.0127, Accuracy: 1.0000\n",
      "Epoch [79/100], Loss: 0.0122, Accuracy: 1.0000\n",
      "Epoch [80/100], Loss: 0.0123, Accuracy: 1.0000\n",
      "Epoch [81/100], Loss: 0.0118, Accuracy: 1.0000\n",
      "Epoch [82/100], Loss: 0.0120, Accuracy: 1.0000\n",
      "Epoch [83/100], Loss: 0.0120, Accuracy: 1.0000\n",
      "Epoch [84/100], Loss: 0.0120, Accuracy: 1.0000\n",
      "Epoch [85/100], Loss: 0.0131, Accuracy: 1.0000\n",
      "Epoch [86/100], Loss: 0.0118, Accuracy: 1.0000\n",
      "Epoch [87/100], Loss: 0.0125, Accuracy: 1.0000\n",
      "Epoch [88/100], Loss: 0.0145, Accuracy: 1.0000\n",
      "Epoch [89/100], Loss: 0.0117, Accuracy: 1.0000\n",
      "Epoch [90/100], Loss: 0.0116, Accuracy: 1.0000\n",
      "Epoch [91/100], Loss: 0.0121, Accuracy: 1.0000\n",
      "Epoch [92/100], Loss: 0.0137, Accuracy: 1.0000\n",
      "Epoch [93/100], Loss: 0.0131, Accuracy: 1.0000\n",
      "Epoch [94/100], Loss: 0.0124, Accuracy: 1.0000\n",
      "Epoch [95/100], Loss: 0.0127, Accuracy: 1.0000\n",
      "Epoch [96/100], Loss: 0.0128, Accuracy: 1.0000\n",
      "Epoch [97/100], Loss: 0.0117, Accuracy: 1.0000\n",
      "Epoch [98/100], Loss: 0.0114, Accuracy: 1.0000\n",
      "Epoch [99/100], Loss: 0.0126, Accuracy: 1.0000\n",
      "Epoch [100/100], Loss: 0.0116, Accuracy: 1.0000\n",
      "Test Accuracy: 0.9044\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "train_images_path = \"brain_train_image_final.npy\"\n",
    "train_labels_path = \"brain_train_label.npy\"\n",
    "test_images_path = \"brain_test_image_final.npy\"\n",
    "test_labels_path = \"brain_test_label.npy\"\n",
    "\n",
    "# Load the data\n",
    "final_X_train_modified = np.load(train_images_path)[:, 1, :, :]\n",
    "final_X_test_modified = np.load(test_images_path)[:, 1, :, :]\n",
    "train_labels = np.load(train_labels_path)\n",
    "test_labels = np.load(test_labels_path)\n",
    "\n",
    "# Normalize and Resize Images using Pillow\n",
    "def normalize_and_resize(images, target_size=(224, 224)):\n",
    "    resized_images = []\n",
    "    for img in images:\n",
    "        img = Image.fromarray((img * 255).astype(np.uint8))\n",
    "        img_resized = img.resize(target_size, Image.Resampling.LANCZOS)\n",
    "        resized_images.append(np.array(img_resized) / 255.0)\n",
    "    return np.array(resized_images)\n",
    "\n",
    "final_X_train_resized = normalize_and_resize(final_X_train_modified)\n",
    "final_X_test_resized = normalize_and_resize(final_X_test_modified)\n",
    "\n",
    "# Define SimCLR Augmentation Transform\n",
    "transform_simclr = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomResizedCrop(size=224),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1),\n",
    "    transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Custom Dataset for SimCLR\n",
    "class SimCLRDataset(Dataset):\n",
    "    def __init__(self, images, transform):\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        img_1 = self.transform(img)\n",
    "        img_2 = self.transform(img)\n",
    "        return img_1, img_2\n",
    "\n",
    "train_dataset = SimCLRDataset(final_X_train_resized, transform_simclr)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "# Define SimCLR Model\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, base_encoder, projection_dim):\n",
    "        super(SimCLR, self).__init__()\n",
    "        self.encoder = base_encoder\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, projection_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        z = self.projector(h)\n",
    "        return z\n",
    "\n",
    "# Define NT-Xent Loss\n",
    "class NTXentLoss(nn.Module):\n",
    "    def __init__(self, batch_size, temperature):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        N = z_i.size(0) + z_j.size(0)\n",
    "        z = torch.cat((z_i, z_j), dim=0)\n",
    "        sim = torch.matmul(z, z.T) / self.temperature\n",
    "        mask = ~torch.eye(N, dtype=torch.bool, device=z.device)\n",
    "\n",
    "        positives = torch.cat([\n",
    "            torch.diag(sim, z_i.size(0)),\n",
    "            torch.diag(sim, -z_i.size(0))\n",
    "        ])\n",
    "\n",
    "        negatives = sim[mask].view(N, -1)\n",
    "        logits = torch.cat((positives.unsqueeze(1), negatives), dim=1)\n",
    "        labels = torch.zeros(N, dtype=torch.long, device=z.device)\n",
    "        loss = self.criterion(logits, labels) / N\n",
    "        return loss\n",
    "\n",
    "# Initialize ResNet-18 Encoder\n",
    "base_encoder = resnet18(pretrained=True)\n",
    "base_encoder.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "base_encoder.fc = nn.Identity()\n",
    "\n",
    "# Initialize SimCLR Model\n",
    "model = SimCLR(base_encoder, projection_dim=128).to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = NTXentLoss(batch_size=512, temperature=0.5)\n",
    "\n",
    "# Train SimCLR Model\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for img_1, img_2 in train_loader:\n",
    "        img_1, img_2 = img_1.to(\"cuda\"), img_2.to(\"cuda\")\n",
    "        z_i = model(img_1)\n",
    "        z_j = model(img_2)\n",
    "\n",
    "        loss = criterion(z_i, z_j)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/100], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    \n",
    "\n",
    "# Define Dataset for Classification\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# Initialize Training and Test Dataset and DataLoader\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "train_dataset = TestDataset(final_X_train_resized, train_labels, transform=train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "test_dataset = TestDataset(final_X_test_resized, test_labels, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Add Classification Head\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "classification_head = ClassificationHead(input_dim=512, num_classes=len(np.unique(train_labels))).to(\"cuda\")\n",
    "optimizer_cls = optim.Adam([\n",
    "    {\"params\": model.encoder.parameters(), \"lr\": 1e-5},\n",
    "    {\"params\": classification_head.parameters(), \"lr\": 1e-3},\n",
    "])\n",
    "scheduler_cls = StepLR(optimizer_cls, step_size=10, gamma=0.5)\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "\n",
    "# Fine-tune Classification Head\n",
    "for epoch in range(100):\n",
    "    model.encoder.train()\n",
    "    classification_head.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for img, label in DataLoader(train_dataset, batch_size=128, shuffle=True):\n",
    "        img, label = img.to(\"cuda\"), label.to(\"cuda\")\n",
    "        features = model.encoder(img)\n",
    "        logits = classification_head(features)\n",
    "        loss = criterion_cls(logits, label)\n",
    "\n",
    "        optimizer_cls.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_cls.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct += (logits.argmax(dim=1) == label).sum().item()\n",
    "\n",
    "    accuracy = correct / len(train_labels)\n",
    "    scheduler_cls.step()\n",
    "    print(f\"Epoch [{epoch+1}/100], Loss: {total_loss/len(train_loader):.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate on Test Dataset\n",
    "classification_head.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for img, label in DataLoader(test_dataset, batch_size=128, shuffle=False):\n",
    "        img, label = img.to(\"cuda\"), label.to(\"cuda\")\n",
    "        features = model.encoder(img)\n",
    "        logits = classification_head(features)\n",
    "        correct += (logits.argmax(dim=1) == label).sum().item()\n",
    "\n",
    "test_accuracy = correct / len(test_labels)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1455e541-f986-47fd-a45f-b0e61d7d0461",
   "metadata": {},
   "source": [
    "# The below one got 85% accuracy, with ResNet34\n",
    "\n",
    "# SimCLR Implementation with ResNet-34\n",
    "\n",
    "## **1. Data Preprocessing and Augmentation**\n",
    "- **Normalization and Resizing**:\n",
    "  - Input images are resized to `224x224` and normalized with a mean of `0.5` and standard deviation of `0.5` for compatibility with the ResNet-34 model.\n",
    "- **Data Augmentation**:\n",
    "  - Rich augmentation pipeline includes:\n",
    "    - `RandomResizedCrop`: Crops images randomly with varying sizes and aspect ratios.\n",
    "    - `RandomHorizontalFlip`: Flips the image horizontally with a probability of `0.5`.\n",
    "    - `ColorJitter`: Modifies brightness, contrast, saturation, and hue to simulate various lighting conditions.\n",
    "    - `GaussianBlur`: Adds random blur to images for robustness.\n",
    "  - These augmentations generate two augmented views of the same image, ensuring robust and invariant feature learning.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Custom Dataset**\n",
    "- **SimCLRDataset**:\n",
    "  - Provides two augmented views of each image to act as positive pairs for contrastive learning.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. SimCLR Model Architecture**\n",
    "- **Base Encoder**:\n",
    "  - A pre-trained `ResNet-34` backbone from ImageNet is used for feature extraction.\n",
    "  - Input channels are modified to handle single-channel (grayscale) images.\n",
    "  - The fully connected (`fc`) layer is replaced with an `Identity` layer to output raw feature embeddings.\n",
    "- **Projection Head**:\n",
    "  - A two-layer MLP maps feature embeddings to a lower-dimensional space:\n",
    "    - First layer: `Linear(512, 512)` followed by `ReLU`.\n",
    "    - Second layer: `Linear(512, 128)` for projection.\n",
    "  - This helps learn more effective representations suited for contrastive learning.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Contrastive Loss (NT-Xent Loss)**\n",
    "- **Normalized Temperature-Scaled Cross-Entropy Loss**:\n",
    "  - Ensures that the similarity between positive pairs is maximized while minimizing similarity to negative pairs within the same batch.\n",
    "  - Applies temperature scaling and uses softmax normalization for improved gradient flow.\n",
    "  - Cross-entropy loss is used to optimize the similarity logits.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Training Procedure**\n",
    "- **Contrastive Learning Stage**:\n",
    "  - The SimCLR model is trained on the augmented pairs using contrastive loss.\n",
    "  - Key hyperparameters:\n",
    "    - Batch size: `512`\n",
    "    - Learning rate: `3e-4`\n",
    "    - Temperature: `0.5`\n",
    "  - Optimizer: Adam\n",
    "  - Scheduler: `CosineAnnealingLR` with `T_max=100` for smooth learning rate decay.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Downstream Task: Classification**\n",
    "- **Linear Evaluation Protocol**:\n",
    "  - The pre-trained encoder is fine-tuned with a linear classification head for the labeled dataset.\n",
    "  - Classification head:\n",
    "    - A single `Linear(512, num_classes)` layer maps encoder features to class logits.\n",
    "  - Fine-tuning strategy:\n",
    "    - Encoder weights updated at a slower learning rate (`1e-5`) compared to the classification head (`3e-4`).\n",
    "    - Cross-entropy loss is used for optimization.\n",
    "- **Performance Evaluation**:\n",
    "  - Accuracy on the test set is computed to evaluate model performance.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Key Features Contributing to Accuracy**\n",
    "1. **Data Augmentation**:\n",
    "   - Extensive augmentations ensure the model learns invariant features that generalize well.\n",
    "2. **ResNet-34 Encoder**:\n",
    "   - A pre-trained encoder offers robust initial feature representations.\n",
    "3. **Projection Head**:\n",
    "   - A low-dimensional mapping improves representation quality for contrastive learning.\n",
    "4. **Contrastive Loss**:\n",
    "   - Effectively encourages discriminative feature learning.\n",
    "5. **Large Batch Size**:\n",
    "   - Provides a diverse set of negative samples, enhancing contrastive loss effectiveness.\n",
    "6. **Cosine Learning Rate Scheduler**:\n",
    "   - Smooth decay of the learning rate ensures stable training.\n",
    "7. **Fine-tuning Protocol**:\n",
    "   - Differential learning rates for the encoder and classification head improve adaptation for downstream tasks.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09e7e3ce-eff8-460e-aced-e3ccd9d25b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 6.3819\n",
      "Epoch [2/100], Loss: 6.1793\n",
      "Epoch [3/100], Loss: 6.1024\n",
      "Epoch [4/100], Loss: 6.0910\n",
      "Epoch [5/100], Loss: 6.0812\n",
      "Epoch [6/100], Loss: 6.0800\n",
      "Epoch [7/100], Loss: 6.0776\n",
      "Epoch [8/100], Loss: 6.0784\n",
      "Epoch [9/100], Loss: 6.0793\n",
      "Epoch [10/100], Loss: 6.0791\n",
      "Epoch [11/100], Loss: 6.0776\n",
      "Epoch [12/100], Loss: 6.0777\n",
      "Epoch [13/100], Loss: 6.0779\n",
      "Epoch [14/100], Loss: 6.0781\n",
      "Epoch [15/100], Loss: 6.0788\n",
      "Epoch [16/100], Loss: 6.0781\n",
      "Epoch [17/100], Loss: 6.0792\n",
      "Epoch [18/100], Loss: 6.0778\n",
      "Epoch [19/100], Loss: 6.0781\n",
      "Epoch [20/100], Loss: 6.0769\n",
      "Epoch [21/100], Loss: 6.0781\n",
      "Epoch [22/100], Loss: 6.0765\n",
      "Epoch [23/100], Loss: 6.0763\n",
      "Epoch [24/100], Loss: 6.0758\n",
      "Epoch [25/100], Loss: 6.0762\n",
      "Epoch [26/100], Loss: 6.0758\n",
      "Epoch [27/100], Loss: 6.0760\n",
      "Epoch [28/100], Loss: 6.0758\n",
      "Epoch [29/100], Loss: 6.0762\n",
      "Epoch [30/100], Loss: 6.0770\n",
      "Epoch [31/100], Loss: 6.0752\n",
      "Epoch [32/100], Loss: 6.0767\n",
      "Epoch [33/100], Loss: 6.0752\n",
      "Epoch [34/100], Loss: 6.0766\n",
      "Epoch [35/100], Loss: 6.0759\n",
      "Epoch [36/100], Loss: 6.0756\n",
      "Epoch [37/100], Loss: 6.0752\n",
      "Epoch [38/100], Loss: 6.0748\n",
      "Epoch [39/100], Loss: 6.0746\n",
      "Epoch [40/100], Loss: 6.0751\n",
      "Epoch [41/100], Loss: 6.0748\n",
      "Epoch [42/100], Loss: 6.0747\n",
      "Epoch [43/100], Loss: 6.0747\n",
      "Epoch [44/100], Loss: 6.0746\n",
      "Epoch [45/100], Loss: 6.0744\n",
      "Epoch [46/100], Loss: 6.0746\n",
      "Epoch [47/100], Loss: 6.0747\n",
      "Epoch [48/100], Loss: 6.0744\n",
      "Epoch [49/100], Loss: 6.0743\n",
      "Epoch [50/100], Loss: 6.0749\n",
      "Epoch [51/100], Loss: 6.0741\n",
      "Epoch [52/100], Loss: 6.0750\n",
      "Epoch [53/100], Loss: 6.0747\n",
      "Epoch [54/100], Loss: 6.0745\n",
      "Epoch [55/100], Loss: 6.0748\n",
      "Epoch [56/100], Loss: 6.0743\n",
      "Epoch [57/100], Loss: 6.0741\n",
      "Epoch [58/100], Loss: 6.0741\n",
      "Epoch [59/100], Loss: 6.0747\n",
      "Epoch [60/100], Loss: 6.0743\n",
      "Epoch [61/100], Loss: 6.0744\n",
      "Epoch [62/100], Loss: 6.0741\n",
      "Epoch [63/100], Loss: 6.0741\n",
      "Epoch [64/100], Loss: 6.0742\n",
      "Epoch [65/100], Loss: 6.0743\n",
      "Epoch [66/100], Loss: 6.0741\n",
      "Epoch [67/100], Loss: 6.0742\n",
      "Epoch [68/100], Loss: 6.0740\n",
      "Epoch [69/100], Loss: 6.0740\n",
      "Epoch [70/100], Loss: 6.0740\n",
      "Epoch [71/100], Loss: 6.0741\n",
      "Epoch [72/100], Loss: 6.0744\n",
      "Epoch [73/100], Loss: 6.0742\n",
      "Epoch [74/100], Loss: 6.0740\n",
      "Epoch [75/100], Loss: 6.0741\n",
      "Epoch [76/100], Loss: 6.0744\n",
      "Epoch [77/100], Loss: 6.0740\n",
      "Epoch [78/100], Loss: 6.0740\n",
      "Epoch [79/100], Loss: 6.0740\n",
      "Epoch [80/100], Loss: 6.0743\n",
      "Epoch [81/100], Loss: 6.0740\n",
      "Epoch [82/100], Loss: 6.0740\n",
      "Epoch [83/100], Loss: 6.0742\n",
      "Epoch [84/100], Loss: 6.0741\n",
      "Epoch [85/100], Loss: 6.0740\n",
      "Epoch [86/100], Loss: 6.0740\n",
      "Epoch [87/100], Loss: 6.0740\n",
      "Epoch [88/100], Loss: 6.0740\n",
      "Epoch [89/100], Loss: 6.0740\n",
      "Epoch [90/100], Loss: 6.0741\n",
      "Epoch [91/100], Loss: 6.0740\n",
      "Epoch [92/100], Loss: 6.0740\n",
      "Epoch [93/100], Loss: 6.0742\n",
      "Epoch [94/100], Loss: 6.0739\n",
      "Epoch [95/100], Loss: 6.0740\n",
      "Epoch [96/100], Loss: 6.0739\n",
      "Epoch [97/100], Loss: 6.0740\n",
      "Epoch [98/100], Loss: 6.0739\n",
      "Epoch [99/100], Loss: 6.0740\n",
      "Epoch [100/100], Loss: 6.0740\n",
      "Epoch [1/100], Loss: 1.2170, Accuracy: 0.3138\n",
      "Epoch [2/100], Loss: 0.9976, Accuracy: 0.5027\n",
      "Epoch [3/100], Loss: 0.9441, Accuracy: 0.5305\n",
      "Epoch [4/100], Loss: 0.8730, Accuracy: 0.5957\n",
      "Epoch [5/100], Loss: 0.7872, Accuracy: 0.6807\n",
      "Epoch [6/100], Loss: 0.7450, Accuracy: 0.7224\n",
      "Epoch [7/100], Loss: 0.6792, Accuracy: 0.7622\n",
      "Epoch [8/100], Loss: 0.6192, Accuracy: 0.8033\n",
      "Epoch [9/100], Loss: 0.5490, Accuracy: 0.8352\n",
      "Epoch [10/100], Loss: 0.4917, Accuracy: 0.8648\n",
      "Epoch [11/100], Loss: 0.4374, Accuracy: 0.8884\n",
      "Epoch [12/100], Loss: 0.3851, Accuracy: 0.9119\n",
      "Epoch [13/100], Loss: 0.3391, Accuracy: 0.9294\n",
      "Epoch [14/100], Loss: 0.2777, Accuracy: 0.9511\n",
      "Epoch [15/100], Loss: 0.2290, Accuracy: 0.9614\n",
      "Epoch [16/100], Loss: 0.1945, Accuracy: 0.9698\n",
      "Epoch [17/100], Loss: 0.1584, Accuracy: 0.9765\n",
      "Epoch [18/100], Loss: 0.1318, Accuracy: 0.9855\n",
      "Epoch [19/100], Loss: 0.1063, Accuracy: 0.9909\n",
      "Epoch [20/100], Loss: 0.0847, Accuracy: 0.9934\n",
      "Epoch [21/100], Loss: 0.0657, Accuracy: 0.9964\n",
      "Epoch [22/100], Loss: 0.0518, Accuracy: 0.9964\n",
      "Epoch [23/100], Loss: 0.0425, Accuracy: 0.9994\n",
      "Epoch [24/100], Loss: 0.0356, Accuracy: 1.0000\n",
      "Epoch [25/100], Loss: 0.0304, Accuracy: 1.0000\n",
      "Epoch [26/100], Loss: 0.0241, Accuracy: 1.0000\n",
      "Epoch [27/100], Loss: 0.0245, Accuracy: 1.0000\n",
      "Epoch [28/100], Loss: 0.0194, Accuracy: 1.0000\n",
      "Epoch [29/100], Loss: 0.0152, Accuracy: 1.0000\n",
      "Epoch [30/100], Loss: 0.0143, Accuracy: 1.0000\n",
      "Epoch [31/100], Loss: 0.0118, Accuracy: 1.0000\n",
      "Epoch [32/100], Loss: 0.0122, Accuracy: 1.0000\n",
      "Epoch [33/100], Loss: 0.0101, Accuracy: 1.0000\n",
      "Epoch [34/100], Loss: 0.0101, Accuracy: 1.0000\n",
      "Epoch [35/100], Loss: 0.0081, Accuracy: 1.0000\n",
      "Epoch [36/100], Loss: 0.0073, Accuracy: 1.0000\n",
      "Epoch [37/100], Loss: 0.0069, Accuracy: 1.0000\n",
      "Epoch [38/100], Loss: 0.0069, Accuracy: 1.0000\n",
      "Epoch [39/100], Loss: 0.0063, Accuracy: 1.0000\n",
      "Epoch [40/100], Loss: 0.0056, Accuracy: 1.0000\n",
      "Epoch [41/100], Loss: 0.0054, Accuracy: 1.0000\n",
      "Epoch [42/100], Loss: 0.0052, Accuracy: 1.0000\n",
      "Epoch [43/100], Loss: 0.0049, Accuracy: 1.0000\n",
      "Epoch [44/100], Loss: 0.0045, Accuracy: 1.0000\n",
      "Epoch [45/100], Loss: 0.0044, Accuracy: 1.0000\n",
      "Epoch [46/100], Loss: 0.0043, Accuracy: 1.0000\n",
      "Epoch [47/100], Loss: 0.0046, Accuracy: 1.0000\n",
      "Epoch [48/100], Loss: 0.0038, Accuracy: 1.0000\n",
      "Epoch [49/100], Loss: 0.0035, Accuracy: 1.0000\n",
      "Epoch [50/100], Loss: 0.0035, Accuracy: 1.0000\n",
      "Epoch [51/100], Loss: 0.0035, Accuracy: 1.0000\n",
      "Epoch [52/100], Loss: 0.0032, Accuracy: 1.0000\n",
      "Epoch [53/100], Loss: 0.0030, Accuracy: 1.0000\n",
      "Epoch [54/100], Loss: 0.0029, Accuracy: 1.0000\n",
      "Epoch [55/100], Loss: 0.0034, Accuracy: 1.0000\n",
      "Epoch [56/100], Loss: 0.0028, Accuracy: 1.0000\n",
      "Epoch [57/100], Loss: 0.0027, Accuracy: 1.0000\n",
      "Epoch [58/100], Loss: 0.0028, Accuracy: 1.0000\n",
      "Epoch [59/100], Loss: 0.0026, Accuracy: 1.0000\n",
      "Epoch [60/100], Loss: 0.0025, Accuracy: 1.0000\n",
      "Epoch [61/100], Loss: 0.0022, Accuracy: 1.0000\n",
      "Epoch [62/100], Loss: 0.0021, Accuracy: 1.0000\n",
      "Epoch [63/100], Loss: 0.0026, Accuracy: 1.0000\n",
      "Epoch [64/100], Loss: 0.0024, Accuracy: 1.0000\n",
      "Epoch [65/100], Loss: 0.0020, Accuracy: 1.0000\n",
      "Epoch [66/100], Loss: 0.0019, Accuracy: 1.0000\n",
      "Epoch [67/100], Loss: 0.0023, Accuracy: 1.0000\n",
      "Epoch [68/100], Loss: 0.0022, Accuracy: 1.0000\n",
      "Epoch [69/100], Loss: 0.0021, Accuracy: 1.0000\n",
      "Epoch [70/100], Loss: 0.0017, Accuracy: 1.0000\n",
      "Epoch [71/100], Loss: 0.0017, Accuracy: 1.0000\n",
      "Epoch [72/100], Loss: 0.0016, Accuracy: 1.0000\n",
      "Epoch [73/100], Loss: 0.0016, Accuracy: 1.0000\n",
      "Epoch [74/100], Loss: 0.0016, Accuracy: 1.0000\n",
      "Epoch [75/100], Loss: 0.0017, Accuracy: 1.0000\n",
      "Epoch [76/100], Loss: 0.0015, Accuracy: 1.0000\n",
      "Epoch [77/100], Loss: 0.0016, Accuracy: 1.0000\n",
      "Epoch [78/100], Loss: 0.0014, Accuracy: 1.0000\n",
      "Epoch [79/100], Loss: 0.0014, Accuracy: 1.0000\n",
      "Epoch [80/100], Loss: 0.0013, Accuracy: 1.0000\n",
      "Epoch [81/100], Loss: 0.0015, Accuracy: 1.0000\n",
      "Epoch [82/100], Loss: 0.0013, Accuracy: 1.0000\n",
      "Epoch [83/100], Loss: 0.0015, Accuracy: 1.0000\n",
      "Epoch [84/100], Loss: 0.0011, Accuracy: 1.0000\n",
      "Epoch [85/100], Loss: 0.0011, Accuracy: 1.0000\n",
      "Epoch [86/100], Loss: 0.0012, Accuracy: 1.0000\n",
      "Epoch [87/100], Loss: 0.0012, Accuracy: 1.0000\n",
      "Epoch [88/100], Loss: 0.0010, Accuracy: 1.0000\n",
      "Epoch [89/100], Loss: 0.0016, Accuracy: 1.0000\n",
      "Epoch [90/100], Loss: 0.0011, Accuracy: 1.0000\n",
      "Epoch [91/100], Loss: 0.0010, Accuracy: 1.0000\n",
      "Epoch [92/100], Loss: 0.0011, Accuracy: 1.0000\n",
      "Epoch [93/100], Loss: 0.0009, Accuracy: 1.0000\n",
      "Epoch [94/100], Loss: 0.0010, Accuracy: 1.0000\n",
      "Epoch [95/100], Loss: 0.0010, Accuracy: 1.0000\n",
      "Epoch [96/100], Loss: 0.0009, Accuracy: 1.0000\n",
      "Epoch [97/100], Loss: 0.0010, Accuracy: 1.0000\n",
      "Epoch [98/100], Loss: 0.0014, Accuracy: 1.0000\n",
      "Epoch [99/100], Loss: 0.0009, Accuracy: 1.0000\n",
      "Epoch [100/100], Loss: 0.0009, Accuracy: 1.0000\n",
      "Test Accuracy: 85.3242\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "train_images_path = \"brain_train_image_final.npy\"\n",
    "train_labels_path = \"brain_train_label.npy\"\n",
    "test_images_path = \"brain_test_image_final.npy\"\n",
    "test_labels_path = \"brain_test_label.npy\"\n",
    "\n",
    "# Load the data\n",
    "final_X_train_modified = np.load(train_images_path)[:, 1, :, :]\n",
    "final_X_test_modified = np.load(test_images_path)[:, 1, :, :]\n",
    "train_labels = np.load(train_labels_path)\n",
    "test_labels = np.load(test_labels_path)\n",
    "\n",
    "# Normalize and Resize Images using Pillow\n",
    "def normalize_and_resize(images, target_size=(224, 224)):\n",
    "    resized_images = []\n",
    "    for img in images:\n",
    "        img = Image.fromarray((img * 255).astype(np.uint8))\n",
    "        img_resized = img.resize(target_size, Image.Resampling.LANCZOS)\n",
    "        resized_images.append(np.array(img_resized) / 255.0)\n",
    "    return np.array(resized_images)\n",
    "\n",
    "final_X_train_resized = normalize_and_resize(final_X_train_modified)\n",
    "final_X_test_resized = normalize_and_resize(final_X_test_modified)\n",
    "\n",
    "# Define SimCLR Augmentation Transform\n",
    "transform_simclr = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0), ratio=(3/4, 4/3)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.8, contrast=0.8, saturation=0.8, hue=0.2),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=23, sigma=(0.1, 2.0))], p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Custom Dataset for SimCLR\n",
    "class SimCLRDataset(Dataset):\n",
    "    def __init__(self, images, transform):\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        img_1 = self.transform(img)\n",
    "        img_2 = self.transform(img)\n",
    "        return img_1, img_2\n",
    "\n",
    "train_dataset = SimCLRDataset(final_X_train_resized, transform_simclr)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "# Define SimCLR Model\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, base_encoder, projection_dim):\n",
    "        super(SimCLR, self).__init__()\n",
    "        self.encoder = base_encoder\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(512, 512),  # Adjusted for ResNet-34 output\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, projection_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        z = self.projector(h)\n",
    "        return z\n",
    "\n",
    "# Define NT-Xent Loss\n",
    "class NTXentLoss(nn.Module):\n",
    "    def __init__(self, temperature):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        N = z_i.size(0) + z_j.size(0)\n",
    "        z = torch.cat((z_i, z_j), dim=0)\n",
    "        sim = torch.mm(z, z.T) / self.temperature\n",
    "        sim = torch.nn.functional.softmax(sim, dim=1)\n",
    "\n",
    "        labels = torch.cat([\n",
    "            torch.arange(z_i.size(0), device=z.device),\n",
    "            torch.arange(z_j.size(0), device=z.device)\n",
    "        ])\n",
    "        loss = self.criterion(sim, labels)\n",
    "        return loss\n",
    "\n",
    "# Initialize ResNet-34 Encoder with updated weights argument\n",
    "base_encoder = resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)\n",
    "base_encoder.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "base_encoder.fc = nn.Identity()\n",
    "\n",
    "# Initialize SimCLR Model\n",
    "model = SimCLR(base_encoder, projection_dim=128).to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion = NTXentLoss(temperature=0.5)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "# Train SimCLR Model\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for img_1, img_2 in train_loader:\n",
    "        img_1, img_2 = img_1.to(\"cuda\"), img_2.to(\"cuda\")\n",
    "        z_i = model(img_1)\n",
    "        z_j = model(img_2)\n",
    "\n",
    "        loss = criterion(z_i, z_j)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch [{epoch+1}/100], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Define Dataset for Classification\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# Initialize Training and Test Dataset and DataLoader\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "train_dataset = TestDataset(final_X_train_resized, train_labels, transform=train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "test_dataset = TestDataset(final_X_test_resized, test_labels, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "# Add Classification Head\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "classification_head = ClassificationHead(input_dim=512, num_classes=len(np.unique(train_labels))).to(\"cuda\")\n",
    "optimizer_cls = optim.Adam([\n",
    "    {\"params\": model.encoder.parameters(), \"lr\": 1e-5},\n",
    "    {\"params\": classification_head.parameters(), \"lr\": 3e-4},\n",
    "])\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "\n",
    "# Fine-tune Classification Head\n",
    "for epoch in range(100):\n",
    "    model.encoder.train()\n",
    "    classification_head.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for img, label in train_loader:\n",
    "        img, label = img.to(\"cuda\"), label.to(\"cuda\")\n",
    "        features = model.encoder(img)\n",
    "        logits = classification_head(features)\n",
    "        loss = criterion_cls(logits, label)\n",
    "\n",
    "        optimizer_cls.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_cls.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct += (logits.argmax(dim=1) == label).sum().item()\n",
    "\n",
    "    accuracy = correct / len(train_labels)\n",
    "    print(f\"Epoch [{epoch+1}/100], Loss: {total_loss/len(train_loader):.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate on Test Dataset\n",
    "classification_head.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for img, label in test_loader:\n",
    "        img, label = img.to(\"cuda\"), label.to(\"cuda\")\n",
    "        features = model.encoder(img)\n",
    "        logits = classification_head(features)\n",
    "        correct += (logits.argmax(dim=1) == label).sum().item()\n",
    "\n",
    "test_accuracy = correct / len(test_labels)\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d8440a-e724-4e89-b18a-889c8bf21d60",
   "metadata": {},
   "source": [
    "# The below one is used the previous SimCLR but has different epoches=200 for testing. the accuracy is 89%\n",
    "\n",
    "# SimCLR Implementation with Alternative Testing Approach\n",
    "\n",
    "## **1. Dataset for Classification**\n",
    "- **TestDataset**:\n",
    "  - A custom dataset class that accepts:\n",
    "    - Images and their corresponding labels.\n",
    "    - An optional transformation function to preprocess input images.\n",
    "  - Returns a single image-label pair at a time.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Data Preprocessing for Testing**\n",
    "- **Training and Testing Transformations**:\n",
    "  - **Training**:\n",
    "    - `Resize`: Resizes the input to `224x224`.\n",
    "    - `ToTensor`: Converts the image into a PyTorch tensor.\n",
    "    - `Normalize`: Normalizes pixel values with a mean of `0.5` and standard deviation of `0.5`.\n",
    "  - **Testing**:\n",
    "    - Applies the same resizing and normalization as training to maintain consistency.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. DataLoader Configuration**\n",
    "- **Batch Size**:\n",
    "  - `512` for both training and testing.\n",
    "- **Shuffling**:\n",
    "  - Training set is shuffled to enhance generalization during training.\n",
    "  - Testing set is not shuffled, ensuring deterministic evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Fine-Tuning with a Classification Head**\n",
    "- **Classification Head**:\n",
    "  - A single linear layer (`Linear(512, num_classes)`) maps encoder features to the output class logits.\n",
    "- **Fine-Tuning Procedure**:\n",
    "  - The SimCLR encoder is fine-tuned along with the classification head.\n",
    "  - Hyperparameters:\n",
    "    - Learning rate for encoder: `1e-5`\n",
    "    - Learning rate for classification head: `3e-4`\n",
    "    - Loss function: `CrossEntropyLoss`\n",
    "    - Epochs: `200`\n",
    "  - Optimizer: Adam optimizes both the encoder and classification head weights.\n",
    "- **Training Outputs**:\n",
    "  - The average training loss and accuracy are logged for each epoch.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Alternative Testing Procedure**\n",
    "- **Evaluation on Test Dataset**:\n",
    "  - The classification head's performance is evaluated on the held-out test set.\n",
    "  - The encoder features are passed to the classification head for predictions.\n",
    "- **Metrics**:\n",
    "  - Accuracy is computed as the ratio of correctly predicted labels to the total number of samples in the test set.\n",
    "- **Logging**:\n",
    "  - The final test accuracy is printed as a percentage for easy interpretation.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Key Changes in Testing**\n",
    "1. **Direct Evaluation with the Classification Head**:\n",
    "   - Features extracted by the encoder are passed to the classification head for inference.\n",
    "2. **Consistency in Data Transformations**:\n",
    "   - The same resizing and normalization steps are applied to training and testing datasets.\n",
    "3. **Logging Metrics**:\n",
    "   - Both training and testing accuracy are explicitly calculated and printed for performance monitoring.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d2467f5-d85c-4c27-b425-090f65f55e2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.1620, Accuracy: 0.3193\n",
      "Epoch [2/100], Loss: 0.6423, Accuracy: 0.8497\n",
      "Epoch [3/100], Loss: 0.3671, Accuracy: 0.9897\n",
      "Epoch [4/100], Loss: 0.2183, Accuracy: 0.9976\n",
      "Epoch [5/100], Loss: 0.1332, Accuracy: 0.9994\n",
      "Epoch [6/100], Loss: 0.0877, Accuracy: 0.9994\n",
      "Epoch [7/100], Loss: 0.0580, Accuracy: 0.9994\n",
      "Epoch [8/100], Loss: 0.0429, Accuracy: 0.9994\n",
      "Epoch [9/100], Loss: 0.0326, Accuracy: 1.0000\n",
      "Epoch [10/100], Loss: 0.0248, Accuracy: 1.0000\n",
      "Epoch [11/100], Loss: 0.0214, Accuracy: 1.0000\n",
      "Epoch [12/100], Loss: 0.0174, Accuracy: 1.0000\n",
      "Epoch [13/100], Loss: 0.0147, Accuracy: 1.0000\n",
      "Epoch [14/100], Loss: 0.0130, Accuracy: 1.0000\n",
      "Epoch [15/100], Loss: 0.0127, Accuracy: 1.0000\n",
      "Epoch [16/100], Loss: 0.0102, Accuracy: 1.0000\n",
      "Epoch [17/100], Loss: 0.0101, Accuracy: 1.0000\n",
      "Epoch [18/100], Loss: 0.0088, Accuracy: 1.0000\n",
      "Epoch [19/100], Loss: 0.0082, Accuracy: 1.0000\n",
      "Epoch [20/100], Loss: 0.0072, Accuracy: 1.0000\n",
      "Epoch [21/100], Loss: 0.0086, Accuracy: 1.0000\n",
      "Epoch [22/100], Loss: 0.0063, Accuracy: 1.0000\n",
      "Epoch [23/100], Loss: 0.0061, Accuracy: 1.0000\n",
      "Epoch [24/100], Loss: 0.0056, Accuracy: 1.0000\n",
      "Epoch [25/100], Loss: 0.0072, Accuracy: 1.0000\n",
      "Epoch [26/100], Loss: 0.0054, Accuracy: 1.0000\n",
      "Epoch [27/100], Loss: 0.0050, Accuracy: 1.0000\n",
      "Epoch [28/100], Loss: 0.0048, Accuracy: 1.0000\n",
      "Epoch [29/100], Loss: 0.0048, Accuracy: 1.0000\n",
      "Epoch [30/100], Loss: 0.0042, Accuracy: 1.0000\n",
      "Epoch [31/100], Loss: 0.0039, Accuracy: 1.0000\n",
      "Epoch [32/100], Loss: 0.0041, Accuracy: 1.0000\n",
      "Epoch [33/100], Loss: 0.0036, Accuracy: 1.0000\n",
      "Epoch [34/100], Loss: 0.0036, Accuracy: 1.0000\n",
      "Epoch [35/100], Loss: 0.0033, Accuracy: 1.0000\n",
      "Epoch [36/100], Loss: 0.0031, Accuracy: 1.0000\n",
      "Epoch [37/100], Loss: 0.0033, Accuracy: 1.0000\n",
      "Epoch [38/100], Loss: 0.0033, Accuracy: 1.0000\n",
      "Epoch [39/100], Loss: 0.0033, Accuracy: 1.0000\n",
      "Epoch [40/100], Loss: 0.0029, Accuracy: 1.0000\n",
      "Epoch [41/100], Loss: 0.0025, Accuracy: 1.0000\n",
      "Epoch [42/100], Loss: 0.0029, Accuracy: 1.0000\n",
      "Epoch [43/100], Loss: 0.0025, Accuracy: 1.0000\n",
      "Epoch [44/100], Loss: 0.0023, Accuracy: 1.0000\n",
      "Epoch [45/100], Loss: 0.0023, Accuracy: 1.0000\n",
      "Epoch [46/100], Loss: 0.0022, Accuracy: 1.0000\n",
      "Epoch [47/100], Loss: 0.0024, Accuracy: 1.0000\n",
      "Epoch [48/100], Loss: 0.0021, Accuracy: 1.0000\n",
      "Epoch [49/100], Loss: 0.0029, Accuracy: 1.0000\n",
      "Epoch [50/100], Loss: 0.0019, Accuracy: 1.0000\n",
      "Epoch [51/100], Loss: 0.0018, Accuracy: 1.0000\n",
      "Epoch [52/100], Loss: 0.0019, Accuracy: 1.0000\n",
      "Epoch [53/100], Loss: 0.0018, Accuracy: 1.0000\n",
      "Epoch [54/100], Loss: 0.0018, Accuracy: 1.0000\n",
      "Epoch [55/100], Loss: 0.0018, Accuracy: 1.0000\n",
      "Epoch [56/100], Loss: 0.0016, Accuracy: 1.0000\n",
      "Epoch [57/100], Loss: 0.0016, Accuracy: 1.0000\n",
      "Epoch [58/100], Loss: 0.0016, Accuracy: 1.0000\n",
      "Epoch [59/100], Loss: 0.0017, Accuracy: 1.0000\n",
      "Epoch [60/100], Loss: 0.0014, Accuracy: 1.0000\n",
      "Epoch [61/100], Loss: 0.0014, Accuracy: 1.0000\n",
      "Epoch [62/100], Loss: 0.0013, Accuracy: 1.0000\n",
      "Epoch [63/100], Loss: 0.0014, Accuracy: 1.0000\n",
      "Epoch [64/100], Loss: 0.0013, Accuracy: 1.0000\n",
      "Epoch [65/100], Loss: 0.0015, Accuracy: 1.0000\n",
      "Epoch [66/100], Loss: 0.0013, Accuracy: 1.0000\n",
      "Epoch [67/100], Loss: 0.0014, Accuracy: 1.0000\n",
      "Epoch [68/100], Loss: 0.0013, Accuracy: 1.0000\n",
      "Epoch [69/100], Loss: 0.0015, Accuracy: 1.0000\n",
      "Epoch [70/100], Loss: 0.0012, Accuracy: 1.0000\n",
      "Epoch [71/100], Loss: 0.0013, Accuracy: 1.0000\n",
      "Epoch [72/100], Loss: 0.0011, Accuracy: 1.0000\n",
      "Epoch [73/100], Loss: 0.0011, Accuracy: 1.0000\n",
      "Epoch [74/100], Loss: 0.0013, Accuracy: 1.0000\n",
      "Epoch [75/100], Loss: 0.0012, Accuracy: 1.0000\n",
      "Epoch [76/100], Loss: 0.0011, Accuracy: 1.0000\n",
      "Epoch [77/100], Loss: 0.0011, Accuracy: 1.0000\n",
      "Epoch [78/100], Loss: 0.0009, Accuracy: 1.0000\n",
      "Epoch [79/100], Loss: 0.0010, Accuracy: 1.0000\n",
      "Epoch [80/100], Loss: 0.0009, Accuracy: 1.0000\n",
      "Epoch [81/100], Loss: 0.0009, Accuracy: 1.0000\n",
      "Epoch [82/100], Loss: 0.0009, Accuracy: 1.0000\n",
      "Epoch [83/100], Loss: 0.0009, Accuracy: 1.0000\n",
      "Epoch [84/100], Loss: 0.0009, Accuracy: 1.0000\n",
      "Epoch [85/100], Loss: 0.0009, Accuracy: 1.0000\n",
      "Epoch [86/100], Loss: 0.0008, Accuracy: 1.0000\n",
      "Epoch [87/100], Loss: 0.0008, Accuracy: 1.0000\n",
      "Epoch [88/100], Loss: 0.0009, Accuracy: 1.0000\n",
      "Epoch [89/100], Loss: 0.0008, Accuracy: 1.0000\n",
      "Epoch [90/100], Loss: 0.0008, Accuracy: 1.0000\n",
      "Epoch [91/100], Loss: 0.0007, Accuracy: 1.0000\n",
      "Epoch [92/100], Loss: 0.0008, Accuracy: 1.0000\n",
      "Epoch [93/100], Loss: 0.0007, Accuracy: 1.0000\n",
      "Epoch [94/100], Loss: 0.0007, Accuracy: 1.0000\n",
      "Epoch [95/100], Loss: 0.0009, Accuracy: 1.0000\n",
      "Epoch [96/100], Loss: 0.0008, Accuracy: 1.0000\n",
      "Epoch [97/100], Loss: 0.0008, Accuracy: 1.0000\n",
      "Epoch [98/100], Loss: 0.0006, Accuracy: 1.0000\n",
      "Epoch [99/100], Loss: 0.0006, Accuracy: 1.0000\n",
      "Epoch [100/100], Loss: 0.0006, Accuracy: 1.0000\n",
      "Epoch [101/100], Loss: 0.0006, Accuracy: 1.0000\n",
      "Epoch [102/100], Loss: 0.0006, Accuracy: 1.0000\n",
      "Epoch [103/100], Loss: 0.0006, Accuracy: 1.0000\n",
      "Epoch [104/100], Loss: 0.0006, Accuracy: 1.0000\n",
      "Epoch [105/100], Loss: 0.0005, Accuracy: 1.0000\n",
      "Epoch [106/100], Loss: 0.0006, Accuracy: 1.0000\n",
      "Epoch [107/100], Loss: 0.0006, Accuracy: 1.0000\n",
      "Epoch [108/100], Loss: 0.0007, Accuracy: 1.0000\n",
      "Epoch [109/100], Loss: 0.0006, Accuracy: 1.0000\n",
      "Epoch [110/100], Loss: 0.0005, Accuracy: 1.0000\n",
      "Epoch [111/100], Loss: 0.0006, Accuracy: 1.0000\n",
      "Epoch [112/100], Loss: 0.0006, Accuracy: 1.0000\n",
      "Epoch [113/100], Loss: 0.0005, Accuracy: 1.0000\n",
      "Epoch [114/100], Loss: 0.0007, Accuracy: 1.0000\n",
      "Epoch [115/100], Loss: 0.0005, Accuracy: 1.0000\n",
      "Epoch [116/100], Loss: 0.0005, Accuracy: 1.0000\n",
      "Epoch [117/100], Loss: 0.0005, Accuracy: 1.0000\n",
      "Epoch [118/100], Loss: 0.0005, Accuracy: 1.0000\n",
      "Epoch [119/100], Loss: 0.0005, Accuracy: 1.0000\n",
      "Epoch [120/100], Loss: 0.0005, Accuracy: 1.0000\n",
      "Epoch [121/100], Loss: 0.0005, Accuracy: 1.0000\n",
      "Epoch [122/100], Loss: 0.0005, Accuracy: 1.0000\n",
      "Epoch [123/100], Loss: 0.0005, Accuracy: 1.0000\n",
      "Epoch [124/100], Loss: 0.0005, Accuracy: 1.0000\n",
      "Epoch [125/100], Loss: 0.0004, Accuracy: 1.0000\n",
      "Epoch [126/100], Loss: 0.0004, Accuracy: 1.0000\n",
      "Epoch [127/100], Loss: 0.0004, Accuracy: 1.0000\n",
      "Epoch [128/100], Loss: 0.0004, Accuracy: 1.0000\n",
      "Epoch [129/100], Loss: 0.0004, Accuracy: 1.0000\n",
      "Epoch [130/100], Loss: 0.0004, Accuracy: 1.0000\n",
      "Epoch [131/100], Loss: 0.0004, Accuracy: 1.0000\n",
      "Epoch [132/100], Loss: 0.0004, Accuracy: 1.0000\n",
      "Epoch [133/100], Loss: 0.0004, Accuracy: 1.0000\n",
      "Epoch [134/100], Loss: 0.0004, Accuracy: 1.0000\n",
      "Epoch [135/100], Loss: 0.0005, Accuracy: 1.0000\n",
      "Epoch [136/100], Loss: 0.0004, Accuracy: 1.0000\n",
      "Epoch [137/100], Loss: 0.0004, Accuracy: 1.0000\n",
      "Epoch [138/100], Loss: 0.0004, Accuracy: 1.0000\n",
      "Epoch [139/100], Loss: 0.0004, Accuracy: 1.0000\n",
      "Epoch [140/100], Loss: 0.0004, Accuracy: 1.0000\n",
      "Epoch [141/100], Loss: 0.0003, Accuracy: 1.0000\n",
      "Epoch [142/100], Loss: 0.0004, Accuracy: 1.0000\n",
      "Epoch [143/100], Loss: 0.0003, Accuracy: 1.0000\n",
      "Epoch [144/100], Loss: 0.0003, Accuracy: 1.0000\n",
      "Epoch [145/100], Loss: 0.0003, Accuracy: 1.0000\n",
      "Epoch [146/100], Loss: 0.0003, Accuracy: 1.0000\n",
      "Epoch [147/100], Loss: 0.0003, Accuracy: 1.0000\n",
      "Epoch [148/100], Loss: 0.0003, Accuracy: 1.0000\n",
      "Epoch [149/100], Loss: 0.0003, Accuracy: 1.0000\n",
      "Epoch [150/100], Loss: 0.0003, Accuracy: 1.0000\n",
      "Epoch [151/100], Loss: 0.0003, Accuracy: 1.0000\n",
      "Epoch [152/100], Loss: 0.0003, Accuracy: 1.0000\n",
      "Epoch [153/100], Loss: 0.0004, Accuracy: 1.0000\n",
      "Epoch [154/100], Loss: 0.0004, Accuracy: 1.0000\n",
      "Epoch [155/100], Loss: 0.0003, Accuracy: 1.0000\n",
      "Epoch [156/100], Loss: 0.0003, Accuracy: 1.0000\n",
      "Epoch [157/100], Loss: 0.0003, Accuracy: 1.0000\n",
      "Epoch [158/100], Loss: 0.0003, Accuracy: 1.0000\n",
      "Epoch [159/100], Loss: 0.0003, Accuracy: 1.0000\n",
      "Epoch [160/100], Loss: 0.0003, Accuracy: 1.0000\n",
      "Epoch [161/100], Loss: 0.0003, Accuracy: 1.0000\n",
      "Epoch [162/100], Loss: 0.0003, Accuracy: 1.0000\n",
      "Epoch [163/100], Loss: 0.0004, Accuracy: 1.0000\n",
      "Epoch [164/100], Loss: 0.0003, Accuracy: 1.0000\n",
      "Epoch [165/100], Loss: 0.0003, Accuracy: 1.0000\n",
      "Epoch [166/100], Loss: 0.0003, Accuracy: 1.0000\n",
      "Epoch [167/100], Loss: 0.0002, Accuracy: 1.0000\n",
      "Epoch [168/100], Loss: 0.0003, Accuracy: 1.0000\n",
      "Epoch [169/100], Loss: 0.0003, Accuracy: 1.0000\n",
      "Epoch [170/100], Loss: 0.0002, Accuracy: 1.0000\n",
      "Epoch [171/100], Loss: 0.0002, Accuracy: 1.0000\n",
      "Epoch [172/100], Loss: 0.0003, Accuracy: 1.0000\n",
      "Epoch [173/100], Loss: 0.0003, Accuracy: 1.0000\n",
      "Epoch [174/100], Loss: 0.0003, Accuracy: 1.0000\n",
      "Epoch [175/100], Loss: 0.0003, Accuracy: 1.0000\n",
      "Epoch [176/100], Loss: 0.0002, Accuracy: 1.0000\n",
      "Epoch [177/100], Loss: 0.0002, Accuracy: 1.0000\n",
      "Epoch [178/100], Loss: 0.0002, Accuracy: 1.0000\n",
      "Epoch [179/100], Loss: 0.0002, Accuracy: 1.0000\n",
      "Epoch [180/100], Loss: 0.0002, Accuracy: 1.0000\n",
      "Epoch [181/100], Loss: 0.0002, Accuracy: 1.0000\n",
      "Epoch [182/100], Loss: 0.0002, Accuracy: 1.0000\n",
      "Epoch [183/100], Loss: 0.0002, Accuracy: 1.0000\n",
      "Epoch [184/100], Loss: 0.0003, Accuracy: 1.0000\n",
      "Epoch [185/100], Loss: 0.0002, Accuracy: 1.0000\n",
      "Epoch [186/100], Loss: 0.0002, Accuracy: 1.0000\n",
      "Epoch [187/100], Loss: 0.0002, Accuracy: 1.0000\n",
      "Epoch [188/100], Loss: 0.0002, Accuracy: 1.0000\n",
      "Epoch [189/100], Loss: 0.0002, Accuracy: 1.0000\n",
      "Epoch [190/100], Loss: 0.0002, Accuracy: 1.0000\n",
      "Epoch [191/100], Loss: 0.0002, Accuracy: 1.0000\n",
      "Epoch [192/100], Loss: 0.0002, Accuracy: 1.0000\n",
      "Epoch [193/100], Loss: 0.0002, Accuracy: 1.0000\n",
      "Epoch [194/100], Loss: 0.0002, Accuracy: 1.0000\n",
      "Epoch [195/100], Loss: 0.0002, Accuracy: 1.0000\n",
      "Epoch [196/100], Loss: 0.0002, Accuracy: 1.0000\n",
      "Epoch [197/100], Loss: 0.0002, Accuracy: 1.0000\n",
      "Epoch [198/100], Loss: 0.0002, Accuracy: 1.0000\n",
      "Epoch [199/100], Loss: 0.0002, Accuracy: 1.0000\n",
      "Epoch [200/100], Loss: 0.0002, Accuracy: 1.0000\n",
      "Test Accuracy: 89.0785\n"
     ]
    }
   ],
   "source": [
    "# Define Dataset for Classification\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# Initialize Training and Test Dataset and DataLoader\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "train_dataset = TestDataset(final_X_train_resized, train_labels, transform=train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "test_dataset = TestDataset(final_X_test_resized, test_labels, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "# Add Classification Head\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "classification_head = ClassificationHead(input_dim=512, num_classes=len(np.unique(train_labels))).to(\"cuda\")\n",
    "optimizer_cls = optim.Adam([\n",
    "    {\"params\": model.encoder.parameters(), \"lr\": 1e-5},\n",
    "    {\"params\": classification_head.parameters(), \"lr\": 3e-4},\n",
    "])\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "\n",
    "# Fine-tune Classification Head\n",
    "for epoch in range(200):\n",
    "    model.encoder.train()\n",
    "    classification_head.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for img, label in train_loader:\n",
    "        img, label = img.to(\"cuda\"), label.to(\"cuda\")\n",
    "        features = model.encoder(img)\n",
    "        logits = classification_head(features)\n",
    "        loss = criterion_cls(logits, label)\n",
    "\n",
    "        optimizer_cls.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_cls.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct += (logits.argmax(dim=1) == label).sum().item()\n",
    "\n",
    "    accuracy = correct / len(train_labels)\n",
    "    print(f\"Epoch [{epoch+1}/100], Loss: {total_loss/len(train_loader):.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate on Test Dataset\n",
    "classification_head.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for img, label in test_loader:\n",
    "        img, label = img.to(\"cuda\"), label.to(\"cuda\")\n",
    "        features = model.encoder(img)\n",
    "        logits = classification_head(features)\n",
    "        correct += (logits.argmax(dim=1) == label).sum().item()\n",
    "\n",
    "test_accuracy = correct / len(test_labels)\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c835630d-5a34-44e0-a78e-358fee5f0378",
   "metadata": {},
   "source": [
    "# The below one is used the previous SimCLR but has different batch size=128 for testing. the accuracy is 88%\n",
    "\n",
    "# SimCLR with Fine-Tuning and Classification Evaluation\n",
    "\n",
    "## **1. Dataset for Classification**\n",
    "- **TestDataset**:\n",
    "  - Custom dataset class designed to handle:\n",
    "    - Images and their corresponding labels.\n",
    "    - Optional transformations for preprocessing.\n",
    "  - Returns a processed image and its label.\n",
    "- **Transformations**:\n",
    "  - Applied to both training and testing datasets:\n",
    "    - `Resize`: Resizes images to `224x224`.\n",
    "    - `ToTensor`: Converts images into PyTorch tensors.\n",
    "    - `Normalize`: Scales pixel values to have a mean of `0.5` and standard deviation of `0.5`.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. DataLoader Initialization**\n",
    "- **Batch Size**:\n",
    "  - Training: `128`\n",
    "  - Testing: `128`\n",
    "- **Shuffling**:\n",
    "  - Enabled for training (`shuffle=True`) to improve generalization.\n",
    "  - Disabled for testing (`shuffle=False`) to maintain consistency in evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Classification Head**\n",
    "- **Structure**:\n",
    "  - Single `Linear` layer with input dimension `512` (encoder output) and output dimension equal to the number of classes (`num_classes`).\n",
    "- **Purpose**:\n",
    "  - Maps features extracted by the encoder to class logits for classification.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Fine-Tuning**\n",
    "- **Objective**:\n",
    "  - Fine-tune the encoder with a classification head on labeled training data.\n",
    "- **Optimization**:\n",
    "  - Encoder: Low learning rate (`1e-5`) to retain pre-trained features.\n",
    "  - Classification Head: Higher learning rate (`3e-4`) to adapt to the task-specific dataset.\n",
    "  - Loss Function: CrossEntropyLoss for classification.\n",
    "  - Optimizer: Adam.\n",
    "- **Epoch-wise Metrics**:\n",
    "  - Logs the average training loss and accuracy per epoch.\n",
    "  - Accuracy is calculated as the percentage of correct predictions on the training dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Evaluation on Test Dataset**\n",
    "- **Procedure**:\n",
    "  - The trained encoder and classification head are evaluated on the test dataset.\n",
    "  - For each test image:\n",
    "    - Encoder extracts features.\n",
    "    - Classification head predicts the class logits.\n",
    "  - Correct predictions are summed to compute the total test accuracy.\n",
    "- **Metric**:\n",
    "  - Test accuracy is reported as the ratio of correct predictions to the total number of test samples.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Key Features**\n",
    "1. **Separate Training and Testing Pipelines**:\n",
    "   - Ensures robust and unbiased evaluation.\n",
    "2. **Consistent Data Transformations**:\n",
    "   - Uniform resizing and normalization enhance compatibility between training and testing datasets.\n",
    "3. **Fine-Tuning Strategy**:\n",
    "   - Differential learning rates for the encoder and classification head maximize performance.\n",
    "4. **Batch-Wise Evaluation**:\n",
    "   - Efficient and scalable evaluation with DataLoader.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a580b0a-9a1c-47dd-aa19-2ec55631df86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.6271, Accuracy: 80.8087\n",
      "Epoch [2/100], Loss: 0.1275, Accuracy: 100.0000\n",
      "Epoch [3/100], Loss: 0.0388, Accuracy: 100.0000\n",
      "Epoch [4/100], Loss: 0.0207, Accuracy: 100.0000\n",
      "Epoch [5/100], Loss: 0.0142, Accuracy: 100.0000\n",
      "Epoch [6/100], Loss: 0.0103, Accuracy: 100.0000\n",
      "Epoch [7/100], Loss: 0.0081, Accuracy: 100.0000\n",
      "Epoch [8/100], Loss: 0.0070, Accuracy: 100.0000\n",
      "Epoch [9/100], Loss: 0.0057, Accuracy: 100.0000\n",
      "Epoch [10/100], Loss: 0.0052, Accuracy: 100.0000\n",
      "Epoch [11/100], Loss: 0.0043, Accuracy: 100.0000\n",
      "Epoch [12/100], Loss: 0.0039, Accuracy: 100.0000\n",
      "Epoch [13/100], Loss: 0.0032, Accuracy: 100.0000\n",
      "Epoch [14/100], Loss: 0.0034, Accuracy: 100.0000\n",
      "Epoch [15/100], Loss: 0.0027, Accuracy: 100.0000\n",
      "Epoch [16/100], Loss: 0.0023, Accuracy: 100.0000\n",
      "Epoch [17/100], Loss: 0.0021, Accuracy: 100.0000\n",
      "Epoch [18/100], Loss: 0.0020, Accuracy: 100.0000\n",
      "Epoch [19/100], Loss: 0.0018, Accuracy: 100.0000\n",
      "Epoch [20/100], Loss: 0.0018, Accuracy: 100.0000\n",
      "Epoch [21/100], Loss: 0.0015, Accuracy: 100.0000\n",
      "Epoch [22/100], Loss: 0.0013, Accuracy: 100.0000\n",
      "Epoch [23/100], Loss: 0.0013, Accuracy: 100.0000\n",
      "Epoch [24/100], Loss: 0.0013, Accuracy: 100.0000\n",
      "Epoch [25/100], Loss: 0.0013, Accuracy: 100.0000\n",
      "Epoch [26/100], Loss: 0.0011, Accuracy: 100.0000\n",
      "Epoch [27/100], Loss: 0.0011, Accuracy: 100.0000\n",
      "Epoch [28/100], Loss: 0.0009, Accuracy: 100.0000\n",
      "Epoch [29/100], Loss: 0.0010, Accuracy: 100.0000\n",
      "Epoch [30/100], Loss: 0.0010, Accuracy: 100.0000\n",
      "Epoch [31/100], Loss: 0.0009, Accuracy: 100.0000\n",
      "Epoch [32/100], Loss: 0.0007, Accuracy: 100.0000\n",
      "Epoch [33/100], Loss: 0.0008, Accuracy: 100.0000\n",
      "Epoch [34/100], Loss: 0.0007, Accuracy: 100.0000\n",
      "Epoch [35/100], Loss: 0.0008, Accuracy: 100.0000\n",
      "Epoch [36/100], Loss: 0.0009, Accuracy: 100.0000\n",
      "Epoch [37/100], Loss: 0.0007, Accuracy: 100.0000\n",
      "Epoch [38/100], Loss: 0.0006, Accuracy: 100.0000\n",
      "Epoch [39/100], Loss: 0.0005, Accuracy: 100.0000\n",
      "Epoch [40/100], Loss: 0.0005, Accuracy: 100.0000\n",
      "Epoch [41/100], Loss: 0.0004, Accuracy: 100.0000\n",
      "Epoch [42/100], Loss: 0.0007, Accuracy: 100.0000\n",
      "Epoch [43/100], Loss: 0.0005, Accuracy: 100.0000\n",
      "Epoch [44/100], Loss: 0.0005, Accuracy: 100.0000\n",
      "Epoch [45/100], Loss: 0.0004, Accuracy: 100.0000\n",
      "Epoch [46/100], Loss: 0.0005, Accuracy: 100.0000\n",
      "Epoch [47/100], Loss: 0.0004, Accuracy: 100.0000\n",
      "Epoch [48/100], Loss: 0.0004, Accuracy: 100.0000\n",
      "Epoch [49/100], Loss: 0.0004, Accuracy: 100.0000\n",
      "Epoch [50/100], Loss: 0.0003, Accuracy: 100.0000\n",
      "Epoch [51/100], Loss: 0.0003, Accuracy: 100.0000\n",
      "Epoch [52/100], Loss: 0.0003, Accuracy: 100.0000\n",
      "Epoch [53/100], Loss: 0.0003, Accuracy: 100.0000\n",
      "Epoch [54/100], Loss: 0.0003, Accuracy: 100.0000\n",
      "Epoch [55/100], Loss: 0.0003, Accuracy: 100.0000\n",
      "Epoch [56/100], Loss: 0.0004, Accuracy: 100.0000\n",
      "Epoch [57/100], Loss: 0.0003, Accuracy: 100.0000\n",
      "Epoch [58/100], Loss: 0.0003, Accuracy: 100.0000\n",
      "Epoch [59/100], Loss: 0.0003, Accuracy: 100.0000\n",
      "Epoch [60/100], Loss: 0.0003, Accuracy: 100.0000\n",
      "Epoch [61/100], Loss: 0.0003, Accuracy: 100.0000\n",
      "Epoch [62/100], Loss: 0.0003, Accuracy: 100.0000\n",
      "Epoch [63/100], Loss: 0.0002, Accuracy: 100.0000\n",
      "Epoch [64/100], Loss: 0.0003, Accuracy: 100.0000\n",
      "Epoch [65/100], Loss: 0.0002, Accuracy: 100.0000\n",
      "Epoch [66/100], Loss: 0.0002, Accuracy: 100.0000\n",
      "Epoch [67/100], Loss: 0.0003, Accuracy: 100.0000\n",
      "Epoch [68/100], Loss: 0.0002, Accuracy: 100.0000\n",
      "Epoch [69/100], Loss: 0.0002, Accuracy: 100.0000\n",
      "Epoch [70/100], Loss: 0.0002, Accuracy: 100.0000\n",
      "Epoch [71/100], Loss: 0.0002, Accuracy: 100.0000\n",
      "Epoch [72/100], Loss: 0.0002, Accuracy: 100.0000\n",
      "Epoch [73/100], Loss: 0.0002, Accuracy: 100.0000\n",
      "Epoch [74/100], Loss: 0.0002, Accuracy: 100.0000\n",
      "Epoch [75/100], Loss: 0.0002, Accuracy: 100.0000\n",
      "Epoch [76/100], Loss: 0.0002, Accuracy: 100.0000\n",
      "Epoch [77/100], Loss: 0.0002, Accuracy: 100.0000\n",
      "Epoch [78/100], Loss: 0.0002, Accuracy: 100.0000\n",
      "Epoch [79/100], Loss: 0.0002, Accuracy: 100.0000\n",
      "Epoch [80/100], Loss: 0.0002, Accuracy: 100.0000\n",
      "Epoch [81/100], Loss: 0.0002, Accuracy: 100.0000\n",
      "Epoch [82/100], Loss: 0.0001, Accuracy: 100.0000\n",
      "Epoch [83/100], Loss: 0.0002, Accuracy: 100.0000\n",
      "Epoch [84/100], Loss: 0.0002, Accuracy: 100.0000\n",
      "Epoch [85/100], Loss: 0.0002, Accuracy: 100.0000\n",
      "Epoch [86/100], Loss: 0.0001, Accuracy: 100.0000\n",
      "Epoch [87/100], Loss: 0.0001, Accuracy: 100.0000\n",
      "Epoch [88/100], Loss: 0.0001, Accuracy: 100.0000\n",
      "Epoch [89/100], Loss: 0.0001, Accuracy: 100.0000\n",
      "Epoch [90/100], Loss: 0.0001, Accuracy: 100.0000\n",
      "Epoch [91/100], Loss: 0.0001, Accuracy: 100.0000\n",
      "Epoch [92/100], Loss: 0.0001, Accuracy: 100.0000\n",
      "Epoch [93/100], Loss: 0.0001, Accuracy: 100.0000\n",
      "Epoch [94/100], Loss: 0.0001, Accuracy: 100.0000\n",
      "Epoch [95/100], Loss: 0.0001, Accuracy: 100.0000\n",
      "Epoch [96/100], Loss: 0.0001, Accuracy: 100.0000\n",
      "Epoch [97/100], Loss: 0.0001, Accuracy: 100.0000\n",
      "Epoch [98/100], Loss: 0.0001, Accuracy: 100.0000\n",
      "Epoch [99/100], Loss: 0.0001, Accuracy: 100.0000\n",
      "Epoch [100/100], Loss: 0.0001, Accuracy: 100.0000\n",
      "Test Accuracy: 0.8840\n"
     ]
    }
   ],
   "source": [
    "# Define Dataset for Classification\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# Initialize Training and Test Dataset and DataLoader\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "train_dataset = TestDataset(final_X_train_resized, train_labels, transform=train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "test_dataset = TestDataset(final_X_test_resized, test_labels, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Add Classification Head\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "classification_head = ClassificationHead(input_dim=512, num_classes=len(np.unique(train_labels))).to(\"cuda\")\n",
    "optimizer_cls = optim.Adam([\n",
    "    {\"params\": model.encoder.parameters(), \"lr\": 1e-5},\n",
    "    {\"params\": classification_head.parameters(), \"lr\": 3e-4},\n",
    "])\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "\n",
    "# Fine-tune Classification Head\n",
    "for epoch in range(100):\n",
    "    model.encoder.train()\n",
    "    classification_head.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for img, label in train_loader:\n",
    "        img, label = img.to(\"cuda\"), label.to(\"cuda\")\n",
    "        features = model.encoder(img)\n",
    "        logits = classification_head(features)\n",
    "        loss = criterion_cls(logits, label)\n",
    "\n",
    "        optimizer_cls.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_cls.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct += (logits.argmax(dim=1) == label).sum().item()\n",
    "\n",
    "    accuracy = (correct / len(train_labels))*100\n",
    "    print(f\"Epoch [{epoch+1}/100], Loss: {total_loss/len(train_loader):.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate on Test Dataset\n",
    "classification_head.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for img, label in test_loader:\n",
    "        img, label = img.to(\"cuda\"), label.to(\"cuda\")\n",
    "        features = model.encoder(img)\n",
    "        logits = classification_head(features)\n",
    "        correct += (logits.argmax(dim=1) == label).sum().item()\n",
    "\n",
    "test_accuracy = correct / len(test_labels)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68c1894-95b8-4f09-9293-5bc93e7233cb",
   "metadata": {},
   "source": [
    "# The Below one is with ResNet18. accuracy is 85%\n",
    "\n",
    "\n",
    "# SimCLR Implementation Explanation\n",
    "\n",
    "## **1. Data Preprocessing and Augmentation**\n",
    "- **Normalization and Resizing**:\n",
    "  - Images are resized to `224x224` using Lanczos resampling and normalized with a mean of `0.5` and standard deviation of `0.5`.\n",
    "- **Data Augmentation**:\n",
    "  - Augmentation pipeline includes:\n",
    "    - `RandomResizedCrop`: Crops images to random sizes and aspect ratios.\n",
    "    - `RandomHorizontalFlip`: Randomly flips the image horizontally.\n",
    "    - `ColorJitter`: Randomly changes brightness, contrast, saturation, and hue.\n",
    "    - `GaussianBlur`: Blurs the image with a random kernel size and sigma.\n",
    "  - These augmentations help generate two different views of the same image, which is crucial for contrastive learning.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Custom Dataset**\n",
    "- **SimCLRDataset**:\n",
    "  - Generates two augmented views of each input image to be used as positive pairs for contrastive learning.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. SimCLR Model Architecture**\n",
    "- **Base Encoder**:\n",
    "  - A pre-trained `ResNet-18` is used as the backbone.\n",
    "  - The first convolutional layer (`conv1`) is modified to handle single-channel (grayscale) images.\n",
    "  - The fully connected layer (`fc`) is replaced with an `Identity` layer to output feature embeddings.\n",
    "- **Projection Head**:\n",
    "  - A two-layer MLP with a hidden size of `256` and output size of `128`.\n",
    "  - Nonlinear activation (`ReLU`) is applied between the layers.\n",
    "  - The projection head helps learn representations better suited for contrastive loss.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Contrastive Loss (NT-Xent Loss)**\n",
    "- **Normalized Temperature-Scaled Cross-Entropy Loss**:\n",
    "  - Encourages the model to maximize the similarity of positive pairs (different views of the same image) while minimizing similarity to negative pairs (different images in the batch).\n",
    "  - Implements temperature scaling and softmax normalization for better gradient flow.\n",
    "  - Cross-entropy loss is used to optimize the logits.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Training Procedure**\n",
    "- **Contrastive Learning Stage**:\n",
    "  - The SimCLR model is trained using the contrastive loss on augmented image pairs.\n",
    "  - Key hyperparameters:\n",
    "    - Batch size: `1024`\n",
    "    - Learning rate: `3e-4`\n",
    "    - Temperature: `0.5`\n",
    "  - The Adam optimizer is used with a `CosineAnnealingLR` scheduler for gradual learning rate decay.\n",
    "- **Training Outputs**:\n",
    "  - Loss values are logged for each epoch to monitor the learning process.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Downstream Task: Classification**\n",
    "- **Linear Evaluation Protocol**:\n",
    "  - The pre-trained encoder is fine-tuned with a classification head for supervised learning on the labeled dataset.\n",
    "  - Classification head:\n",
    "    - A single linear layer mapping encoder outputs (`512`) to the number of classes.\n",
    "  - Fine-tuning:\n",
    "    - Encoder weights are updated at a slower learning rate (`1e-5`) than the classification head (`3e-4`).\n",
    "    - Cross-entropy loss is used for optimization.\n",
    "- **Performance Evaluation**:\n",
    "  - Test accuracy is computed on the held-out test set to measure the model's performance.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Key Features Contributing to Accuracy**\n",
    "1. **Data Augmentation**:\n",
    "   - Rich transformations ensure the model learns robust and invariant features.\n",
    "2. **Pre-trained ResNet-18 Encoder**:\n",
    "   - Provides a strong starting point for feature extraction.\n",
    "3. **Projection Head**:\n",
    "   - Helps improve representation learning by mapping features to a contrastive space.\n",
    "4. **Contrastive Loss**:\n",
    "   - NT-Xent loss ensures the learned features are discriminative and generalizable.\n",
    "5. **Fine-tuning Strategy**:\n",
    "   - Gradual learning rate decay and careful tuning of encoder and head ensure effective adaptation to the downstream task.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af97f564-8c66-42fd-b535-a726d03e2d38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 7.3593\n",
      "Epoch [2/100], Loss: 7.3082\n",
      "Epoch [3/100], Loss: 7.1995\n",
      "Epoch [4/100], Loss: 7.1010\n",
      "Epoch [5/100], Loss: 7.0257\n",
      "Epoch [6/100], Loss: 6.9785\n",
      "Epoch [7/100], Loss: 6.9562\n",
      "Epoch [8/100], Loss: 6.9369\n",
      "Epoch [9/100], Loss: 6.9158\n",
      "Epoch [10/100], Loss: 6.9056\n",
      "Epoch [11/100], Loss: 6.9009\n",
      "Epoch [12/100], Loss: 6.8963\n",
      "Epoch [13/100], Loss: 6.8949\n",
      "Epoch [14/100], Loss: 6.8927\n",
      "Epoch [15/100], Loss: 6.8909\n",
      "Epoch [16/100], Loss: 6.8916\n",
      "Epoch [17/100], Loss: 6.8904\n",
      "Epoch [18/100], Loss: 6.8895\n",
      "Epoch [19/100], Loss: 6.8896\n",
      "Epoch [20/100], Loss: 6.8895\n",
      "Epoch [21/100], Loss: 6.8907\n",
      "Epoch [22/100], Loss: 6.8900\n",
      "Epoch [23/100], Loss: 6.8884\n",
      "Epoch [24/100], Loss: 6.8883\n",
      "Epoch [25/100], Loss: 6.8880\n",
      "Epoch [26/100], Loss: 6.8878\n",
      "Epoch [27/100], Loss: 6.8876\n",
      "Epoch [28/100], Loss: 6.8881\n",
      "Epoch [29/100], Loss: 6.8873\n",
      "Epoch [30/100], Loss: 6.8866\n",
      "Epoch [31/100], Loss: 6.8883\n",
      "Epoch [32/100], Loss: 6.8870\n",
      "Epoch [33/100], Loss: 6.8872\n",
      "Epoch [34/100], Loss: 6.8867\n",
      "Epoch [35/100], Loss: 6.8873\n",
      "Epoch [36/100], Loss: 6.8869\n",
      "Epoch [37/100], Loss: 6.8865\n",
      "Epoch [38/100], Loss: 6.8869\n",
      "Epoch [39/100], Loss: 6.8867\n",
      "Epoch [40/100], Loss: 6.8866\n",
      "Epoch [41/100], Loss: 6.8871\n",
      "Epoch [42/100], Loss: 6.8873\n",
      "Epoch [43/100], Loss: 6.8865\n",
      "Epoch [44/100], Loss: 6.8872\n",
      "Epoch [45/100], Loss: 6.8863\n",
      "Epoch [46/100], Loss: 6.8858\n",
      "Epoch [47/100], Loss: 6.8863\n",
      "Epoch [48/100], Loss: 6.8865\n",
      "Epoch [49/100], Loss: 6.8857\n",
      "Epoch [50/100], Loss: 6.8868\n",
      "Epoch [51/100], Loss: 6.8863\n",
      "Epoch [52/100], Loss: 6.8859\n",
      "Epoch [53/100], Loss: 6.8868\n",
      "Epoch [54/100], Loss: 6.8866\n",
      "Epoch [55/100], Loss: 6.8861\n",
      "Epoch [56/100], Loss: 6.8859\n",
      "Epoch [57/100], Loss: 6.8863\n",
      "Epoch [58/100], Loss: 6.8860\n",
      "Epoch [59/100], Loss: 6.8860\n",
      "Epoch [60/100], Loss: 6.8862\n",
      "Epoch [61/100], Loss: 6.8862\n",
      "Epoch [62/100], Loss: 6.8858\n",
      "Epoch [63/100], Loss: 6.8857\n",
      "Epoch [64/100], Loss: 6.8861\n",
      "Epoch [65/100], Loss: 6.8865\n",
      "Epoch [66/100], Loss: 6.8858\n",
      "Epoch [67/100], Loss: 6.8857\n",
      "Epoch [68/100], Loss: 6.8862\n",
      "Epoch [69/100], Loss: 6.8856\n",
      "Epoch [70/100], Loss: 6.8859\n",
      "Epoch [71/100], Loss: 6.8856\n",
      "Epoch [72/100], Loss: 6.8863\n",
      "Epoch [73/100], Loss: 6.8856\n",
      "Epoch [74/100], Loss: 6.8860\n",
      "Epoch [75/100], Loss: 6.8857\n",
      "Epoch [76/100], Loss: 6.8856\n",
      "Epoch [77/100], Loss: 6.8859\n",
      "Epoch [78/100], Loss: 6.8856\n",
      "Epoch [79/100], Loss: 6.8862\n",
      "Epoch [80/100], Loss: 6.8856\n",
      "Epoch [81/100], Loss: 6.8856\n",
      "Epoch [82/100], Loss: 6.8857\n",
      "Epoch [83/100], Loss: 6.8858\n",
      "Epoch [84/100], Loss: 6.8856\n",
      "Epoch [85/100], Loss: 6.8858\n",
      "Epoch [86/100], Loss: 6.8857\n",
      "Epoch [87/100], Loss: 6.8856\n",
      "Epoch [88/100], Loss: 6.8855\n",
      "Epoch [89/100], Loss: 6.8861\n",
      "Epoch [90/100], Loss: 6.8858\n",
      "Epoch [91/100], Loss: 6.8854\n",
      "Epoch [92/100], Loss: 6.8856\n",
      "Epoch [93/100], Loss: 6.8857\n",
      "Epoch [94/100], Loss: 6.8857\n",
      "Epoch [95/100], Loss: 6.8856\n",
      "Epoch [96/100], Loss: 6.8858\n",
      "Epoch [97/100], Loss: 6.8858\n",
      "Epoch [98/100], Loss: 6.8858\n",
      "Epoch [99/100], Loss: 6.8858\n",
      "Epoch [100/100], Loss: 6.8855\n",
      "Epoch [1/100], Loss: 1.1010, Accuracy: 0.3778\n",
      "Epoch [2/100], Loss: 0.9779, Accuracy: 0.5081\n",
      "Epoch [3/100], Loss: 0.9459, Accuracy: 0.5395\n",
      "Epoch [4/100], Loss: 0.8799, Accuracy: 0.6083\n",
      "Epoch [5/100], Loss: 0.8503, Accuracy: 0.6349\n",
      "Epoch [6/100], Loss: 0.8111, Accuracy: 0.6789\n",
      "Epoch [7/100], Loss: 0.7533, Accuracy: 0.7103\n",
      "Epoch [8/100], Loss: 0.7188, Accuracy: 0.7447\n",
      "Epoch [9/100], Loss: 0.6689, Accuracy: 0.7731\n",
      "Epoch [10/100], Loss: 0.6375, Accuracy: 0.8063\n",
      "Epoch [11/100], Loss: 0.6017, Accuracy: 0.8407\n",
      "Epoch [12/100], Loss: 0.5579, Accuracy: 0.8612\n",
      "Epoch [13/100], Loss: 0.5103, Accuracy: 0.8841\n",
      "Epoch [14/100], Loss: 0.4859, Accuracy: 0.9016\n",
      "Epoch [15/100], Loss: 0.4388, Accuracy: 0.9137\n",
      "Epoch [16/100], Loss: 0.4101, Accuracy: 0.9264\n",
      "Epoch [17/100], Loss: 0.3628, Accuracy: 0.9439\n",
      "Epoch [18/100], Loss: 0.3245, Accuracy: 0.9505\n",
      "Epoch [19/100], Loss: 0.2854, Accuracy: 0.9596\n",
      "Epoch [20/100], Loss: 0.2581, Accuracy: 0.9668\n",
      "Epoch [21/100], Loss: 0.2277, Accuracy: 0.9747\n",
      "Epoch [22/100], Loss: 0.2000, Accuracy: 0.9807\n",
      "Epoch [23/100], Loss: 0.1826, Accuracy: 0.9831\n",
      "Epoch [24/100], Loss: 0.1568, Accuracy: 0.9855\n",
      "Epoch [25/100], Loss: 0.1385, Accuracy: 0.9897\n",
      "Epoch [26/100], Loss: 0.1225, Accuracy: 0.9891\n",
      "Epoch [27/100], Loss: 0.1055, Accuracy: 0.9946\n",
      "Epoch [28/100], Loss: 0.0946, Accuracy: 0.9964\n",
      "Epoch [29/100], Loss: 0.0830, Accuracy: 0.9988\n",
      "Epoch [30/100], Loss: 0.0722, Accuracy: 0.9988\n",
      "Epoch [31/100], Loss: 0.0616, Accuracy: 0.9988\n",
      "Epoch [32/100], Loss: 0.0578, Accuracy: 0.9994\n",
      "Epoch [33/100], Loss: 0.0487, Accuracy: 0.9994\n",
      "Epoch [34/100], Loss: 0.0465, Accuracy: 1.0000\n",
      "Epoch [35/100], Loss: 0.0383, Accuracy: 1.0000\n",
      "Epoch [36/100], Loss: 0.0350, Accuracy: 1.0000\n",
      "Epoch [37/100], Loss: 0.0331, Accuracy: 1.0000\n",
      "Epoch [38/100], Loss: 0.0313, Accuracy: 1.0000\n",
      "Epoch [39/100], Loss: 0.0271, Accuracy: 1.0000\n",
      "Epoch [40/100], Loss: 0.0241, Accuracy: 1.0000\n",
      "Epoch [41/100], Loss: 0.0223, Accuracy: 1.0000\n",
      "Epoch [42/100], Loss: 0.0202, Accuracy: 1.0000\n",
      "Epoch [43/100], Loss: 0.0208, Accuracy: 1.0000\n",
      "Epoch [44/100], Loss: 0.0182, Accuracy: 1.0000\n",
      "Epoch [45/100], Loss: 0.0172, Accuracy: 1.0000\n",
      "Epoch [46/100], Loss: 0.0156, Accuracy: 1.0000\n",
      "Epoch [47/100], Loss: 0.0138, Accuracy: 1.0000\n",
      "Epoch [48/100], Loss: 0.0135, Accuracy: 1.0000\n",
      "Epoch [49/100], Loss: 0.0130, Accuracy: 1.0000\n",
      "Epoch [50/100], Loss: 0.0123, Accuracy: 1.0000\n",
      "Epoch [51/100], Loss: 0.0119, Accuracy: 1.0000\n",
      "Epoch [52/100], Loss: 0.0103, Accuracy: 1.0000\n",
      "Epoch [53/100], Loss: 0.0105, Accuracy: 1.0000\n",
      "Epoch [54/100], Loss: 0.0102, Accuracy: 1.0000\n",
      "Epoch [55/100], Loss: 0.0085, Accuracy: 1.0000\n",
      "Epoch [56/100], Loss: 0.0094, Accuracy: 1.0000\n",
      "Epoch [57/100], Loss: 0.0081, Accuracy: 1.0000\n",
      "Epoch [58/100], Loss: 0.0075, Accuracy: 1.0000\n",
      "Epoch [59/100], Loss: 0.0074, Accuracy: 1.0000\n",
      "Epoch [60/100], Loss: 0.0071, Accuracy: 1.0000\n",
      "Epoch [61/100], Loss: 0.0067, Accuracy: 1.0000\n",
      "Epoch [62/100], Loss: 0.0065, Accuracy: 1.0000\n",
      "Epoch [63/100], Loss: 0.0064, Accuracy: 1.0000\n",
      "Epoch [64/100], Loss: 0.0061, Accuracy: 1.0000\n",
      "Epoch [65/100], Loss: 0.0057, Accuracy: 1.0000\n",
      "Epoch [66/100], Loss: 0.0057, Accuracy: 1.0000\n",
      "Epoch [67/100], Loss: 0.0053, Accuracy: 1.0000\n",
      "Epoch [68/100], Loss: 0.0056, Accuracy: 1.0000\n",
      "Epoch [69/100], Loss: 0.0050, Accuracy: 1.0000\n",
      "Epoch [70/100], Loss: 0.0049, Accuracy: 1.0000\n",
      "Epoch [71/100], Loss: 0.0049, Accuracy: 1.0000\n",
      "Epoch [72/100], Loss: 0.0046, Accuracy: 1.0000\n",
      "Epoch [73/100], Loss: 0.0043, Accuracy: 1.0000\n",
      "Epoch [74/100], Loss: 0.0042, Accuracy: 1.0000\n",
      "Epoch [75/100], Loss: 0.0041, Accuracy: 1.0000\n",
      "Epoch [76/100], Loss: 0.0041, Accuracy: 1.0000\n",
      "Epoch [77/100], Loss: 0.0038, Accuracy: 1.0000\n",
      "Epoch [78/100], Loss: 0.0038, Accuracy: 1.0000\n",
      "Epoch [79/100], Loss: 0.0040, Accuracy: 1.0000\n",
      "Epoch [80/100], Loss: 0.0035, Accuracy: 1.0000\n",
      "Epoch [81/100], Loss: 0.0034, Accuracy: 1.0000\n",
      "Epoch [82/100], Loss: 0.0033, Accuracy: 1.0000\n",
      "Epoch [83/100], Loss: 0.0033, Accuracy: 1.0000\n",
      "Epoch [84/100], Loss: 0.0031, Accuracy: 1.0000\n",
      "Epoch [85/100], Loss: 0.0031, Accuracy: 1.0000\n",
      "Epoch [86/100], Loss: 0.0029, Accuracy: 1.0000\n",
      "Epoch [87/100], Loss: 0.0030, Accuracy: 1.0000\n",
      "Epoch [88/100], Loss: 0.0029, Accuracy: 1.0000\n",
      "Epoch [89/100], Loss: 0.0028, Accuracy: 1.0000\n",
      "Epoch [90/100], Loss: 0.0028, Accuracy: 1.0000\n",
      "Epoch [91/100], Loss: 0.0030, Accuracy: 1.0000\n",
      "Epoch [92/100], Loss: 0.0026, Accuracy: 1.0000\n",
      "Epoch [93/100], Loss: 0.0030, Accuracy: 1.0000\n",
      "Epoch [94/100], Loss: 0.0024, Accuracy: 1.0000\n",
      "Epoch [95/100], Loss: 0.0023, Accuracy: 1.0000\n",
      "Epoch [96/100], Loss: 0.0024, Accuracy: 1.0000\n",
      "Epoch [97/100], Loss: 0.0022, Accuracy: 1.0000\n",
      "Epoch [98/100], Loss: 0.0022, Accuracy: 1.0000\n",
      "Epoch [99/100], Loss: 0.0021, Accuracy: 1.0000\n",
      "Epoch [100/100], Loss: 0.0021, Accuracy: 1.0000\n",
      "Test Accuracy: 85.6655\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "train_images_path = \"brain_train_image_final.npy\"\n",
    "train_labels_path = \"brain_train_label.npy\"\n",
    "test_images_path = \"brain_test_image_final.npy\"\n",
    "test_labels_path = \"brain_test_label.npy\"\n",
    "\n",
    "# Load the data\n",
    "final_X_train_modified = np.load(train_images_path)[:, 1, :, :]\n",
    "final_X_test_modified = np.load(test_images_path)[:, 1, :, :]\n",
    "train_labels = np.load(train_labels_path)\n",
    "test_labels = np.load(test_labels_path)\n",
    "\n",
    "# Normalize and Resize Images using Pillow\n",
    "def normalize_and_resize(images, target_size=(224, 224)):\n",
    "    resized_images = []\n",
    "    for img in images:\n",
    "        img = Image.fromarray((img * 255).astype(np.uint8))\n",
    "        img_resized = img.resize(target_size, Image.Resampling.LANCZOS)\n",
    "        resized_images.append(np.array(img_resized) / 255.0)\n",
    "    return np.array(resized_images)\n",
    "\n",
    "final_X_train_resized = normalize_and_resize(final_X_train_modified)\n",
    "final_X_test_resized = normalize_and_resize(final_X_test_modified)\n",
    "\n",
    "# Define SimCLR Augmentation Transform\n",
    "transform_simclr = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0), ratio=(3/4, 4/3)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.8, contrast=0.8, saturation=0.8, hue=0.2),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=23, sigma=(0.1, 2.0))], p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Custom Dataset for SimCLR\n",
    "class SimCLRDataset(Dataset):\n",
    "    def __init__(self, images, transform):\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        img_1 = self.transform(img)\n",
    "        img_2 = self.transform(img)\n",
    "        return img_1, img_2\n",
    "\n",
    "train_dataset = SimCLRDataset(final_X_train_resized, transform_simclr)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "# Define SimCLR Model\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, base_encoder, projection_dim):\n",
    "        super(SimCLR, self).__init__()\n",
    "        self.encoder = base_encoder\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(512, 256),  # Match encoder output size\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, projection_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        z = self.projector(h)\n",
    "        return z\n",
    "\n",
    "\n",
    "# Define NT-Xent Loss\n",
    "class NTXentLoss(nn.Module):\n",
    "    def __init__(self, temperature):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        N = z_i.size(0) + z_j.size(0)\n",
    "        z = torch.cat((z_i, z_j), dim=0)\n",
    "        sim = torch.mm(z, z.T) / self.temperature\n",
    "        sim = torch.nn.functional.softmax(sim, dim=1)\n",
    "\n",
    "        labels = torch.cat([\n",
    "            torch.arange(z_i.size(0), device=z.device),\n",
    "            torch.arange(z_j.size(0), device=z.device)\n",
    "        ])\n",
    "        loss = self.criterion(sim, labels)\n",
    "        return loss\n",
    "\n",
    "# Initialize ResNet-18 Encoder\n",
    "base_encoder = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "base_encoder.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "base_encoder.fc = nn.Identity()\n",
    "\n",
    "# Initialize SimCLR Model\n",
    "model = SimCLR(base_encoder, projection_dim=128).to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion = NTXentLoss(temperature=0.5)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "# Train SimCLR Model\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for img_1, img_2 in train_loader:\n",
    "        img_1, img_2 = img_1.to(\"cuda\"), img_2.to(\"cuda\")\n",
    "        z_i = model(img_1)\n",
    "        z_j = model(img_2)\n",
    "\n",
    "        loss = criterion(z_i, z_j)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch [{epoch+1}/100], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Define Dataset for Classification\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# Initialize Training and Test Dataset and DataLoader\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "train_dataset = TestDataset(final_X_train_resized, train_labels, transform=train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "test_dataset = TestDataset(final_X_test_resized, test_labels, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "# Add Classification Head\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "classification_head = ClassificationHead(input_dim=512, num_classes=len(np.unique(train_labels))).to(\"cuda\")\n",
    "\n",
    "optimizer_cls = optim.Adam([\n",
    "    {\"params\": model.encoder.parameters(), \"lr\": 1e-5},\n",
    "    {\"params\": classification_head.parameters(), \"lr\": 3e-4},\n",
    "])\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "\n",
    "# Fine-tune Classification Head\n",
    "for epoch in range(100):\n",
    "    model.encoder.train()\n",
    "    classification_head.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for img, label in train_loader:\n",
    "        img, label = img.to(\"cuda\"), label.to(\"cuda\")\n",
    "        features = model.encoder(img)\n",
    "        logits = classification_head(features)\n",
    "        loss = criterion_cls(logits, label)\n",
    "\n",
    "        optimizer_cls.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_cls.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct += (logits.argmax(dim=1) == label).sum().item()\n",
    "\n",
    "    accuracy = correct / len(train_labels)\n",
    "    print(f\"Epoch [{epoch+1}/100], Loss: {total_loss/len(train_loader):.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate on Test Dataset\n",
    "classification_head.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for img, label in test_loader:\n",
    "        img, label = img.to(\"cuda\"), label.to(\"cuda\")\n",
    "        features = model.encoder(img)\n",
    "        logits = classification_head(features)\n",
    "        correct += (logits.argmax(dim=1) == label).sum().item()\n",
    "\n",
    "test_accuracy = correct / len(test_labels)\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.4.0",
   "language": "python",
   "name": "pytorch-2.4.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
