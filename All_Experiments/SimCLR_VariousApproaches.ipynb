{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5d35553b-5bd9-4b45-abdd-b657fdc72197",
      "metadata": {
        "id": "5d35553b-5bd9-4b45-abdd-b657fdc72197"
      },
      "source": [
        "# The Below one got 90% accuracy, with ResNet18\n",
        "\n",
        "#Training Procedure**\n",
        "- **Contrastive Learning Stage**:\n",
        "  - The SimCLR model is trained using the contrastive loss on augmented image pairs.\n",
        "  - Key hyperparameters:\n",
        "    - Batch size: `512`\n",
        "    - Learning rate: `1e-4`\n",
        "    - Temperature: `0.5`\n",
        "  - The Adam optimizer is used with a `StepLR` scheduler for learning rate decay.\n",
        "- **Training Outputs**:\n",
        "  - Loss values are logged for each epoch to monitor the learning process.\n",
        "\n",
        "---\n",
        "\n",
        "## Downstream Task: Classification**\n",
        "- **Linear Evaluation Protocol**:\n",
        "  - The pre-trained encoder is fine-tuned with a classification head for supervised learning on the labeled dataset.\n",
        "  - Classification head:\n",
        "    - A single linear layer mapping encoder outputs (`512`) to the number of classes.\n",
        "  - Fine-tuning:\n",
        "    - Encoder weights are updated at a slower learning rate (`1e-5`) than the classification head (`1e-3`).\n",
        "    - Cross-entropy loss is used for optimization.\n",
        "- **Performance Evaluation**:\n",
        "  - Test accuracy is computed on the held-out test set to measure the model's performance.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b13db38a-11d6-4b1c-97ed-a3f34551607c",
      "metadata": {
        "tags": [],
        "id": "b13db38a-11d6-4b1c-97ed-a3f34551607c",
        "outputId": "8f6ceeca-2ef6-47b4-a4a8-caf9bca6840c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Loss: 8.1232\n",
            "Epoch [2/100], Loss: 6.6977\n",
            "Epoch [3/100], Loss: 6.5628\n",
            "Epoch [4/100], Loss: 6.4204\n",
            "Epoch [5/100], Loss: 6.2939\n",
            "Epoch [6/100], Loss: 6.2331\n",
            "Epoch [7/100], Loss: 6.1769\n",
            "Epoch [8/100], Loss: 6.0556\n",
            "Epoch [9/100], Loss: 5.9819\n",
            "Epoch [10/100], Loss: 5.8945\n",
            "Epoch [11/100], Loss: 5.7674\n",
            "Epoch [12/100], Loss: 5.6089\n",
            "Epoch [13/100], Loss: 5.5685\n",
            "Epoch [14/100], Loss: 5.4512\n",
            "Epoch [15/100], Loss: 5.3110\n",
            "Epoch [16/100], Loss: 5.1675\n",
            "Epoch [17/100], Loss: 5.1007\n",
            "Epoch [18/100], Loss: 4.9024\n",
            "Epoch [19/100], Loss: 4.8179\n",
            "Epoch [20/100], Loss: 4.7292\n",
            "Epoch [21/100], Loss: 4.6493\n",
            "Epoch [22/100], Loss: 4.5762\n",
            "Epoch [23/100], Loss: 4.4276\n",
            "Epoch [24/100], Loss: 4.3101\n",
            "Epoch [25/100], Loss: 4.2424\n",
            "Epoch [26/100], Loss: 4.1504\n",
            "Epoch [27/100], Loss: 4.0430\n",
            "Epoch [28/100], Loss: 3.9242\n",
            "Epoch [29/100], Loss: 3.8458\n",
            "Epoch [30/100], Loss: 3.7267\n",
            "Epoch [31/100], Loss: 3.8131\n",
            "Epoch [32/100], Loss: 3.7184\n",
            "Epoch [33/100], Loss: 3.6482\n",
            "Epoch [34/100], Loss: 3.5053\n",
            "Epoch [35/100], Loss: 3.5145\n",
            "Epoch [36/100], Loss: 3.5030\n",
            "Epoch [37/100], Loss: 3.3975\n",
            "Epoch [38/100], Loss: 3.4146\n",
            "Epoch [39/100], Loss: 3.3098\n",
            "Epoch [40/100], Loss: 3.2722\n",
            "Epoch [41/100], Loss: 3.3186\n",
            "Epoch [42/100], Loss: 3.2364\n",
            "Epoch [43/100], Loss: 3.2709\n",
            "Epoch [44/100], Loss: 3.1460\n",
            "Epoch [45/100], Loss: 3.1851\n",
            "Epoch [46/100], Loss: 3.0631\n",
            "Epoch [47/100], Loss: 3.0215\n",
            "Epoch [48/100], Loss: 2.9695\n",
            "Epoch [49/100], Loss: 3.0149\n",
            "Epoch [50/100], Loss: 2.9137\n",
            "Epoch [51/100], Loss: 2.9919\n",
            "Epoch [52/100], Loss: 2.9731\n",
            "Epoch [53/100], Loss: 2.9027\n",
            "Epoch [54/100], Loss: 2.9106\n",
            "Epoch [55/100], Loss: 2.7613\n",
            "Epoch [56/100], Loss: 2.8544\n",
            "Epoch [57/100], Loss: 2.7912\n",
            "Epoch [58/100], Loss: 2.8075\n",
            "Epoch [59/100], Loss: 2.8088\n",
            "Epoch [60/100], Loss: 2.7960\n",
            "Epoch [61/100], Loss: 2.7302\n",
            "Epoch [62/100], Loss: 2.6979\n",
            "Epoch [63/100], Loss: 2.7768\n",
            "Epoch [64/100], Loss: 2.5908\n",
            "Epoch [65/100], Loss: 2.6183\n",
            "Epoch [66/100], Loss: 2.6636\n",
            "Epoch [67/100], Loss: 2.7368\n",
            "Epoch [68/100], Loss: 2.5110\n",
            "Epoch [69/100], Loss: 2.5872\n",
            "Epoch [70/100], Loss: 2.5331\n",
            "Epoch [71/100], Loss: 2.6693\n",
            "Epoch [72/100], Loss: 2.6570\n",
            "Epoch [73/100], Loss: 2.6254\n",
            "Epoch [74/100], Loss: 2.6197\n",
            "Epoch [75/100], Loss: 2.6155\n",
            "Epoch [76/100], Loss: 2.5663\n",
            "Epoch [77/100], Loss: 2.5080\n",
            "Epoch [78/100], Loss: 2.4358\n",
            "Epoch [79/100], Loss: 2.4760\n",
            "Epoch [80/100], Loss: 2.4601\n",
            "Epoch [81/100], Loss: 2.5365\n",
            "Epoch [82/100], Loss: 2.5075\n",
            "Epoch [83/100], Loss: 2.3272\n",
            "Epoch [84/100], Loss: 2.3784\n",
            "Epoch [85/100], Loss: 2.4384\n",
            "Epoch [86/100], Loss: 2.3695\n",
            "Epoch [87/100], Loss: 2.4377\n",
            "Epoch [88/100], Loss: 2.4123\n",
            "Epoch [89/100], Loss: 2.4479\n",
            "Epoch [90/100], Loss: 2.4060\n",
            "Epoch [91/100], Loss: 2.3577\n",
            "Epoch [92/100], Loss: 2.3522\n",
            "Epoch [93/100], Loss: 2.4021\n",
            "Epoch [94/100], Loss: 2.4130\n",
            "Epoch [95/100], Loss: 2.3261\n",
            "Epoch [96/100], Loss: 2.4397\n",
            "Epoch [97/100], Loss: 2.3650\n",
            "Epoch [98/100], Loss: 2.2320\n",
            "Epoch [99/100], Loss: 2.3905\n",
            "Epoch [100/100], Loss: 2.2970\n",
            "Epoch [1/100], Loss: 1.0364, Accuracy: 0.4568\n",
            "Epoch [2/100], Loss: 0.9136, Accuracy: 0.5673\n",
            "Epoch [3/100], Loss: 0.8605, Accuracy: 0.6150\n",
            "Epoch [4/100], Loss: 0.7857, Accuracy: 0.6693\n",
            "Epoch [5/100], Loss: 0.7083, Accuracy: 0.7206\n",
            "Epoch [6/100], Loss: 0.6258, Accuracy: 0.7719\n",
            "Epoch [7/100], Loss: 0.5381, Accuracy: 0.8159\n",
            "Epoch [8/100], Loss: 0.4524, Accuracy: 0.8582\n",
            "Epoch [9/100], Loss: 0.3609, Accuracy: 0.9028\n",
            "Epoch [10/100], Loss: 0.2725, Accuracy: 0.9390\n",
            "Epoch [11/100], Loss: 0.2193, Accuracy: 0.9559\n",
            "Epoch [12/100], Loss: 0.1906, Accuracy: 0.9662\n",
            "Epoch [13/100], Loss: 0.1556, Accuracy: 0.9819\n",
            "Epoch [14/100], Loss: 0.1338, Accuracy: 0.9867\n",
            "Epoch [15/100], Loss: 0.1124, Accuracy: 0.9922\n",
            "Epoch [16/100], Loss: 0.0980, Accuracy: 0.9934\n",
            "Epoch [17/100], Loss: 0.0812, Accuracy: 0.9952\n",
            "Epoch [18/100], Loss: 0.0727, Accuracy: 0.9976\n",
            "Epoch [19/100], Loss: 0.0609, Accuracy: 0.9976\n",
            "Epoch [20/100], Loss: 0.0494, Accuracy: 0.9988\n",
            "Epoch [21/100], Loss: 0.0446, Accuracy: 0.9988\n",
            "Epoch [22/100], Loss: 0.0400, Accuracy: 0.9994\n",
            "Epoch [23/100], Loss: 0.0377, Accuracy: 0.9988\n",
            "Epoch [24/100], Loss: 0.0380, Accuracy: 0.9994\n",
            "Epoch [25/100], Loss: 0.0358, Accuracy: 0.9994\n",
            "Epoch [26/100], Loss: 0.0309, Accuracy: 0.9994\n",
            "Epoch [27/100], Loss: 0.0302, Accuracy: 0.9994\n",
            "Epoch [28/100], Loss: 0.0278, Accuracy: 0.9994\n",
            "Epoch [29/100], Loss: 0.0262, Accuracy: 0.9994\n",
            "Epoch [30/100], Loss: 0.0250, Accuracy: 0.9994\n",
            "Epoch [31/100], Loss: 0.0229, Accuracy: 0.9994\n",
            "Epoch [32/100], Loss: 0.0224, Accuracy: 0.9994\n",
            "Epoch [33/100], Loss: 0.0227, Accuracy: 0.9994\n",
            "Epoch [34/100], Loss: 0.0203, Accuracy: 0.9994\n",
            "Epoch [35/100], Loss: 0.0194, Accuracy: 1.0000\n",
            "Epoch [36/100], Loss: 0.0192, Accuracy: 0.9994\n",
            "Epoch [37/100], Loss: 0.0183, Accuracy: 1.0000\n",
            "Epoch [38/100], Loss: 0.0178, Accuracy: 1.0000\n",
            "Epoch [39/100], Loss: 0.0183, Accuracy: 1.0000\n",
            "Epoch [40/100], Loss: 0.0184, Accuracy: 1.0000\n",
            "Epoch [41/100], Loss: 0.0171, Accuracy: 1.0000\n",
            "Epoch [42/100], Loss: 0.0171, Accuracy: 1.0000\n",
            "Epoch [43/100], Loss: 0.0171, Accuracy: 1.0000\n",
            "Epoch [44/100], Loss: 0.0158, Accuracy: 1.0000\n",
            "Epoch [45/100], Loss: 0.0151, Accuracy: 1.0000\n",
            "Epoch [46/100], Loss: 0.0144, Accuracy: 1.0000\n",
            "Epoch [47/100], Loss: 0.0164, Accuracy: 1.0000\n",
            "Epoch [48/100], Loss: 0.0155, Accuracy: 1.0000\n",
            "Epoch [49/100], Loss: 0.0138, Accuracy: 1.0000\n",
            "Epoch [50/100], Loss: 0.0151, Accuracy: 1.0000\n",
            "Epoch [51/100], Loss: 0.0139, Accuracy: 1.0000\n",
            "Epoch [52/100], Loss: 0.0135, Accuracy: 1.0000\n",
            "Epoch [53/100], Loss: 0.0142, Accuracy: 1.0000\n",
            "Epoch [54/100], Loss: 0.0147, Accuracy: 1.0000\n",
            "Epoch [55/100], Loss: 0.0150, Accuracy: 1.0000\n",
            "Epoch [56/100], Loss: 0.0145, Accuracy: 1.0000\n",
            "Epoch [57/100], Loss: 0.0143, Accuracy: 1.0000\n",
            "Epoch [58/100], Loss: 0.0151, Accuracy: 1.0000\n",
            "Epoch [59/100], Loss: 0.0124, Accuracy: 1.0000\n",
            "Epoch [60/100], Loss: 0.0128, Accuracy: 1.0000\n",
            "Epoch [61/100], Loss: 0.0125, Accuracy: 1.0000\n",
            "Epoch [62/100], Loss: 0.0125, Accuracy: 1.0000\n",
            "Epoch [63/100], Loss: 0.0125, Accuracy: 1.0000\n",
            "Epoch [64/100], Loss: 0.0124, Accuracy: 1.0000\n",
            "Epoch [65/100], Loss: 0.0127, Accuracy: 1.0000\n",
            "Epoch [66/100], Loss: 0.0121, Accuracy: 1.0000\n",
            "Epoch [67/100], Loss: 0.0136, Accuracy: 1.0000\n",
            "Epoch [68/100], Loss: 0.0117, Accuracy: 1.0000\n",
            "Epoch [69/100], Loss: 0.0125, Accuracy: 1.0000\n",
            "Epoch [70/100], Loss: 0.0123, Accuracy: 1.0000\n",
            "Epoch [71/100], Loss: 0.0129, Accuracy: 1.0000\n",
            "Epoch [72/100], Loss: 0.0122, Accuracy: 1.0000\n",
            "Epoch [73/100], Loss: 0.0125, Accuracy: 1.0000\n",
            "Epoch [74/100], Loss: 0.0130, Accuracy: 1.0000\n",
            "Epoch [75/100], Loss: 0.0125, Accuracy: 1.0000\n",
            "Epoch [76/100], Loss: 0.0132, Accuracy: 1.0000\n",
            "Epoch [77/100], Loss: 0.0128, Accuracy: 1.0000\n",
            "Epoch [78/100], Loss: 0.0127, Accuracy: 1.0000\n",
            "Epoch [79/100], Loss: 0.0122, Accuracy: 1.0000\n",
            "Epoch [80/100], Loss: 0.0123, Accuracy: 1.0000\n",
            "Epoch [81/100], Loss: 0.0118, Accuracy: 1.0000\n",
            "Epoch [82/100], Loss: 0.0120, Accuracy: 1.0000\n",
            "Epoch [83/100], Loss: 0.0120, Accuracy: 1.0000\n",
            "Epoch [84/100], Loss: 0.0120, Accuracy: 1.0000\n",
            "Epoch [85/100], Loss: 0.0131, Accuracy: 1.0000\n",
            "Epoch [86/100], Loss: 0.0118, Accuracy: 1.0000\n",
            "Epoch [87/100], Loss: 0.0125, Accuracy: 1.0000\n",
            "Epoch [88/100], Loss: 0.0145, Accuracy: 1.0000\n",
            "Epoch [89/100], Loss: 0.0117, Accuracy: 1.0000\n",
            "Epoch [90/100], Loss: 0.0116, Accuracy: 1.0000\n",
            "Epoch [91/100], Loss: 0.0121, Accuracy: 1.0000\n",
            "Epoch [92/100], Loss: 0.0137, Accuracy: 1.0000\n",
            "Epoch [93/100], Loss: 0.0131, Accuracy: 1.0000\n",
            "Epoch [94/100], Loss: 0.0124, Accuracy: 1.0000\n",
            "Epoch [95/100], Loss: 0.0127, Accuracy: 1.0000\n",
            "Epoch [96/100], Loss: 0.0128, Accuracy: 1.0000\n",
            "Epoch [97/100], Loss: 0.0117, Accuracy: 1.0000\n",
            "Epoch [98/100], Loss: 0.0114, Accuracy: 1.0000\n",
            "Epoch [99/100], Loss: 0.0126, Accuracy: 1.0000\n",
            "Epoch [100/100], Loss: 0.0116, Accuracy: 1.0000\n",
            "Test Accuracy: 0.9044\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet18\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "train_images_path = \"brain_train_image_final.npy\"\n",
        "train_labels_path = \"brain_train_label.npy\"\n",
        "test_images_path = \"brain_test_image_final.npy\"\n",
        "test_labels_path = \"brain_test_label.npy\"\n",
        "\n",
        "# Load the data\n",
        "final_X_train_modified = np.load(train_images_path)[:, 1, :, :]\n",
        "final_X_test_modified = np.load(test_images_path)[:, 1, :, :]\n",
        "train_labels = np.load(train_labels_path)\n",
        "test_labels = np.load(test_labels_path)\n",
        "\n",
        "# Normalize and Resize Images using Pillow\n",
        "def normalize_and_resize(images, target_size=(224, 224)):\n",
        "    resized_images = []\n",
        "    for img in images:\n",
        "        img = Image.fromarray((img * 255).astype(np.uint8))\n",
        "        img_resized = img.resize(target_size, Image.Resampling.LANCZOS)\n",
        "        resized_images.append(np.array(img_resized) / 255.0)\n",
        "    return np.array(resized_images)\n",
        "\n",
        "final_X_train_resized = normalize_and_resize(final_X_train_modified)\n",
        "final_X_test_resized = normalize_and_resize(final_X_test_modified)\n",
        "\n",
        "# Define SimCLR Augmentation Transform\n",
        "transform_simclr = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomResizedCrop(size=224),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1),\n",
        "    transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# Custom Dataset for SimCLR\n",
        "class SimCLRDataset(Dataset):\n",
        "    def __init__(self, images, transform):\n",
        "        self.images = images\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]\n",
        "        img_1 = self.transform(img)\n",
        "        img_2 = self.transform(img)\n",
        "        return img_1, img_2\n",
        "\n",
        "train_dataset = SimCLRDataset(final_X_train_resized, transform_simclr)\n",
        "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
        "\n",
        "# Define SimCLR Model\n",
        "class SimCLR(nn.Module):\n",
        "    def __init__(self, base_encoder, projection_dim):\n",
        "        super(SimCLR, self).__init__()\n",
        "        self.encoder = base_encoder\n",
        "        self.projector = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, projection_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.encoder(x)\n",
        "        z = self.projector(h)\n",
        "        return z\n",
        "\n",
        "# Define NT-Xent Loss\n",
        "class NTXentLoss(nn.Module):\n",
        "    def __init__(self, batch_size, temperature):\n",
        "        super(NTXentLoss, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.temperature = temperature\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "\n",
        "    def forward(self, z_i, z_j):\n",
        "        N = z_i.size(0) + z_j.size(0)\n",
        "        z = torch.cat((z_i, z_j), dim=0)\n",
        "        sim = torch.matmul(z, z.T) / self.temperature\n",
        "        mask = ~torch.eye(N, dtype=torch.bool, device=z.device)\n",
        "\n",
        "        positives = torch.cat([\n",
        "            torch.diag(sim, z_i.size(0)),\n",
        "            torch.diag(sim, -z_i.size(0))\n",
        "        ])\n",
        "\n",
        "        negatives = sim[mask].view(N, -1)\n",
        "        logits = torch.cat((positives.unsqueeze(1), negatives), dim=1)\n",
        "        labels = torch.zeros(N, dtype=torch.long, device=z.device)\n",
        "        loss = self.criterion(logits, labels) / N\n",
        "        return loss\n",
        "\n",
        "# Initialize ResNet-18 Encoder\n",
        "base_encoder = resnet18(pretrained=True)\n",
        "base_encoder.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "base_encoder.fc = nn.Identity()\n",
        "\n",
        "# Initialize SimCLR Model\n",
        "model = SimCLR(base_encoder, projection_dim=128).to(\"cuda\")\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = NTXentLoss(batch_size=512, temperature=0.5)\n",
        "\n",
        "# Train SimCLR Model\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    for img_1, img_2 in train_loader:\n",
        "        img_1, img_2 = img_1.to(\"cuda\"), img_2.to(\"cuda\")\n",
        "        z_i = model(img_1)\n",
        "        z_j = model(img_2)\n",
        "\n",
        "        loss = criterion(z_i, z_j)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/100], Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "# Define Dataset for Classification\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "# Initialize Training and Test Dataset and DataLoader\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "train_dataset = TestDataset(final_X_train_resized, train_labels, transform=train_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "test_dataset = TestDataset(final_X_test_resized, test_labels, transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# Add Classification Head\n",
        "class ClassificationHead(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(ClassificationHead, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "classification_head = ClassificationHead(input_dim=512, num_classes=len(np.unique(train_labels))).to(\"cuda\")\n",
        "optimizer_cls = optim.Adam([\n",
        "    {\"params\": model.encoder.parameters(), \"lr\": 1e-5},\n",
        "    {\"params\": classification_head.parameters(), \"lr\": 1e-3},\n",
        "])\n",
        "scheduler_cls = StepLR(optimizer_cls, step_size=10, gamma=0.5)\n",
        "criterion_cls = nn.CrossEntropyLoss()\n",
        "\n",
        "# Fine-tune Classification Head\n",
        "for epoch in range(100):\n",
        "    model.encoder.train()\n",
        "    classification_head.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    for img, label in DataLoader(train_dataset, batch_size=128, shuffle=True):\n",
        "        img, label = img.to(\"cuda\"), label.to(\"cuda\")\n",
        "        features = model.encoder(img)\n",
        "        logits = classification_head(features)\n",
        "        loss = criterion_cls(logits, label)\n",
        "\n",
        "        optimizer_cls.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_cls.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        correct += (logits.argmax(dim=1) == label).sum().item()\n",
        "\n",
        "    accuracy = correct / len(train_labels)\n",
        "    scheduler_cls.step()\n",
        "    print(f\"Epoch [{epoch+1}/100], Loss: {total_loss/len(train_loader):.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Evaluate on Test Dataset\n",
        "classification_head.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for img, label in DataLoader(test_dataset, batch_size=128, shuffle=False):\n",
        "        img, label = img.to(\"cuda\"), label.to(\"cuda\")\n",
        "        features = model.encoder(img)\n",
        "        logits = classification_head(features)\n",
        "        correct += (logits.argmax(dim=1) == label).sum().item()\n",
        "\n",
        "test_accuracy = correct / len(test_labels)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1455e541-f986-47fd-a45f-b0e61d7d0461",
      "metadata": {
        "id": "1455e541-f986-47fd-a45f-b0e61d7d0461"
      },
      "source": [
        "# The below one got 85% accuracy, with ResNet34\n",
        "\n",
        "# SimCLR Implementation with ResNet-34\n",
        "\n",
        "## Training Procedure\n",
        "- **Contrastive Learning Stage**:\n",
        "  - The SimCLR model is trained on the augmented pairs using contrastive loss.\n",
        "  - Key hyperparameters:\n",
        "    - Batch size: `512`\n",
        "    - Learning rate: `3e-4`\n",
        "    - Temperature: `0.5`\n",
        "  - Optimizer: Adam\n",
        "  - Scheduler: `CosineAnnealingLR` with `T_max=100` for smooth learning rate decay.\n",
        "\n",
        "---\n",
        "\n",
        "## Downstream Task: Classification\n",
        "- **Linear Evaluation Protocol**:\n",
        "  - The pre-trained encoder is fine-tuned with a linear classification head for the labeled dataset.\n",
        "  - Classification head:\n",
        "    - A single `Linear(512, num_classes)` layer maps encoder features to class logits.\n",
        "  - Fine-tuning strategy:\n",
        "    - Encoder weights updated at a slower learning rate (`1e-5`) compared to the classification head (`3e-4`).\n",
        "    - Cross-entropy loss is used for optimization.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09e7e3ce-eff8-460e-aced-e3ccd9d25b33",
      "metadata": {
        "id": "09e7e3ce-eff8-460e-aced-e3ccd9d25b33",
        "outputId": "c134bda7-21b8-4988-a969-9a7183b6b2fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Loss: 6.3819\n",
            "Epoch [2/100], Loss: 6.1793\n",
            "Epoch [3/100], Loss: 6.1024\n",
            "Epoch [4/100], Loss: 6.0910\n",
            "Epoch [5/100], Loss: 6.0812\n",
            "Epoch [6/100], Loss: 6.0800\n",
            "Epoch [7/100], Loss: 6.0776\n",
            "Epoch [8/100], Loss: 6.0784\n",
            "Epoch [9/100], Loss: 6.0793\n",
            "Epoch [10/100], Loss: 6.0791\n",
            "Epoch [11/100], Loss: 6.0776\n",
            "Epoch [12/100], Loss: 6.0777\n",
            "Epoch [13/100], Loss: 6.0779\n",
            "Epoch [14/100], Loss: 6.0781\n",
            "Epoch [15/100], Loss: 6.0788\n",
            "Epoch [16/100], Loss: 6.0781\n",
            "Epoch [17/100], Loss: 6.0792\n",
            "Epoch [18/100], Loss: 6.0778\n",
            "Epoch [19/100], Loss: 6.0781\n",
            "Epoch [20/100], Loss: 6.0769\n",
            "Epoch [21/100], Loss: 6.0781\n",
            "Epoch [22/100], Loss: 6.0765\n",
            "Epoch [23/100], Loss: 6.0763\n",
            "Epoch [24/100], Loss: 6.0758\n",
            "Epoch [25/100], Loss: 6.0762\n",
            "Epoch [26/100], Loss: 6.0758\n",
            "Epoch [27/100], Loss: 6.0760\n",
            "Epoch [28/100], Loss: 6.0758\n",
            "Epoch [29/100], Loss: 6.0762\n",
            "Epoch [30/100], Loss: 6.0770\n",
            "Epoch [31/100], Loss: 6.0752\n",
            "Epoch [32/100], Loss: 6.0767\n",
            "Epoch [33/100], Loss: 6.0752\n",
            "Epoch [34/100], Loss: 6.0766\n",
            "Epoch [35/100], Loss: 6.0759\n",
            "Epoch [36/100], Loss: 6.0756\n",
            "Epoch [37/100], Loss: 6.0752\n",
            "Epoch [38/100], Loss: 6.0748\n",
            "Epoch [39/100], Loss: 6.0746\n",
            "Epoch [40/100], Loss: 6.0751\n",
            "Epoch [41/100], Loss: 6.0748\n",
            "Epoch [42/100], Loss: 6.0747\n",
            "Epoch [43/100], Loss: 6.0747\n",
            "Epoch [44/100], Loss: 6.0746\n",
            "Epoch [45/100], Loss: 6.0744\n",
            "Epoch [46/100], Loss: 6.0746\n",
            "Epoch [47/100], Loss: 6.0747\n",
            "Epoch [48/100], Loss: 6.0744\n",
            "Epoch [49/100], Loss: 6.0743\n",
            "Epoch [50/100], Loss: 6.0749\n",
            "Epoch [51/100], Loss: 6.0741\n",
            "Epoch [52/100], Loss: 6.0750\n",
            "Epoch [53/100], Loss: 6.0747\n",
            "Epoch [54/100], Loss: 6.0745\n",
            "Epoch [55/100], Loss: 6.0748\n",
            "Epoch [56/100], Loss: 6.0743\n",
            "Epoch [57/100], Loss: 6.0741\n",
            "Epoch [58/100], Loss: 6.0741\n",
            "Epoch [59/100], Loss: 6.0747\n",
            "Epoch [60/100], Loss: 6.0743\n",
            "Epoch [61/100], Loss: 6.0744\n",
            "Epoch [62/100], Loss: 6.0741\n",
            "Epoch [63/100], Loss: 6.0741\n",
            "Epoch [64/100], Loss: 6.0742\n",
            "Epoch [65/100], Loss: 6.0743\n",
            "Epoch [66/100], Loss: 6.0741\n",
            "Epoch [67/100], Loss: 6.0742\n",
            "Epoch [68/100], Loss: 6.0740\n",
            "Epoch [69/100], Loss: 6.0740\n",
            "Epoch [70/100], Loss: 6.0740\n",
            "Epoch [71/100], Loss: 6.0741\n",
            "Epoch [72/100], Loss: 6.0744\n",
            "Epoch [73/100], Loss: 6.0742\n",
            "Epoch [74/100], Loss: 6.0740\n",
            "Epoch [75/100], Loss: 6.0741\n",
            "Epoch [76/100], Loss: 6.0744\n",
            "Epoch [77/100], Loss: 6.0740\n",
            "Epoch [78/100], Loss: 6.0740\n",
            "Epoch [79/100], Loss: 6.0740\n",
            "Epoch [80/100], Loss: 6.0743\n",
            "Epoch [81/100], Loss: 6.0740\n",
            "Epoch [82/100], Loss: 6.0740\n",
            "Epoch [83/100], Loss: 6.0742\n",
            "Epoch [84/100], Loss: 6.0741\n",
            "Epoch [85/100], Loss: 6.0740\n",
            "Epoch [86/100], Loss: 6.0740\n",
            "Epoch [87/100], Loss: 6.0740\n",
            "Epoch [88/100], Loss: 6.0740\n",
            "Epoch [89/100], Loss: 6.0740\n",
            "Epoch [90/100], Loss: 6.0741\n",
            "Epoch [91/100], Loss: 6.0740\n",
            "Epoch [92/100], Loss: 6.0740\n",
            "Epoch [93/100], Loss: 6.0742\n",
            "Epoch [94/100], Loss: 6.0739\n",
            "Epoch [95/100], Loss: 6.0740\n",
            "Epoch [96/100], Loss: 6.0739\n",
            "Epoch [97/100], Loss: 6.0740\n",
            "Epoch [98/100], Loss: 6.0739\n",
            "Epoch [99/100], Loss: 6.0740\n",
            "Epoch [100/100], Loss: 6.0740\n",
            "Epoch [1/100], Loss: 1.2170, Accuracy: 0.3138\n",
            "Epoch [2/100], Loss: 0.9976, Accuracy: 0.5027\n",
            "Epoch [3/100], Loss: 0.9441, Accuracy: 0.5305\n",
            "Epoch [4/100], Loss: 0.8730, Accuracy: 0.5957\n",
            "Epoch [5/100], Loss: 0.7872, Accuracy: 0.6807\n",
            "Epoch [6/100], Loss: 0.7450, Accuracy: 0.7224\n",
            "Epoch [7/100], Loss: 0.6792, Accuracy: 0.7622\n",
            "Epoch [8/100], Loss: 0.6192, Accuracy: 0.8033\n",
            "Epoch [9/100], Loss: 0.5490, Accuracy: 0.8352\n",
            "Epoch [10/100], Loss: 0.4917, Accuracy: 0.8648\n",
            "Epoch [11/100], Loss: 0.4374, Accuracy: 0.8884\n",
            "Epoch [12/100], Loss: 0.3851, Accuracy: 0.9119\n",
            "Epoch [13/100], Loss: 0.3391, Accuracy: 0.9294\n",
            "Epoch [14/100], Loss: 0.2777, Accuracy: 0.9511\n",
            "Epoch [15/100], Loss: 0.2290, Accuracy: 0.9614\n",
            "Epoch [16/100], Loss: 0.1945, Accuracy: 0.9698\n",
            "Epoch [17/100], Loss: 0.1584, Accuracy: 0.9765\n",
            "Epoch [18/100], Loss: 0.1318, Accuracy: 0.9855\n",
            "Epoch [19/100], Loss: 0.1063, Accuracy: 0.9909\n",
            "Epoch [20/100], Loss: 0.0847, Accuracy: 0.9934\n",
            "Epoch [21/100], Loss: 0.0657, Accuracy: 0.9964\n",
            "Epoch [22/100], Loss: 0.0518, Accuracy: 0.9964\n",
            "Epoch [23/100], Loss: 0.0425, Accuracy: 0.9994\n",
            "Epoch [24/100], Loss: 0.0356, Accuracy: 1.0000\n",
            "Epoch [25/100], Loss: 0.0304, Accuracy: 1.0000\n",
            "Epoch [26/100], Loss: 0.0241, Accuracy: 1.0000\n",
            "Epoch [27/100], Loss: 0.0245, Accuracy: 1.0000\n",
            "Epoch [28/100], Loss: 0.0194, Accuracy: 1.0000\n",
            "Epoch [29/100], Loss: 0.0152, Accuracy: 1.0000\n",
            "Epoch [30/100], Loss: 0.0143, Accuracy: 1.0000\n",
            "Epoch [31/100], Loss: 0.0118, Accuracy: 1.0000\n",
            "Epoch [32/100], Loss: 0.0122, Accuracy: 1.0000\n",
            "Epoch [33/100], Loss: 0.0101, Accuracy: 1.0000\n",
            "Epoch [34/100], Loss: 0.0101, Accuracy: 1.0000\n",
            "Epoch [35/100], Loss: 0.0081, Accuracy: 1.0000\n",
            "Epoch [36/100], Loss: 0.0073, Accuracy: 1.0000\n",
            "Epoch [37/100], Loss: 0.0069, Accuracy: 1.0000\n",
            "Epoch [38/100], Loss: 0.0069, Accuracy: 1.0000\n",
            "Epoch [39/100], Loss: 0.0063, Accuracy: 1.0000\n",
            "Epoch [40/100], Loss: 0.0056, Accuracy: 1.0000\n",
            "Epoch [41/100], Loss: 0.0054, Accuracy: 1.0000\n",
            "Epoch [42/100], Loss: 0.0052, Accuracy: 1.0000\n",
            "Epoch [43/100], Loss: 0.0049, Accuracy: 1.0000\n",
            "Epoch [44/100], Loss: 0.0045, Accuracy: 1.0000\n",
            "Epoch [45/100], Loss: 0.0044, Accuracy: 1.0000\n",
            "Epoch [46/100], Loss: 0.0043, Accuracy: 1.0000\n",
            "Epoch [47/100], Loss: 0.0046, Accuracy: 1.0000\n",
            "Epoch [48/100], Loss: 0.0038, Accuracy: 1.0000\n",
            "Epoch [49/100], Loss: 0.0035, Accuracy: 1.0000\n",
            "Epoch [50/100], Loss: 0.0035, Accuracy: 1.0000\n",
            "Epoch [51/100], Loss: 0.0035, Accuracy: 1.0000\n",
            "Epoch [52/100], Loss: 0.0032, Accuracy: 1.0000\n",
            "Epoch [53/100], Loss: 0.0030, Accuracy: 1.0000\n",
            "Epoch [54/100], Loss: 0.0029, Accuracy: 1.0000\n",
            "Epoch [55/100], Loss: 0.0034, Accuracy: 1.0000\n",
            "Epoch [56/100], Loss: 0.0028, Accuracy: 1.0000\n",
            "Epoch [57/100], Loss: 0.0027, Accuracy: 1.0000\n",
            "Epoch [58/100], Loss: 0.0028, Accuracy: 1.0000\n",
            "Epoch [59/100], Loss: 0.0026, Accuracy: 1.0000\n",
            "Epoch [60/100], Loss: 0.0025, Accuracy: 1.0000\n",
            "Epoch [61/100], Loss: 0.0022, Accuracy: 1.0000\n",
            "Epoch [62/100], Loss: 0.0021, Accuracy: 1.0000\n",
            "Epoch [63/100], Loss: 0.0026, Accuracy: 1.0000\n",
            "Epoch [64/100], Loss: 0.0024, Accuracy: 1.0000\n",
            "Epoch [65/100], Loss: 0.0020, Accuracy: 1.0000\n",
            "Epoch [66/100], Loss: 0.0019, Accuracy: 1.0000\n",
            "Epoch [67/100], Loss: 0.0023, Accuracy: 1.0000\n",
            "Epoch [68/100], Loss: 0.0022, Accuracy: 1.0000\n",
            "Epoch [69/100], Loss: 0.0021, Accuracy: 1.0000\n",
            "Epoch [70/100], Loss: 0.0017, Accuracy: 1.0000\n",
            "Epoch [71/100], Loss: 0.0017, Accuracy: 1.0000\n",
            "Epoch [72/100], Loss: 0.0016, Accuracy: 1.0000\n",
            "Epoch [73/100], Loss: 0.0016, Accuracy: 1.0000\n",
            "Epoch [74/100], Loss: 0.0016, Accuracy: 1.0000\n",
            "Epoch [75/100], Loss: 0.0017, Accuracy: 1.0000\n",
            "Epoch [76/100], Loss: 0.0015, Accuracy: 1.0000\n",
            "Epoch [77/100], Loss: 0.0016, Accuracy: 1.0000\n",
            "Epoch [78/100], Loss: 0.0014, Accuracy: 1.0000\n",
            "Epoch [79/100], Loss: 0.0014, Accuracy: 1.0000\n",
            "Epoch [80/100], Loss: 0.0013, Accuracy: 1.0000\n",
            "Epoch [81/100], Loss: 0.0015, Accuracy: 1.0000\n",
            "Epoch [82/100], Loss: 0.0013, Accuracy: 1.0000\n",
            "Epoch [83/100], Loss: 0.0015, Accuracy: 1.0000\n",
            "Epoch [84/100], Loss: 0.0011, Accuracy: 1.0000\n",
            "Epoch [85/100], Loss: 0.0011, Accuracy: 1.0000\n",
            "Epoch [86/100], Loss: 0.0012, Accuracy: 1.0000\n",
            "Epoch [87/100], Loss: 0.0012, Accuracy: 1.0000\n",
            "Epoch [88/100], Loss: 0.0010, Accuracy: 1.0000\n",
            "Epoch [89/100], Loss: 0.0016, Accuracy: 1.0000\n",
            "Epoch [90/100], Loss: 0.0011, Accuracy: 1.0000\n",
            "Epoch [91/100], Loss: 0.0010, Accuracy: 1.0000\n",
            "Epoch [92/100], Loss: 0.0011, Accuracy: 1.0000\n",
            "Epoch [93/100], Loss: 0.0009, Accuracy: 1.0000\n",
            "Epoch [94/100], Loss: 0.0010, Accuracy: 1.0000\n",
            "Epoch [95/100], Loss: 0.0010, Accuracy: 1.0000\n",
            "Epoch [96/100], Loss: 0.0009, Accuracy: 1.0000\n",
            "Epoch [97/100], Loss: 0.0010, Accuracy: 1.0000\n",
            "Epoch [98/100], Loss: 0.0014, Accuracy: 1.0000\n",
            "Epoch [99/100], Loss: 0.0009, Accuracy: 1.0000\n",
            "Epoch [100/100], Loss: 0.0009, Accuracy: 1.0000\n",
            "Test Accuracy: 85.3242\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet34, ResNet34_Weights\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from PIL import Image\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "train_images_path = \"brain_train_image_final.npy\"\n",
        "train_labels_path = \"brain_train_label.npy\"\n",
        "test_images_path = \"brain_test_image_final.npy\"\n",
        "test_labels_path = \"brain_test_label.npy\"\n",
        "\n",
        "# Load the data\n",
        "final_X_train_modified = np.load(train_images_path)[:, 1, :, :]\n",
        "final_X_test_modified = np.load(test_images_path)[:, 1, :, :]\n",
        "train_labels = np.load(train_labels_path)\n",
        "test_labels = np.load(test_labels_path)\n",
        "\n",
        "# Normalize and Resize Images using Pillow\n",
        "def normalize_and_resize(images, target_size=(224, 224)):\n",
        "    resized_images = []\n",
        "    for img in images:\n",
        "        img = Image.fromarray((img * 255).astype(np.uint8))\n",
        "        img_resized = img.resize(target_size, Image.Resampling.LANCZOS)\n",
        "        resized_images.append(np.array(img_resized) / 255.0)\n",
        "    return np.array(resized_images)\n",
        "\n",
        "final_X_train_resized = normalize_and_resize(final_X_train_modified)\n",
        "final_X_test_resized = normalize_and_resize(final_X_test_modified)\n",
        "\n",
        "# Define SimCLR Augmentation Transform\n",
        "transform_simclr = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0), ratio=(3/4, 4/3)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=0.8, contrast=0.8, saturation=0.8, hue=0.2),\n",
        "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=23, sigma=(0.1, 2.0))], p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# Custom Dataset for SimCLR\n",
        "class SimCLRDataset(Dataset):\n",
        "    def __init__(self, images, transform):\n",
        "        self.images = images\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]\n",
        "        img_1 = self.transform(img)\n",
        "        img_2 = self.transform(img)\n",
        "        return img_1, img_2\n",
        "\n",
        "train_dataset = SimCLRDataset(final_X_train_resized, transform_simclr)\n",
        "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
        "\n",
        "# Define SimCLR Model\n",
        "class SimCLR(nn.Module):\n",
        "    def __init__(self, base_encoder, projection_dim):\n",
        "        super(SimCLR, self).__init__()\n",
        "        self.encoder = base_encoder\n",
        "        self.projector = nn.Sequential(\n",
        "            nn.Linear(512, 512),  # Adjusted for ResNet-34 output\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, projection_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.encoder(x)\n",
        "        z = self.projector(h)\n",
        "        return z\n",
        "\n",
        "# Define NT-Xent Loss\n",
        "class NTXentLoss(nn.Module):\n",
        "    def __init__(self, temperature):\n",
        "        super(NTXentLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "\n",
        "    def forward(self, z_i, z_j):\n",
        "        N = z_i.size(0) + z_j.size(0)\n",
        "        z = torch.cat((z_i, z_j), dim=0)\n",
        "        sim = torch.mm(z, z.T) / self.temperature\n",
        "        sim = torch.nn.functional.softmax(sim, dim=1)\n",
        "\n",
        "        labels = torch.cat([\n",
        "            torch.arange(z_i.size(0), device=z.device),\n",
        "            torch.arange(z_j.size(0), device=z.device)\n",
        "        ])\n",
        "        loss = self.criterion(sim, labels)\n",
        "        return loss\n",
        "\n",
        "# Initialize ResNet-34 Encoder with updated weights argument\n",
        "base_encoder = resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)\n",
        "base_encoder.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "base_encoder.fc = nn.Identity()\n",
        "\n",
        "# Initialize SimCLR Model\n",
        "model = SimCLR(base_encoder, projection_dim=128).to(\"cuda\")\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
        "criterion = NTXentLoss(temperature=0.5)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=100)\n",
        "\n",
        "# Train SimCLR Model\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    for img_1, img_2 in train_loader:\n",
        "        img_1, img_2 = img_1.to(\"cuda\"), img_2.to(\"cuda\")\n",
        "        z_i = model(img_1)\n",
        "        z_j = model(img_2)\n",
        "\n",
        "        loss = criterion(z_i, z_j)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch [{epoch+1}/100], Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Define Dataset for Classification\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "# Initialize Training and Test Dataset and DataLoader\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "train_dataset = TestDataset(final_X_train_resized, train_labels, transform=train_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
        "\n",
        "test_dataset = TestDataset(final_X_test_resized, test_labels, transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
        "\n",
        "# Add Classification Head\n",
        "class ClassificationHead(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(ClassificationHead, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "classification_head = ClassificationHead(input_dim=512, num_classes=len(np.unique(train_labels))).to(\"cuda\")\n",
        "optimizer_cls = optim.Adam([\n",
        "    {\"params\": model.encoder.parameters(), \"lr\": 1e-5},\n",
        "    {\"params\": classification_head.parameters(), \"lr\": 3e-4},\n",
        "])\n",
        "criterion_cls = nn.CrossEntropyLoss()\n",
        "\n",
        "# Fine-tune Classification Head\n",
        "for epoch in range(100):\n",
        "    model.encoder.train()\n",
        "    classification_head.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    for img, label in train_loader:\n",
        "        img, label = img.to(\"cuda\"), label.to(\"cuda\")\n",
        "        features = model.encoder(img)\n",
        "        logits = classification_head(features)\n",
        "        loss = criterion_cls(logits, label)\n",
        "\n",
        "        optimizer_cls.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_cls.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        correct += (logits.argmax(dim=1) == label).sum().item()\n",
        "\n",
        "    accuracy = correct / len(train_labels)\n",
        "    print(f\"Epoch [{epoch+1}/100], Loss: {total_loss/len(train_loader):.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Evaluate on Test Dataset\n",
        "classification_head.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for img, label in test_loader:\n",
        "        img, label = img.to(\"cuda\"), label.to(\"cuda\")\n",
        "        features = model.encoder(img)\n",
        "        logits = classification_head(features)\n",
        "        correct += (logits.argmax(dim=1) == label).sum().item()\n",
        "\n",
        "test_accuracy = correct / len(test_labels)\n",
        "print(f\"Test Accuracy: {test_accuracy*100:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7d8440a-e724-4e89-b18a-889c8bf21d60",
      "metadata": {
        "id": "c7d8440a-e724-4e89-b18a-889c8bf21d60"
      },
      "source": [
        "# The below one is used the previous SimCLR but has different fine-tuning epoches=200 for testing. the accuracy is 89%\n",
        "\n",
        "\n",
        "- **Batch Size**:\n",
        "  - `512` for both training and testing.\n",
        "- **Shuffling**:\n",
        "  - Training set is shuffled to enhance generalization during training.\n",
        "  - Testing set is not shuffled, ensuring deterministic evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Fine-Tuning with a Classification Head**\n",
        "- **Classification Head**:\n",
        "  - A single linear layer (`Linear(512, num_classes)`) maps encoder features to the output class logits.\n",
        "- **Fine-Tuning Procedure**:\n",
        "  - The SimCLR encoder is fine-tuned along with the classification head.\n",
        "  - Hyperparameters:\n",
        "    - Learning rate for encoder: `1e-5`\n",
        "    - Learning rate for classification head: `3e-4`\n",
        "    - Loss function: `CrossEntropyLoss`\n",
        "    - Epochs: `200`\n",
        "  - Optimizer: Adam optimizes both the encoder and classification head weights.\n",
        "- **Training Outputs**:\n",
        "  - The average training loss and accuracy are logged for each epoch.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d2467f5-d85c-4c27-b425-090f65f55e2d",
      "metadata": {
        "tags": [],
        "id": "5d2467f5-d85c-4c27-b425-090f65f55e2d",
        "outputId": "2e498046-dfd3-4f3d-a6db-be23cf0c3c47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Loss: 1.1620, Accuracy: 0.3193\n",
            "Epoch [2/100], Loss: 0.6423, Accuracy: 0.8497\n",
            "Epoch [3/100], Loss: 0.3671, Accuracy: 0.9897\n",
            "Epoch [4/100], Loss: 0.2183, Accuracy: 0.9976\n",
            "Epoch [5/100], Loss: 0.1332, Accuracy: 0.9994\n",
            "Epoch [6/100], Loss: 0.0877, Accuracy: 0.9994\n",
            "Epoch [7/100], Loss: 0.0580, Accuracy: 0.9994\n",
            "Epoch [8/100], Loss: 0.0429, Accuracy: 0.9994\n",
            "Epoch [9/100], Loss: 0.0326, Accuracy: 1.0000\n",
            "Epoch [10/100], Loss: 0.0248, Accuracy: 1.0000\n",
            "Epoch [11/100], Loss: 0.0214, Accuracy: 1.0000\n",
            "Epoch [12/100], Loss: 0.0174, Accuracy: 1.0000\n",
            "Epoch [13/100], Loss: 0.0147, Accuracy: 1.0000\n",
            "Epoch [14/100], Loss: 0.0130, Accuracy: 1.0000\n",
            "Epoch [15/100], Loss: 0.0127, Accuracy: 1.0000\n",
            "Epoch [16/100], Loss: 0.0102, Accuracy: 1.0000\n",
            "Epoch [17/100], Loss: 0.0101, Accuracy: 1.0000\n",
            "Epoch [18/100], Loss: 0.0088, Accuracy: 1.0000\n",
            "Epoch [19/100], Loss: 0.0082, Accuracy: 1.0000\n",
            "Epoch [20/100], Loss: 0.0072, Accuracy: 1.0000\n",
            "Epoch [21/100], Loss: 0.0086, Accuracy: 1.0000\n",
            "Epoch [22/100], Loss: 0.0063, Accuracy: 1.0000\n",
            "Epoch [23/100], Loss: 0.0061, Accuracy: 1.0000\n",
            "Epoch [24/100], Loss: 0.0056, Accuracy: 1.0000\n",
            "Epoch [25/100], Loss: 0.0072, Accuracy: 1.0000\n",
            "Epoch [26/100], Loss: 0.0054, Accuracy: 1.0000\n",
            "Epoch [27/100], Loss: 0.0050, Accuracy: 1.0000\n",
            "Epoch [28/100], Loss: 0.0048, Accuracy: 1.0000\n",
            "Epoch [29/100], Loss: 0.0048, Accuracy: 1.0000\n",
            "Epoch [30/100], Loss: 0.0042, Accuracy: 1.0000\n",
            "Epoch [31/100], Loss: 0.0039, Accuracy: 1.0000\n",
            "Epoch [32/100], Loss: 0.0041, Accuracy: 1.0000\n",
            "Epoch [33/100], Loss: 0.0036, Accuracy: 1.0000\n",
            "Epoch [34/100], Loss: 0.0036, Accuracy: 1.0000\n",
            "Epoch [35/100], Loss: 0.0033, Accuracy: 1.0000\n",
            "Epoch [36/100], Loss: 0.0031, Accuracy: 1.0000\n",
            "Epoch [37/100], Loss: 0.0033, Accuracy: 1.0000\n",
            "Epoch [38/100], Loss: 0.0033, Accuracy: 1.0000\n",
            "Epoch [39/100], Loss: 0.0033, Accuracy: 1.0000\n",
            "Epoch [40/100], Loss: 0.0029, Accuracy: 1.0000\n",
            "Epoch [41/100], Loss: 0.0025, Accuracy: 1.0000\n",
            "Epoch [42/100], Loss: 0.0029, Accuracy: 1.0000\n",
            "Epoch [43/100], Loss: 0.0025, Accuracy: 1.0000\n",
            "Epoch [44/100], Loss: 0.0023, Accuracy: 1.0000\n",
            "Epoch [45/100], Loss: 0.0023, Accuracy: 1.0000\n",
            "Epoch [46/100], Loss: 0.0022, Accuracy: 1.0000\n",
            "Epoch [47/100], Loss: 0.0024, Accuracy: 1.0000\n",
            "Epoch [48/100], Loss: 0.0021, Accuracy: 1.0000\n",
            "Epoch [49/100], Loss: 0.0029, Accuracy: 1.0000\n",
            "Epoch [50/100], Loss: 0.0019, Accuracy: 1.0000\n",
            "Epoch [51/100], Loss: 0.0018, Accuracy: 1.0000\n",
            "Epoch [52/100], Loss: 0.0019, Accuracy: 1.0000\n",
            "Epoch [53/100], Loss: 0.0018, Accuracy: 1.0000\n",
            "Epoch [54/100], Loss: 0.0018, Accuracy: 1.0000\n",
            "Epoch [55/100], Loss: 0.0018, Accuracy: 1.0000\n",
            "Epoch [56/100], Loss: 0.0016, Accuracy: 1.0000\n",
            "Epoch [57/100], Loss: 0.0016, Accuracy: 1.0000\n",
            "Epoch [58/100], Loss: 0.0016, Accuracy: 1.0000\n",
            "Epoch [59/100], Loss: 0.0017, Accuracy: 1.0000\n",
            "Epoch [60/100], Loss: 0.0014, Accuracy: 1.0000\n",
            "Epoch [61/100], Loss: 0.0014, Accuracy: 1.0000\n",
            "Epoch [62/100], Loss: 0.0013, Accuracy: 1.0000\n",
            "Epoch [63/100], Loss: 0.0014, Accuracy: 1.0000\n",
            "Epoch [64/100], Loss: 0.0013, Accuracy: 1.0000\n",
            "Epoch [65/100], Loss: 0.0015, Accuracy: 1.0000\n",
            "Epoch [66/100], Loss: 0.0013, Accuracy: 1.0000\n",
            "Epoch [67/100], Loss: 0.0014, Accuracy: 1.0000\n",
            "Epoch [68/100], Loss: 0.0013, Accuracy: 1.0000\n",
            "Epoch [69/100], Loss: 0.0015, Accuracy: 1.0000\n",
            "Epoch [70/100], Loss: 0.0012, Accuracy: 1.0000\n",
            "Epoch [71/100], Loss: 0.0013, Accuracy: 1.0000\n",
            "Epoch [72/100], Loss: 0.0011, Accuracy: 1.0000\n",
            "Epoch [73/100], Loss: 0.0011, Accuracy: 1.0000\n",
            "Epoch [74/100], Loss: 0.0013, Accuracy: 1.0000\n",
            "Epoch [75/100], Loss: 0.0012, Accuracy: 1.0000\n",
            "Epoch [76/100], Loss: 0.0011, Accuracy: 1.0000\n",
            "Epoch [77/100], Loss: 0.0011, Accuracy: 1.0000\n",
            "Epoch [78/100], Loss: 0.0009, Accuracy: 1.0000\n",
            "Epoch [79/100], Loss: 0.0010, Accuracy: 1.0000\n",
            "Epoch [80/100], Loss: 0.0009, Accuracy: 1.0000\n",
            "Epoch [81/100], Loss: 0.0009, Accuracy: 1.0000\n",
            "Epoch [82/100], Loss: 0.0009, Accuracy: 1.0000\n",
            "Epoch [83/100], Loss: 0.0009, Accuracy: 1.0000\n",
            "Epoch [84/100], Loss: 0.0009, Accuracy: 1.0000\n",
            "Epoch [85/100], Loss: 0.0009, Accuracy: 1.0000\n",
            "Epoch [86/100], Loss: 0.0008, Accuracy: 1.0000\n",
            "Epoch [87/100], Loss: 0.0008, Accuracy: 1.0000\n",
            "Epoch [88/100], Loss: 0.0009, Accuracy: 1.0000\n",
            "Epoch [89/100], Loss: 0.0008, Accuracy: 1.0000\n",
            "Epoch [90/100], Loss: 0.0008, Accuracy: 1.0000\n",
            "Epoch [91/100], Loss: 0.0007, Accuracy: 1.0000\n",
            "Epoch [92/100], Loss: 0.0008, Accuracy: 1.0000\n",
            "Epoch [93/100], Loss: 0.0007, Accuracy: 1.0000\n",
            "Epoch [94/100], Loss: 0.0007, Accuracy: 1.0000\n",
            "Epoch [95/100], Loss: 0.0009, Accuracy: 1.0000\n",
            "Epoch [96/100], Loss: 0.0008, Accuracy: 1.0000\n",
            "Epoch [97/100], Loss: 0.0008, Accuracy: 1.0000\n",
            "Epoch [98/100], Loss: 0.0006, Accuracy: 1.0000\n",
            "Epoch [99/100], Loss: 0.0006, Accuracy: 1.0000\n",
            "Epoch [100/100], Loss: 0.0006, Accuracy: 1.0000\n",
            "Epoch [101/100], Loss: 0.0006, Accuracy: 1.0000\n",
            "Epoch [102/100], Loss: 0.0006, Accuracy: 1.0000\n",
            "Epoch [103/100], Loss: 0.0006, Accuracy: 1.0000\n",
            "Epoch [104/100], Loss: 0.0006, Accuracy: 1.0000\n",
            "Epoch [105/100], Loss: 0.0005, Accuracy: 1.0000\n",
            "Epoch [106/100], Loss: 0.0006, Accuracy: 1.0000\n",
            "Epoch [107/100], Loss: 0.0006, Accuracy: 1.0000\n",
            "Epoch [108/100], Loss: 0.0007, Accuracy: 1.0000\n",
            "Epoch [109/100], Loss: 0.0006, Accuracy: 1.0000\n",
            "Epoch [110/100], Loss: 0.0005, Accuracy: 1.0000\n",
            "Epoch [111/100], Loss: 0.0006, Accuracy: 1.0000\n",
            "Epoch [112/100], Loss: 0.0006, Accuracy: 1.0000\n",
            "Epoch [113/100], Loss: 0.0005, Accuracy: 1.0000\n",
            "Epoch [114/100], Loss: 0.0007, Accuracy: 1.0000\n",
            "Epoch [115/100], Loss: 0.0005, Accuracy: 1.0000\n",
            "Epoch [116/100], Loss: 0.0005, Accuracy: 1.0000\n",
            "Epoch [117/100], Loss: 0.0005, Accuracy: 1.0000\n",
            "Epoch [118/100], Loss: 0.0005, Accuracy: 1.0000\n",
            "Epoch [119/100], Loss: 0.0005, Accuracy: 1.0000\n",
            "Epoch [120/100], Loss: 0.0005, Accuracy: 1.0000\n",
            "Epoch [121/100], Loss: 0.0005, Accuracy: 1.0000\n",
            "Epoch [122/100], Loss: 0.0005, Accuracy: 1.0000\n",
            "Epoch [123/100], Loss: 0.0005, Accuracy: 1.0000\n",
            "Epoch [124/100], Loss: 0.0005, Accuracy: 1.0000\n",
            "Epoch [125/100], Loss: 0.0004, Accuracy: 1.0000\n",
            "Epoch [126/100], Loss: 0.0004, Accuracy: 1.0000\n",
            "Epoch [127/100], Loss: 0.0004, Accuracy: 1.0000\n",
            "Epoch [128/100], Loss: 0.0004, Accuracy: 1.0000\n",
            "Epoch [129/100], Loss: 0.0004, Accuracy: 1.0000\n",
            "Epoch [130/100], Loss: 0.0004, Accuracy: 1.0000\n",
            "Epoch [131/100], Loss: 0.0004, Accuracy: 1.0000\n",
            "Epoch [132/100], Loss: 0.0004, Accuracy: 1.0000\n",
            "Epoch [133/100], Loss: 0.0004, Accuracy: 1.0000\n",
            "Epoch [134/100], Loss: 0.0004, Accuracy: 1.0000\n",
            "Epoch [135/100], Loss: 0.0005, Accuracy: 1.0000\n",
            "Epoch [136/100], Loss: 0.0004, Accuracy: 1.0000\n",
            "Epoch [137/100], Loss: 0.0004, Accuracy: 1.0000\n",
            "Epoch [138/100], Loss: 0.0004, Accuracy: 1.0000\n",
            "Epoch [139/100], Loss: 0.0004, Accuracy: 1.0000\n",
            "Epoch [140/100], Loss: 0.0004, Accuracy: 1.0000\n",
            "Epoch [141/100], Loss: 0.0003, Accuracy: 1.0000\n",
            "Epoch [142/100], Loss: 0.0004, Accuracy: 1.0000\n",
            "Epoch [143/100], Loss: 0.0003, Accuracy: 1.0000\n",
            "Epoch [144/100], Loss: 0.0003, Accuracy: 1.0000\n",
            "Epoch [145/100], Loss: 0.0003, Accuracy: 1.0000\n",
            "Epoch [146/100], Loss: 0.0003, Accuracy: 1.0000\n",
            "Epoch [147/100], Loss: 0.0003, Accuracy: 1.0000\n",
            "Epoch [148/100], Loss: 0.0003, Accuracy: 1.0000\n",
            "Epoch [149/100], Loss: 0.0003, Accuracy: 1.0000\n",
            "Epoch [150/100], Loss: 0.0003, Accuracy: 1.0000\n",
            "Epoch [151/100], Loss: 0.0003, Accuracy: 1.0000\n",
            "Epoch [152/100], Loss: 0.0003, Accuracy: 1.0000\n",
            "Epoch [153/100], Loss: 0.0004, Accuracy: 1.0000\n",
            "Epoch [154/100], Loss: 0.0004, Accuracy: 1.0000\n",
            "Epoch [155/100], Loss: 0.0003, Accuracy: 1.0000\n",
            "Epoch [156/100], Loss: 0.0003, Accuracy: 1.0000\n",
            "Epoch [157/100], Loss: 0.0003, Accuracy: 1.0000\n",
            "Epoch [158/100], Loss: 0.0003, Accuracy: 1.0000\n",
            "Epoch [159/100], Loss: 0.0003, Accuracy: 1.0000\n",
            "Epoch [160/100], Loss: 0.0003, Accuracy: 1.0000\n",
            "Epoch [161/100], Loss: 0.0003, Accuracy: 1.0000\n",
            "Epoch [162/100], Loss: 0.0003, Accuracy: 1.0000\n",
            "Epoch [163/100], Loss: 0.0004, Accuracy: 1.0000\n",
            "Epoch [164/100], Loss: 0.0003, Accuracy: 1.0000\n",
            "Epoch [165/100], Loss: 0.0003, Accuracy: 1.0000\n",
            "Epoch [166/100], Loss: 0.0003, Accuracy: 1.0000\n",
            "Epoch [167/100], Loss: 0.0002, Accuracy: 1.0000\n",
            "Epoch [168/100], Loss: 0.0003, Accuracy: 1.0000\n",
            "Epoch [169/100], Loss: 0.0003, Accuracy: 1.0000\n",
            "Epoch [170/100], Loss: 0.0002, Accuracy: 1.0000\n",
            "Epoch [171/100], Loss: 0.0002, Accuracy: 1.0000\n",
            "Epoch [172/100], Loss: 0.0003, Accuracy: 1.0000\n",
            "Epoch [173/100], Loss: 0.0003, Accuracy: 1.0000\n",
            "Epoch [174/100], Loss: 0.0003, Accuracy: 1.0000\n",
            "Epoch [175/100], Loss: 0.0003, Accuracy: 1.0000\n",
            "Epoch [176/100], Loss: 0.0002, Accuracy: 1.0000\n",
            "Epoch [177/100], Loss: 0.0002, Accuracy: 1.0000\n",
            "Epoch [178/100], Loss: 0.0002, Accuracy: 1.0000\n",
            "Epoch [179/100], Loss: 0.0002, Accuracy: 1.0000\n",
            "Epoch [180/100], Loss: 0.0002, Accuracy: 1.0000\n",
            "Epoch [181/100], Loss: 0.0002, Accuracy: 1.0000\n",
            "Epoch [182/100], Loss: 0.0002, Accuracy: 1.0000\n",
            "Epoch [183/100], Loss: 0.0002, Accuracy: 1.0000\n",
            "Epoch [184/100], Loss: 0.0003, Accuracy: 1.0000\n",
            "Epoch [185/100], Loss: 0.0002, Accuracy: 1.0000\n",
            "Epoch [186/100], Loss: 0.0002, Accuracy: 1.0000\n",
            "Epoch [187/100], Loss: 0.0002, Accuracy: 1.0000\n",
            "Epoch [188/100], Loss: 0.0002, Accuracy: 1.0000\n",
            "Epoch [189/100], Loss: 0.0002, Accuracy: 1.0000\n",
            "Epoch [190/100], Loss: 0.0002, Accuracy: 1.0000\n",
            "Epoch [191/100], Loss: 0.0002, Accuracy: 1.0000\n",
            "Epoch [192/100], Loss: 0.0002, Accuracy: 1.0000\n",
            "Epoch [193/100], Loss: 0.0002, Accuracy: 1.0000\n",
            "Epoch [194/100], Loss: 0.0002, Accuracy: 1.0000\n",
            "Epoch [195/100], Loss: 0.0002, Accuracy: 1.0000\n",
            "Epoch [196/100], Loss: 0.0002, Accuracy: 1.0000\n",
            "Epoch [197/100], Loss: 0.0002, Accuracy: 1.0000\n",
            "Epoch [198/100], Loss: 0.0002, Accuracy: 1.0000\n",
            "Epoch [199/100], Loss: 0.0002, Accuracy: 1.0000\n",
            "Epoch [200/100], Loss: 0.0002, Accuracy: 1.0000\n",
            "Test Accuracy: 89.0785\n"
          ]
        }
      ],
      "source": [
        "# Define Dataset for Classification\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "# Initialize Training and Test Dataset and DataLoader\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "train_dataset = TestDataset(final_X_train_resized, train_labels, transform=train_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
        "\n",
        "test_dataset = TestDataset(final_X_test_resized, test_labels, transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
        "\n",
        "# Add Classification Head\n",
        "class ClassificationHead(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(ClassificationHead, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "classification_head = ClassificationHead(input_dim=512, num_classes=len(np.unique(train_labels))).to(\"cuda\")\n",
        "optimizer_cls = optim.Adam([\n",
        "    {\"params\": model.encoder.parameters(), \"lr\": 1e-5},\n",
        "    {\"params\": classification_head.parameters(), \"lr\": 3e-4},\n",
        "])\n",
        "criterion_cls = nn.CrossEntropyLoss()\n",
        "\n",
        "# Fine-tune Classification Head\n",
        "for epoch in range(200):\n",
        "    model.encoder.train()\n",
        "    classification_head.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    for img, label in train_loader:\n",
        "        img, label = img.to(\"cuda\"), label.to(\"cuda\")\n",
        "        features = model.encoder(img)\n",
        "        logits = classification_head(features)\n",
        "        loss = criterion_cls(logits, label)\n",
        "\n",
        "        optimizer_cls.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_cls.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        correct += (logits.argmax(dim=1) == label).sum().item()\n",
        "\n",
        "    accuracy = correct / len(train_labels)\n",
        "    print(f\"Epoch [{epoch+1}/100], Loss: {total_loss/len(train_loader):.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Evaluate on Test Dataset\n",
        "classification_head.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for img, label in test_loader:\n",
        "        img, label = img.to(\"cuda\"), label.to(\"cuda\")\n",
        "        features = model.encoder(img)\n",
        "        logits = classification_head(features)\n",
        "        correct += (logits.argmax(dim=1) == label).sum().item()\n",
        "\n",
        "test_accuracy = correct / len(test_labels)\n",
        "print(f\"Test Accuracy: {test_accuracy*100:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "PyTorch 2.4.0",
      "language": "python",
      "name": "pytorch-2.4.0"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}